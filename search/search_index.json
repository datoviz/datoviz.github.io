{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Datoviz: scientific visualization with Vulkan \u00b6 Note: this file is unused, the mkdocs build process loads the README.md file at the root instead.","title":"Home"},{"location":"#datoviz-scientific-visualization-with-vulkan","text":"Note: this file is unused, the mkdocs build process loads the README.md file at the root instead.","title":"Datoviz: scientific visualization with Vulkan"},{"location":"api/","text":"C API documentation \u00b6 Scene API Visual API Controller API GUI API Canvas API GPU API vklite API Misc API Enumerations","title":"Index"},{"location":"api/#c-api-documentation","text":"Scene API Visual API Controller API GUI API Canvas API GPU API vklite API Misc API Enumerations","title":"C API documentation"},{"location":"api/canvas/","text":"Canvas API \u00b6 Canvas \u00b6 dvz_canvas_commands() \u00b6 dvz_canvas_clear_color() \u00b6 dvz_canvas_size() \u00b6 dvz_canvas_close_on_esc() \u00b6 dvz_canvas_recreate() \u00b6 dvz_canvas_to_refill() \u00b6 dvz_canvas_to_close() \u00b6 dvz_canvases_destroy() \u00b6 Misc \u00b6 dvz_viewport_full() \u00b6 dvz_viewport_default() \u00b6 Event emitting \u00b6 dvz_event_mouse_press() \u00b6 dvz_event_mouse_release() \u00b6 dvz_event_mouse_move() \u00b6 dvz_event_mouse_wheel() \u00b6 dvz_event_mouse_click() \u00b6 dvz_event_mouse_double_click() \u00b6 dvz_event_mouse_drag() \u00b6 dvz_event_mouse_drag_end() \u00b6 dvz_event_key_press() \u00b6 dvz_event_key_release() \u00b6 dvz_event_frame() \u00b6 dvz_event_timer() \u00b6 Screencast \u00b6 dvz_screencast() \u00b6 dvz_screencast_destroy() \u00b6 dvz_screenshot() \u00b6 dvz_screenshot_file() \u00b6 dvz_canvas_video() \u00b6 dvz_canvas_pause() \u00b6 dvz_canvas_stop() \u00b6 Internal event loop \u00b6 dvz_canvas_frame() \u00b6 dvz_canvas_frame_submit() \u00b6 Internal event system \u00b6 dvz_event_callback() \u00b6 dvz_event_pending() \u00b6 dvz_event_stop() \u00b6 dvz_mouse() \u00b6 dvz_mouse_reset() \u00b6 dvz_mouse_event() \u00b6 dvz_mouse_local() \u00b6 dvz_keyboard() \u00b6 dvz_keyboard_reset() \u00b6 dvz_keyboard_event() \u00b6","title":"Canvas API"},{"location":"api/canvas/#canvas-api","text":"","title":"Canvas API"},{"location":"api/canvas/#canvas","text":"","title":"Canvas"},{"location":"api/canvas/#dvz_canvas_commands","text":"","title":"dvz_canvas_commands()"},{"location":"api/canvas/#dvz_canvas_clear_color","text":"","title":"dvz_canvas_clear_color()"},{"location":"api/canvas/#dvz_canvas_size","text":"","title":"dvz_canvas_size()"},{"location":"api/canvas/#dvz_canvas_close_on_esc","text":"","title":"dvz_canvas_close_on_esc()"},{"location":"api/canvas/#dvz_canvas_recreate","text":"","title":"dvz_canvas_recreate()"},{"location":"api/canvas/#dvz_canvas_to_refill","text":"","title":"dvz_canvas_to_refill()"},{"location":"api/canvas/#dvz_canvas_to_close","text":"","title":"dvz_canvas_to_close()"},{"location":"api/canvas/#dvz_canvases_destroy","text":"","title":"dvz_canvases_destroy()"},{"location":"api/canvas/#misc","text":"","title":"Misc"},{"location":"api/canvas/#dvz_viewport_full","text":"","title":"dvz_viewport_full()"},{"location":"api/canvas/#dvz_viewport_default","text":"","title":"dvz_viewport_default()"},{"location":"api/canvas/#event-emitting","text":"","title":"Event emitting"},{"location":"api/canvas/#dvz_event_mouse_press","text":"","title":"dvz_event_mouse_press()"},{"location":"api/canvas/#dvz_event_mouse_release","text":"","title":"dvz_event_mouse_release()"},{"location":"api/canvas/#dvz_event_mouse_move","text":"","title":"dvz_event_mouse_move()"},{"location":"api/canvas/#dvz_event_mouse_wheel","text":"","title":"dvz_event_mouse_wheel()"},{"location":"api/canvas/#dvz_event_mouse_click","text":"","title":"dvz_event_mouse_click()"},{"location":"api/canvas/#dvz_event_mouse_double_click","text":"","title":"dvz_event_mouse_double_click()"},{"location":"api/canvas/#dvz_event_mouse_drag","text":"","title":"dvz_event_mouse_drag()"},{"location":"api/canvas/#dvz_event_mouse_drag_end","text":"","title":"dvz_event_mouse_drag_end()"},{"location":"api/canvas/#dvz_event_key_press","text":"","title":"dvz_event_key_press()"},{"location":"api/canvas/#dvz_event_key_release","text":"","title":"dvz_event_key_release()"},{"location":"api/canvas/#dvz_event_frame","text":"","title":"dvz_event_frame()"},{"location":"api/canvas/#dvz_event_timer","text":"","title":"dvz_event_timer()"},{"location":"api/canvas/#screencast","text":"","title":"Screencast"},{"location":"api/canvas/#dvz_screencast","text":"","title":"dvz_screencast()"},{"location":"api/canvas/#dvz_screencast_destroy","text":"","title":"dvz_screencast_destroy()"},{"location":"api/canvas/#dvz_screenshot","text":"","title":"dvz_screenshot()"},{"location":"api/canvas/#dvz_screenshot_file","text":"","title":"dvz_screenshot_file()"},{"location":"api/canvas/#dvz_canvas_video","text":"","title":"dvz_canvas_video()"},{"location":"api/canvas/#dvz_canvas_pause","text":"","title":"dvz_canvas_pause()"},{"location":"api/canvas/#dvz_canvas_stop","text":"","title":"dvz_canvas_stop()"},{"location":"api/canvas/#internal-event-loop","text":"","title":"Internal event loop"},{"location":"api/canvas/#dvz_canvas_frame","text":"","title":"dvz_canvas_frame()"},{"location":"api/canvas/#dvz_canvas_frame_submit","text":"","title":"dvz_canvas_frame_submit()"},{"location":"api/canvas/#internal-event-system","text":"","title":"Internal event system"},{"location":"api/canvas/#dvz_event_callback","text":"","title":"dvz_event_callback()"},{"location":"api/canvas/#dvz_event_pending","text":"","title":"dvz_event_pending()"},{"location":"api/canvas/#dvz_event_stop","text":"","title":"dvz_event_stop()"},{"location":"api/canvas/#dvz_mouse","text":"","title":"dvz_mouse()"},{"location":"api/canvas/#dvz_mouse_reset","text":"","title":"dvz_mouse_reset()"},{"location":"api/canvas/#dvz_mouse_event","text":"","title":"dvz_mouse_event()"},{"location":"api/canvas/#dvz_mouse_local","text":"","title":"dvz_mouse_local()"},{"location":"api/canvas/#dvz_keyboard","text":"","title":"dvz_keyboard()"},{"location":"api/canvas/#dvz_keyboard_reset","text":"","title":"dvz_keyboard_reset()"},{"location":"api/canvas/#dvz_keyboard_event","text":"","title":"dvz_keyboard_event()"},{"location":"api/controller/","text":"Controller API \u00b6 Controller utilities \u00b6 dvz_camera_pos() \u00b6 dvz_camera_look() \u00b6 dvz_arcball_rotate() \u00b6 dvz_mvp_camera() \u00b6 Internal controller \u00b6 dvz_controller() \u00b6 dvz_controller_visual() \u00b6 dvz_controller_interact() \u00b6 dvz_controller_callback() \u00b6 dvz_controller_update() \u00b6 dvz_controller_destroy() \u00b6 dvz_controller_builtin() \u00b6 Transform \u00b6 dvz_transform_pos() \u00b6 dvz_transform() \u00b6","title":"Controller API"},{"location":"api/controller/#controller-api","text":"","title":"Controller API"},{"location":"api/controller/#controller-utilities","text":"","title":"Controller utilities"},{"location":"api/controller/#dvz_camera_pos","text":"","title":"dvz_camera_pos()"},{"location":"api/controller/#dvz_camera_look","text":"","title":"dvz_camera_look()"},{"location":"api/controller/#dvz_arcball_rotate","text":"","title":"dvz_arcball_rotate()"},{"location":"api/controller/#dvz_mvp_camera","text":"","title":"dvz_mvp_camera()"},{"location":"api/controller/#internal-controller","text":"","title":"Internal controller"},{"location":"api/controller/#dvz_controller","text":"","title":"dvz_controller()"},{"location":"api/controller/#dvz_controller_visual","text":"","title":"dvz_controller_visual()"},{"location":"api/controller/#dvz_controller_interact","text":"","title":"dvz_controller_interact()"},{"location":"api/controller/#dvz_controller_callback","text":"","title":"dvz_controller_callback()"},{"location":"api/controller/#dvz_controller_update","text":"","title":"dvz_controller_update()"},{"location":"api/controller/#dvz_controller_destroy","text":"","title":"dvz_controller_destroy()"},{"location":"api/controller/#dvz_controller_builtin","text":"","title":"dvz_controller_builtin()"},{"location":"api/controller/#transform","text":"","title":"Transform"},{"location":"api/controller/#dvz_transform_pos","text":"","title":"dvz_transform_pos()"},{"location":"api/controller/#dvz_transform","text":"","title":"dvz_transform()"},{"location":"api/enums/","text":"Enumerations \u00b6 Scene \u00b6 DvzControllerType \u00b6 Visuals \u00b6 DvzMarkerType \u00b6 DvzJoinType \u00b6 DvzCapType \u00b6 DvzPathTopology \u00b6 Miscellaneous \u00b6 DvzDataType \u00b6","title":"Enumerations"},{"location":"api/enums/#enumerations","text":"","title":"Enumerations"},{"location":"api/enums/#scene","text":"","title":"Scene"},{"location":"api/enums/#dvzcontrollertype","text":"","title":"DvzControllerType"},{"location":"api/enums/#visuals","text":"","title":"Visuals"},{"location":"api/enums/#dvzmarkertype","text":"","title":"DvzMarkerType"},{"location":"api/enums/#dvzjointype","text":"","title":"DvzJoinType"},{"location":"api/enums/#dvzcaptype","text":"","title":"DvzCapType"},{"location":"api/enums/#dvzpathtopology","text":"","title":"DvzPathTopology"},{"location":"api/enums/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"api/enums/#dvzdatatype","text":"","title":"DvzDataType"},{"location":"api/gpu/","text":"GPU context and objects \u00b6 Context \u00b6 dvz_context() \u00b6 dvz_context_reset() \u00b6 dvz_context_destroy() \u00b6 Buffers \u00b6 dvz_ctx_buffers() \u00b6 dvz_ctx_buffers_resize() \u00b6 Textures \u00b6 dvz_ctx_texture() \u00b6 dvz_texture_resize() \u00b6 dvz_texture_filter() \u00b6 dvz_texture_address_mode() \u00b6 dvz_texture_upload() \u00b6 dvz_texture_download() \u00b6 dvz_texture_copy() \u00b6 dvz_texture_destroy() \u00b6 Compute pipeline \u00b6 dvz_ctx_compute() \u00b6 Data transfers \u00b6 dvz_upload_buffers() \u00b6 dvz_download_buffers() \u00b6 dvz_copy_buffers() \u00b6 dvz_upload_texture() \u00b6 dvz_download_texture() \u00b6 dvz_copy_texture() \u00b6 dvz_process_transfers() \u00b6","title":"GPU API"},{"location":"api/gpu/#gpu-context-and-objects","text":"","title":"GPU context and objects"},{"location":"api/gpu/#context","text":"","title":"Context"},{"location":"api/gpu/#dvz_context","text":"","title":"dvz_context()"},{"location":"api/gpu/#dvz_context_reset","text":"","title":"dvz_context_reset()"},{"location":"api/gpu/#dvz_context_destroy","text":"","title":"dvz_context_destroy()"},{"location":"api/gpu/#buffers","text":"","title":"Buffers"},{"location":"api/gpu/#dvz_ctx_buffers","text":"","title":"dvz_ctx_buffers()"},{"location":"api/gpu/#dvz_ctx_buffers_resize","text":"","title":"dvz_ctx_buffers_resize()"},{"location":"api/gpu/#textures","text":"","title":"Textures"},{"location":"api/gpu/#dvz_ctx_texture","text":"","title":"dvz_ctx_texture()"},{"location":"api/gpu/#dvz_texture_resize","text":"","title":"dvz_texture_resize()"},{"location":"api/gpu/#dvz_texture_filter","text":"","title":"dvz_texture_filter()"},{"location":"api/gpu/#dvz_texture_address_mode","text":"","title":"dvz_texture_address_mode()"},{"location":"api/gpu/#dvz_texture_upload","text":"","title":"dvz_texture_upload()"},{"location":"api/gpu/#dvz_texture_download","text":"","title":"dvz_texture_download()"},{"location":"api/gpu/#dvz_texture_copy","text":"","title":"dvz_texture_copy()"},{"location":"api/gpu/#dvz_texture_destroy","text":"","title":"dvz_texture_destroy()"},{"location":"api/gpu/#compute-pipeline","text":"","title":"Compute pipeline"},{"location":"api/gpu/#dvz_ctx_compute","text":"","title":"dvz_ctx_compute()"},{"location":"api/gpu/#data-transfers","text":"","title":"Data transfers"},{"location":"api/gpu/#dvz_upload_buffers","text":"","title":"dvz_upload_buffers()"},{"location":"api/gpu/#dvz_download_buffers","text":"","title":"dvz_download_buffers()"},{"location":"api/gpu/#dvz_copy_buffers","text":"","title":"dvz_copy_buffers()"},{"location":"api/gpu/#dvz_upload_texture","text":"","title":"dvz_upload_texture()"},{"location":"api/gpu/#dvz_download_texture","text":"","title":"dvz_download_texture()"},{"location":"api/gpu/#dvz_copy_texture","text":"","title":"dvz_copy_texture()"},{"location":"api/gpu/#dvz_process_transfers","text":"","title":"dvz_process_transfers()"},{"location":"api/gui/","text":"GUI \u00b6 GUI controls \u00b6 dvz_gui() \u00b6 dvz_gui_checkbox() \u00b6 dvz_gui_slider_float() \u00b6 dvz_gui_slider_int() \u00b6 dvz_gui_label() \u00b6 dvz_gui_textbox() \u00b6 dvz_gui_button() \u00b6 dvz_gui_colormap() \u00b6 dvz_gui_demo() \u00b6 dvz_gui_destroy() \u00b6","title":"GUI API"},{"location":"api/gui/#gui","text":"","title":"GUI"},{"location":"api/gui/#gui-controls","text":"","title":"GUI controls"},{"location":"api/gui/#dvz_gui","text":"","title":"dvz_gui()"},{"location":"api/gui/#dvz_gui_checkbox","text":"","title":"dvz_gui_checkbox()"},{"location":"api/gui/#dvz_gui_slider_float","text":"","title":"dvz_gui_slider_float()"},{"location":"api/gui/#dvz_gui_slider_int","text":"","title":"dvz_gui_slider_int()"},{"location":"api/gui/#dvz_gui_label","text":"","title":"dvz_gui_label()"},{"location":"api/gui/#dvz_gui_textbox","text":"","title":"dvz_gui_textbox()"},{"location":"api/gui/#dvz_gui_button","text":"","title":"dvz_gui_button()"},{"location":"api/gui/#dvz_gui_colormap","text":"","title":"dvz_gui_colormap()"},{"location":"api/gui/#dvz_gui_demo","text":"","title":"dvz_gui_demo()"},{"location":"api/gui/#dvz_gui_destroy","text":"","title":"dvz_gui_destroy()"},{"location":"api/misc/","text":"Misc API \u00b6 Array \u00b6 dvz_array() \u00b6 dvz_array_point() \u00b6 dvz_array_wrap() \u00b6 dvz_array_struct() \u00b6 dvz_array_3D() \u00b6 dvz_array_resize() \u00b6 dvz_array_clear() \u00b6 dvz_array_reshape() \u00b6 dvz_array_data() \u00b6 dvz_array_item() \u00b6 dvz_array_column() \u00b6 dvz_array_insert() \u00b6 dvz_array_copy_region() \u00b6 dvz_array_destroy() \u00b6 Object \u00b6 dvz_obj_init() \u00b6 dvz_obj_created() \u00b6 dvz_obj_destroyed() \u00b6 dvz_obj_is_created() \u00b6 Container \u00b6 dvz_container() \u00b6 dvz_container_delete_if_destroyed() \u00b6 dvz_container_alloc() \u00b6 dvz_container_get() \u00b6 dvz_container_iterator() \u00b6 dvz_container_iter() \u00b6 dvz_container_destroy() \u00b6 I/O \u00b6 dvz_write_png() \u00b6 dvz_write_ppm() \u00b6 dvz_read_file() \u00b6 dvz_read_npy() \u00b6 dvz_read_ppm() \u00b6 Thread \u00b6 dvz_thread() \u00b6 dvz_thread_lock() \u00b6 dvz_thread_unlock() \u00b6 dvz_thread_join() \u00b6 FIFO queue \u00b6 dvz_fifo() \u00b6 dvz_fifo_enqueue() \u00b6 dvz_fifo_dequeue() \u00b6 dvz_fifo_size() \u00b6 dvz_fifo_discard() \u00b6 dvz_fifo_reset() \u00b6 dvz_fifo_destroy() \u00b6 Mesh \u00b6 dvz_mesh() \u00b6 dvz_mesh_obj() \u00b6 dvz_mesh_grid() \u00b6 dvz_mesh_surface() \u00b6 dvz_mesh_cube() \u00b6 dvz_mesh_sphere() \u00b6 dvz_mesh_cylinder() \u00b6 dvz_mesh_cone() \u00b6 dvz_mesh_square() \u00b6 dvz_mesh_disc() \u00b6 dvz_mesh_normalize() \u00b6 dvz_mesh_destroy() \u00b6 Mesh transform \u00b6 dvz_mesh_transform_reset() \u00b6 dvz_mesh_transform_add() \u00b6 dvz_mesh_translate() \u00b6 dvz_mesh_scale() \u00b6 dvz_mesh_rotate() \u00b6 dvz_mesh_transform() \u00b6 Random \u00b6 dvz_rand_byte() \u00b6 dvz_rand_float() \u00b6 dvz_rand_normal() \u00b6 Misc \u00b6 dvz_sleep() \u00b6 dvz_next_pow2() \u00b6","title":"Misc API"},{"location":"api/misc/#misc-api","text":"","title":"Misc API"},{"location":"api/misc/#array","text":"","title":"Array"},{"location":"api/misc/#dvz_array","text":"","title":"dvz_array()"},{"location":"api/misc/#dvz_array_point","text":"","title":"dvz_array_point()"},{"location":"api/misc/#dvz_array_wrap","text":"","title":"dvz_array_wrap()"},{"location":"api/misc/#dvz_array_struct","text":"","title":"dvz_array_struct()"},{"location":"api/misc/#dvz_array_3d","text":"","title":"dvz_array_3D()"},{"location":"api/misc/#dvz_array_resize","text":"","title":"dvz_array_resize()"},{"location":"api/misc/#dvz_array_clear","text":"","title":"dvz_array_clear()"},{"location":"api/misc/#dvz_array_reshape","text":"","title":"dvz_array_reshape()"},{"location":"api/misc/#dvz_array_data","text":"","title":"dvz_array_data()"},{"location":"api/misc/#dvz_array_item","text":"","title":"dvz_array_item()"},{"location":"api/misc/#dvz_array_column","text":"","title":"dvz_array_column()"},{"location":"api/misc/#dvz_array_insert","text":"","title":"dvz_array_insert()"},{"location":"api/misc/#dvz_array_copy_region","text":"","title":"dvz_array_copy_region()"},{"location":"api/misc/#dvz_array_destroy","text":"","title":"dvz_array_destroy()"},{"location":"api/misc/#object","text":"","title":"Object"},{"location":"api/misc/#dvz_obj_init","text":"","title":"dvz_obj_init()"},{"location":"api/misc/#dvz_obj_created","text":"","title":"dvz_obj_created()"},{"location":"api/misc/#dvz_obj_destroyed","text":"","title":"dvz_obj_destroyed()"},{"location":"api/misc/#dvz_obj_is_created","text":"","title":"dvz_obj_is_created()"},{"location":"api/misc/#container","text":"","title":"Container"},{"location":"api/misc/#dvz_container","text":"","title":"dvz_container()"},{"location":"api/misc/#dvz_container_delete_if_destroyed","text":"","title":"dvz_container_delete_if_destroyed()"},{"location":"api/misc/#dvz_container_alloc","text":"","title":"dvz_container_alloc()"},{"location":"api/misc/#dvz_container_get","text":"","title":"dvz_container_get()"},{"location":"api/misc/#dvz_container_iterator","text":"","title":"dvz_container_iterator()"},{"location":"api/misc/#dvz_container_iter","text":"","title":"dvz_container_iter()"},{"location":"api/misc/#dvz_container_destroy","text":"","title":"dvz_container_destroy()"},{"location":"api/misc/#io","text":"","title":"I/O"},{"location":"api/misc/#dvz_write_png","text":"","title":"dvz_write_png()"},{"location":"api/misc/#dvz_write_ppm","text":"","title":"dvz_write_ppm()"},{"location":"api/misc/#dvz_read_file","text":"","title":"dvz_read_file()"},{"location":"api/misc/#dvz_read_npy","text":"","title":"dvz_read_npy()"},{"location":"api/misc/#dvz_read_ppm","text":"","title":"dvz_read_ppm()"},{"location":"api/misc/#thread","text":"","title":"Thread"},{"location":"api/misc/#dvz_thread","text":"","title":"dvz_thread()"},{"location":"api/misc/#dvz_thread_lock","text":"","title":"dvz_thread_lock()"},{"location":"api/misc/#dvz_thread_unlock","text":"","title":"dvz_thread_unlock()"},{"location":"api/misc/#dvz_thread_join","text":"","title":"dvz_thread_join()"},{"location":"api/misc/#fifo-queue","text":"","title":"FIFO queue"},{"location":"api/misc/#dvz_fifo","text":"","title":"dvz_fifo()"},{"location":"api/misc/#dvz_fifo_enqueue","text":"","title":"dvz_fifo_enqueue()"},{"location":"api/misc/#dvz_fifo_dequeue","text":"","title":"dvz_fifo_dequeue()"},{"location":"api/misc/#dvz_fifo_size","text":"","title":"dvz_fifo_size()"},{"location":"api/misc/#dvz_fifo_discard","text":"","title":"dvz_fifo_discard()"},{"location":"api/misc/#dvz_fifo_reset","text":"","title":"dvz_fifo_reset()"},{"location":"api/misc/#dvz_fifo_destroy","text":"","title":"dvz_fifo_destroy()"},{"location":"api/misc/#mesh","text":"","title":"Mesh"},{"location":"api/misc/#dvz_mesh","text":"","title":"dvz_mesh()"},{"location":"api/misc/#dvz_mesh_obj","text":"","title":"dvz_mesh_obj()"},{"location":"api/misc/#dvz_mesh_grid","text":"","title":"dvz_mesh_grid()"},{"location":"api/misc/#dvz_mesh_surface","text":"","title":"dvz_mesh_surface()"},{"location":"api/misc/#dvz_mesh_cube","text":"","title":"dvz_mesh_cube()"},{"location":"api/misc/#dvz_mesh_sphere","text":"","title":"dvz_mesh_sphere()"},{"location":"api/misc/#dvz_mesh_cylinder","text":"","title":"dvz_mesh_cylinder()"},{"location":"api/misc/#dvz_mesh_cone","text":"","title":"dvz_mesh_cone()"},{"location":"api/misc/#dvz_mesh_square","text":"","title":"dvz_mesh_square()"},{"location":"api/misc/#dvz_mesh_disc","text":"","title":"dvz_mesh_disc()"},{"location":"api/misc/#dvz_mesh_normalize","text":"","title":"dvz_mesh_normalize()"},{"location":"api/misc/#dvz_mesh_destroy","text":"","title":"dvz_mesh_destroy()"},{"location":"api/misc/#mesh-transform","text":"","title":"Mesh transform"},{"location":"api/misc/#dvz_mesh_transform_reset","text":"","title":"dvz_mesh_transform_reset()"},{"location":"api/misc/#dvz_mesh_transform_add","text":"","title":"dvz_mesh_transform_add()"},{"location":"api/misc/#dvz_mesh_translate","text":"","title":"dvz_mesh_translate()"},{"location":"api/misc/#dvz_mesh_scale","text":"","title":"dvz_mesh_scale()"},{"location":"api/misc/#dvz_mesh_rotate","text":"","title":"dvz_mesh_rotate()"},{"location":"api/misc/#dvz_mesh_transform","text":"","title":"dvz_mesh_transform()"},{"location":"api/misc/#random","text":"","title":"Random"},{"location":"api/misc/#dvz_rand_byte","text":"","title":"dvz_rand_byte()"},{"location":"api/misc/#dvz_rand_float","text":"","title":"dvz_rand_float()"},{"location":"api/misc/#dvz_rand_normal","text":"","title":"dvz_rand_normal()"},{"location":"api/misc/#misc","text":"","title":"Misc"},{"location":"api/misc/#dvz_sleep","text":"","title":"dvz_sleep()"},{"location":"api/misc/#dvz_next_pow2","text":"","title":"dvz_next_pow2()"},{"location":"api/scene/","text":"Scene API \u00b6 App, canvas, main loop \u00b6 dvz_app() \u00b6 dvz_canvas() \u00b6 dvz_canvas_offscreen() \u00b6 dvz_canvas_dpi_scaling() \u00b6 dvz_scene() \u00b6 dvz_app_run() \u00b6 dvz_scene_destroy() \u00b6 dvz_canvas_destroy() \u00b6 dvz_app_destroy() \u00b6 Set panels, add visuals \u00b6 dvz_scene_panel() \u00b6 dvz_scene_visual() \u00b6 Custom visuals and graphics \u00b6 dvz_blank_graphics() \u00b6 dvz_custom_graphics() \u00b6 dvz_blank_visual() \u00b6 dvz_custom_visual() \u00b6 Grid and panels \u00b6 dvz_grid() \u00b6 dvz_grid_destroy() \u00b6 dvz_panel() \u00b6 dvz_panel_update() \u00b6 dvz_panel_margins() \u00b6 dvz_panel_unit() \u00b6 dvz_panel_mode() \u00b6 dvz_panel_visual() \u00b6 dvz_panel_pos() \u00b6 dvz_panel_size() \u00b6 dvz_panel_span() \u00b6 dvz_panel_cell() \u00b6 dvz_panel_transpose() \u00b6 dvz_panel_contains() \u00b6 dvz_panel_at() \u00b6 dvz_panel_destroy() \u00b6 dvz_panel_viewport() \u00b6 Colormaps \u00b6 dvz_colormap() \u00b6 dvz_colormap_idx() \u00b6 dvz_colormap_uv() \u00b6 dvz_colormap_scale() \u00b6 dvz_colormap_array() \u00b6 dvz_colormap_packuv() \u00b6 dvz_colormap_extent() \u00b6","title":"Scene API"},{"location":"api/scene/#scene-api","text":"","title":"Scene API"},{"location":"api/scene/#app-canvas-main-loop","text":"","title":"App, canvas, main loop"},{"location":"api/scene/#dvz_app","text":"","title":"dvz_app()"},{"location":"api/scene/#dvz_canvas","text":"","title":"dvz_canvas()"},{"location":"api/scene/#dvz_canvas_offscreen","text":"","title":"dvz_canvas_offscreen()"},{"location":"api/scene/#dvz_canvas_dpi_scaling","text":"","title":"dvz_canvas_dpi_scaling()"},{"location":"api/scene/#dvz_scene","text":"","title":"dvz_scene()"},{"location":"api/scene/#dvz_app_run","text":"","title":"dvz_app_run()"},{"location":"api/scene/#dvz_scene_destroy","text":"","title":"dvz_scene_destroy()"},{"location":"api/scene/#dvz_canvas_destroy","text":"","title":"dvz_canvas_destroy()"},{"location":"api/scene/#dvz_app_destroy","text":"","title":"dvz_app_destroy()"},{"location":"api/scene/#set-panels-add-visuals","text":"","title":"Set panels, add visuals"},{"location":"api/scene/#dvz_scene_panel","text":"","title":"dvz_scene_panel()"},{"location":"api/scene/#dvz_scene_visual","text":"","title":"dvz_scene_visual()"},{"location":"api/scene/#custom-visuals-and-graphics","text":"","title":"Custom visuals and graphics"},{"location":"api/scene/#dvz_blank_graphics","text":"","title":"dvz_blank_graphics()"},{"location":"api/scene/#dvz_custom_graphics","text":"","title":"dvz_custom_graphics()"},{"location":"api/scene/#dvz_blank_visual","text":"","title":"dvz_blank_visual()"},{"location":"api/scene/#dvz_custom_visual","text":"","title":"dvz_custom_visual()"},{"location":"api/scene/#grid-and-panels","text":"","title":"Grid and panels"},{"location":"api/scene/#dvz_grid","text":"","title":"dvz_grid()"},{"location":"api/scene/#dvz_grid_destroy","text":"","title":"dvz_grid_destroy()"},{"location":"api/scene/#dvz_panel","text":"","title":"dvz_panel()"},{"location":"api/scene/#dvz_panel_update","text":"","title":"dvz_panel_update()"},{"location":"api/scene/#dvz_panel_margins","text":"","title":"dvz_panel_margins()"},{"location":"api/scene/#dvz_panel_unit","text":"","title":"dvz_panel_unit()"},{"location":"api/scene/#dvz_panel_mode","text":"","title":"dvz_panel_mode()"},{"location":"api/scene/#dvz_panel_visual","text":"","title":"dvz_panel_visual()"},{"location":"api/scene/#dvz_panel_pos","text":"","title":"dvz_panel_pos()"},{"location":"api/scene/#dvz_panel_size","text":"","title":"dvz_panel_size()"},{"location":"api/scene/#dvz_panel_span","text":"","title":"dvz_panel_span()"},{"location":"api/scene/#dvz_panel_cell","text":"","title":"dvz_panel_cell()"},{"location":"api/scene/#dvz_panel_transpose","text":"","title":"dvz_panel_transpose()"},{"location":"api/scene/#dvz_panel_contains","text":"","title":"dvz_panel_contains()"},{"location":"api/scene/#dvz_panel_at","text":"","title":"dvz_panel_at()"},{"location":"api/scene/#dvz_panel_destroy","text":"","title":"dvz_panel_destroy()"},{"location":"api/scene/#dvz_panel_viewport","text":"","title":"dvz_panel_viewport()"},{"location":"api/scene/#colormaps","text":"","title":"Colormaps"},{"location":"api/scene/#dvz_colormap","text":"","title":"dvz_colormap()"},{"location":"api/scene/#dvz_colormap_idx","text":"","title":"dvz_colormap_idx()"},{"location":"api/scene/#dvz_colormap_uv","text":"","title":"dvz_colormap_uv()"},{"location":"api/scene/#dvz_colormap_scale","text":"","title":"dvz_colormap_scale()"},{"location":"api/scene/#dvz_colormap_array","text":"","title":"dvz_colormap_array()"},{"location":"api/scene/#dvz_colormap_packuv","text":"","title":"dvz_colormap_packuv()"},{"location":"api/scene/#dvz_colormap_extent","text":"","title":"dvz_colormap_extent()"},{"location":"api/visual/","text":"Visual API \u00b6 Visual creation and destruction \u00b6 dvz_visual() \u00b6 dvz_visual_graphics() \u00b6 dvz_visual_compute() \u00b6 dvz_visual_destroy() \u00b6 Visual data \u00b6 dvz_visual_group() \u00b6 dvz_visual_data() \u00b6 dvz_visual_data_partial() \u00b6 dvz_visual_data_append() \u00b6 dvz_visual_data_source() \u00b6 dvz_visual_buffer() \u00b6 dvz_visual_texture() \u00b6 Visual sources and props \u00b6 dvz_visual_source() \u00b6 dvz_visual_source_share() \u00b6 dvz_visual_prop() \u00b6 dvz_visual_prop_default() \u00b6 dvz_visual_prop_copy() \u00b6 dvz_visual_prop_cast() \u00b6 Visual fill utils and callback \u00b6 dvz_visual_fill_callback() \u00b6 dvz_visual_fill_event() \u00b6 dvz_visual_fill_begin() \u00b6 dvz_visual_fill_end() \u00b6 dvz_visual_callback_bake() \u00b6 Visual baking helpers \u00b6 dvz_source_get() \u00b6 dvz_prop_get() \u00b6 dvz_prop_array() \u00b6 dvz_prop_size() \u00b6 dvz_prop_item() \u00b6 Graphics pipeline \u00b6 dvz_graphics_callback() \u00b6 dvz_graphics_data() \u00b6 dvz_graphics_alloc() \u00b6 dvz_graphics_append() \u00b6 dvz_graphics_builtin() \u00b6 Visual internal system \u00b6 dvz_visual_update() \u00b6","title":"Visual API"},{"location":"api/visual/#visual-api","text":"","title":"Visual API"},{"location":"api/visual/#visual-creation-and-destruction","text":"","title":"Visual creation and destruction"},{"location":"api/visual/#dvz_visual","text":"","title":"dvz_visual()"},{"location":"api/visual/#dvz_visual_graphics","text":"","title":"dvz_visual_graphics()"},{"location":"api/visual/#dvz_visual_compute","text":"","title":"dvz_visual_compute()"},{"location":"api/visual/#dvz_visual_destroy","text":"","title":"dvz_visual_destroy()"},{"location":"api/visual/#visual-data","text":"","title":"Visual data"},{"location":"api/visual/#dvz_visual_group","text":"","title":"dvz_visual_group()"},{"location":"api/visual/#dvz_visual_data","text":"","title":"dvz_visual_data()"},{"location":"api/visual/#dvz_visual_data_partial","text":"","title":"dvz_visual_data_partial()"},{"location":"api/visual/#dvz_visual_data_append","text":"","title":"dvz_visual_data_append()"},{"location":"api/visual/#dvz_visual_data_source","text":"","title":"dvz_visual_data_source()"},{"location":"api/visual/#dvz_visual_buffer","text":"","title":"dvz_visual_buffer()"},{"location":"api/visual/#dvz_visual_texture","text":"","title":"dvz_visual_texture()"},{"location":"api/visual/#visual-sources-and-props","text":"","title":"Visual sources and props"},{"location":"api/visual/#dvz_visual_source","text":"","title":"dvz_visual_source()"},{"location":"api/visual/#dvz_visual_source_share","text":"","title":"dvz_visual_source_share()"},{"location":"api/visual/#dvz_visual_prop","text":"","title":"dvz_visual_prop()"},{"location":"api/visual/#dvz_visual_prop_default","text":"","title":"dvz_visual_prop_default()"},{"location":"api/visual/#dvz_visual_prop_copy","text":"","title":"dvz_visual_prop_copy()"},{"location":"api/visual/#dvz_visual_prop_cast","text":"","title":"dvz_visual_prop_cast()"},{"location":"api/visual/#visual-fill-utils-and-callback","text":"","title":"Visual fill utils and callback"},{"location":"api/visual/#dvz_visual_fill_callback","text":"","title":"dvz_visual_fill_callback()"},{"location":"api/visual/#dvz_visual_fill_event","text":"","title":"dvz_visual_fill_event()"},{"location":"api/visual/#dvz_visual_fill_begin","text":"","title":"dvz_visual_fill_begin()"},{"location":"api/visual/#dvz_visual_fill_end","text":"","title":"dvz_visual_fill_end()"},{"location":"api/visual/#dvz_visual_callback_bake","text":"","title":"dvz_visual_callback_bake()"},{"location":"api/visual/#visual-baking-helpers","text":"","title":"Visual baking helpers"},{"location":"api/visual/#dvz_source_get","text":"","title":"dvz_source_get()"},{"location":"api/visual/#dvz_prop_get","text":"","title":"dvz_prop_get()"},{"location":"api/visual/#dvz_prop_array","text":"","title":"dvz_prop_array()"},{"location":"api/visual/#dvz_prop_size","text":"","title":"dvz_prop_size()"},{"location":"api/visual/#dvz_prop_item","text":"","title":"dvz_prop_item()"},{"location":"api/visual/#graphics-pipeline","text":"","title":"Graphics pipeline"},{"location":"api/visual/#dvz_graphics_callback","text":"","title":"dvz_graphics_callback()"},{"location":"api/visual/#dvz_graphics_data","text":"","title":"dvz_graphics_data()"},{"location":"api/visual/#dvz_graphics_alloc","text":"","title":"dvz_graphics_alloc()"},{"location":"api/visual/#dvz_graphics_append","text":"","title":"dvz_graphics_append()"},{"location":"api/visual/#dvz_graphics_builtin","text":"","title":"dvz_graphics_builtin()"},{"location":"api/visual/#visual-internal-system","text":"","title":"Visual internal system"},{"location":"api/visual/#dvz_visual_update","text":"","title":"dvz_visual_update()"},{"location":"api/vklite/","text":"vklite API \u00b6 GPU \u00b6 dvz_gpu() \u00b6 dvz_gpu_request_features() \u00b6 dvz_gpu_queue() \u00b6 dvz_gpu_create() \u00b6 dvz_gpu_destroy() \u00b6 Coarse synchronization \u00b6 dvz_queue_wait() \u00b6 dvz_app_wait() \u00b6 dvz_gpu_wait() \u00b6 Window \u00b6 dvz_window() \u00b6 dvz_window_get_size() \u00b6 dvz_window_poll_events() \u00b6 dvz_window_destroy() \u00b6 Swapchain \u00b6 dvz_swapchain() \u00b6 dvz_swapchain_format() \u00b6 dvz_swapchain_present_mode() \u00b6 dvz_swapchain_requested_size() \u00b6 dvz_swapchain_create() \u00b6 dvz_swapchain_recreate() \u00b6 dvz_swapchain_acquire() \u00b6 dvz_swapchain_present() \u00b6 dvz_swapchain_destroy() \u00b6 Command buffers \u00b6 dvz_commands() \u00b6 dvz_cmd_begin() \u00b6 dvz_cmd_end() \u00b6 dvz_cmd_reset() \u00b6 dvz_cmd_free() \u00b6 dvz_cmd_submit_sync() \u00b6 dvz_commands_destroy() \u00b6 GPU buffer \u00b6 dvz_buffer() \u00b6 dvz_buffer_size() \u00b6 dvz_buffer_type() \u00b6 dvz_buffer_usage() \u00b6 dvz_buffer_memory() \u00b6 dvz_buffer_queue_access() \u00b6 dvz_buffer_create() \u00b6 dvz_buffer_resize() \u00b6 dvz_buffer_map() \u00b6 dvz_buffer_unmap() \u00b6 dvz_buffer_download() \u00b6 dvz_buffer_upload() \u00b6 dvz_buffer_destroy() \u00b6 dvz_buffer_regions() \u00b6 dvz_buffer_regions_map() \u00b6 dvz_buffer_regions_unmap() \u00b6 dvz_buffer_regions_upload() \u00b6 GPU images \u00b6 dvz_images() \u00b6 dvz_images_format() \u00b6 dvz_images_layout() \u00b6 dvz_images_size() \u00b6 dvz_images_tiling() \u00b6 dvz_images_usage() \u00b6 dvz_images_memory() \u00b6 dvz_images_aspect() \u00b6 dvz_images_queue_access() \u00b6 dvz_images_create() \u00b6 dvz_images_resize() \u00b6 dvz_images_transition() \u00b6 dvz_images_download() \u00b6 dvz_images_destroy() \u00b6 Sampler \u00b6 dvz_sampler() \u00b6 dvz_sampler_min_filter() \u00b6 dvz_sampler_mag_filter() \u00b6 dvz_sampler_address_mode() \u00b6 dvz_sampler_create() \u00b6 dvz_sampler_destroy() \u00b6 Pipeline slots \u00b6 Vulkan terminology: descriptor set layout. dvz_slots() \u00b6 dvz_slots_binding() \u00b6 dvz_slots_push() \u00b6 dvz_slots_create() \u00b6 dvz_slots_destroy() \u00b6 Pipeline bindings \u00b6 Vulkan terminology: descriptor sets dvz_bindings() \u00b6 dvz_bindings_buffer() \u00b6 dvz_bindings_texture() \u00b6 dvz_bindings_update() \u00b6 dvz_bindings_destroy() \u00b6 Graphics pipeline \u00b6 dvz_graphics() \u00b6 dvz_graphics_renderpass() \u00b6 dvz_graphics_topology() \u00b6 dvz_graphics_shader_glsl() \u00b6 dvz_graphics_shader_spirv() \u00b6 dvz_graphics_shader() \u00b6 dvz_graphics_vertex_binding() \u00b6 dvz_graphics_vertex_attr() \u00b6 dvz_graphics_blend() \u00b6 dvz_graphics_depth_test() \u00b6 dvz_graphics_polygon_mode() \u00b6 dvz_graphics_cull_mode() \u00b6 dvz_graphics_front_face() \u00b6 dvz_graphics_create() \u00b6 dvz_graphics_slot() \u00b6 dvz_graphics_push() \u00b6 dvz_graphics_destroy() \u00b6 Compute pipeline \u00b6 dvz_compute() \u00b6 dvz_compute_create() \u00b6 dvz_compute_code() \u00b6 dvz_compute_slot() \u00b6 dvz_compute_push() \u00b6 dvz_compute_bindings() \u00b6 dvz_compute_destroy() \u00b6 Barrier \u00b6 dvz_barrier() \u00b6 dvz_barrier_stages() \u00b6 dvz_barrier_buffer() \u00b6 dvz_barrier_buffer_queue() \u00b6 dvz_barrier_buffer_access() \u00b6 dvz_barrier_images() \u00b6 dvz_barrier_images_layout() \u00b6 dvz_barrier_images_queue() \u00b6 dvz_barrier_images_access() \u00b6 Semaphores \u00b6 dvz_semaphores() \u00b6 dvz_semaphores_destroy() \u00b6 Fences \u00b6 dvz_fences() \u00b6 dvz_fences_copy() \u00b6 dvz_fences_wait() \u00b6 dvz_fences_ready() \u00b6 dvz_fences_reset() \u00b6 dvz_fences_destroy() \u00b6 Renderpass \u00b6 dvz_renderpass() \u00b6 dvz_renderpass_clear() \u00b6 dvz_renderpass_attachment() \u00b6 dvz_renderpass_attachment_layout() \u00b6 dvz_renderpass_attachment_ops() \u00b6 dvz_renderpass_subpass_attachment() \u00b6 dvz_renderpass_subpass_dependency() \u00b6 dvz_renderpass_subpass_dependency_access() \u00b6 dvz_renderpass_subpass_dependency_stage() \u00b6 dvz_renderpass_create() \u00b6 dvz_renderpass_destroy() \u00b6 Framebuffers \u00b6 dvz_framebuffers() \u00b6 dvz_framebuffers_attachment() \u00b6 dvz_framebuffers_create() \u00b6 dvz_framebuffers_destroy() \u00b6 Submit \u00b6 dvz_submit() \u00b6 dvz_submit_commands() \u00b6 dvz_submit_wait_semaphores() \u00b6 dvz_submit_signal_semaphores() \u00b6 dvz_submit_send() \u00b6 dvz_submit_reset() \u00b6 Command buffer recording \u00b6 dvz_cmd_begin_renderpass() \u00b6 dvz_cmd_end_renderpass() \u00b6 dvz_cmd_compute() \u00b6 dvz_cmd_barrier() \u00b6 dvz_cmd_copy_buffer_to_image() \u00b6 dvz_cmd_copy_image_to_buffer() \u00b6 dvz_cmd_copy_image() \u00b6 dvz_cmd_viewport() \u00b6 dvz_cmd_bind_graphics() \u00b6 dvz_cmd_bind_vertex_buffer() \u00b6 dvz_cmd_bind_index_buffer() \u00b6 dvz_cmd_draw() \u00b6 dvz_cmd_draw_indexed() \u00b6 dvz_cmd_draw_indirect() \u00b6 dvz_cmd_draw_indexed_indirect() \u00b6 dvz_cmd_copy_buffer() \u00b6 dvz_cmd_push() \u00b6","title":"vklite API"},{"location":"api/vklite/#vklite-api","text":"","title":"vklite API"},{"location":"api/vklite/#gpu","text":"","title":"GPU"},{"location":"api/vklite/#dvz_gpu","text":"","title":"dvz_gpu()"},{"location":"api/vklite/#dvz_gpu_request_features","text":"","title":"dvz_gpu_request_features()"},{"location":"api/vklite/#dvz_gpu_queue","text":"","title":"dvz_gpu_queue()"},{"location":"api/vklite/#dvz_gpu_create","text":"","title":"dvz_gpu_create()"},{"location":"api/vklite/#dvz_gpu_destroy","text":"","title":"dvz_gpu_destroy()"},{"location":"api/vklite/#coarse-synchronization","text":"","title":"Coarse synchronization"},{"location":"api/vklite/#dvz_queue_wait","text":"","title":"dvz_queue_wait()"},{"location":"api/vklite/#dvz_app_wait","text":"","title":"dvz_app_wait()"},{"location":"api/vklite/#dvz_gpu_wait","text":"","title":"dvz_gpu_wait()"},{"location":"api/vklite/#window","text":"","title":"Window"},{"location":"api/vklite/#dvz_window","text":"","title":"dvz_window()"},{"location":"api/vklite/#dvz_window_get_size","text":"","title":"dvz_window_get_size()"},{"location":"api/vklite/#dvz_window_poll_events","text":"","title":"dvz_window_poll_events()"},{"location":"api/vklite/#dvz_window_destroy","text":"","title":"dvz_window_destroy()"},{"location":"api/vklite/#swapchain","text":"","title":"Swapchain"},{"location":"api/vklite/#dvz_swapchain","text":"","title":"dvz_swapchain()"},{"location":"api/vklite/#dvz_swapchain_format","text":"","title":"dvz_swapchain_format()"},{"location":"api/vklite/#dvz_swapchain_present_mode","text":"","title":"dvz_swapchain_present_mode()"},{"location":"api/vklite/#dvz_swapchain_requested_size","text":"","title":"dvz_swapchain_requested_size()"},{"location":"api/vklite/#dvz_swapchain_create","text":"","title":"dvz_swapchain_create()"},{"location":"api/vklite/#dvz_swapchain_recreate","text":"","title":"dvz_swapchain_recreate()"},{"location":"api/vklite/#dvz_swapchain_acquire","text":"","title":"dvz_swapchain_acquire()"},{"location":"api/vklite/#dvz_swapchain_present","text":"","title":"dvz_swapchain_present()"},{"location":"api/vklite/#dvz_swapchain_destroy","text":"","title":"dvz_swapchain_destroy()"},{"location":"api/vklite/#command-buffers","text":"","title":"Command buffers"},{"location":"api/vklite/#dvz_commands","text":"","title":"dvz_commands()"},{"location":"api/vklite/#dvz_cmd_begin","text":"","title":"dvz_cmd_begin()"},{"location":"api/vklite/#dvz_cmd_end","text":"","title":"dvz_cmd_end()"},{"location":"api/vklite/#dvz_cmd_reset","text":"","title":"dvz_cmd_reset()"},{"location":"api/vklite/#dvz_cmd_free","text":"","title":"dvz_cmd_free()"},{"location":"api/vklite/#dvz_cmd_submit_sync","text":"","title":"dvz_cmd_submit_sync()"},{"location":"api/vklite/#dvz_commands_destroy","text":"","title":"dvz_commands_destroy()"},{"location":"api/vklite/#gpu-buffer","text":"","title":"GPU buffer"},{"location":"api/vklite/#dvz_buffer","text":"","title":"dvz_buffer()"},{"location":"api/vklite/#dvz_buffer_size","text":"","title":"dvz_buffer_size()"},{"location":"api/vklite/#dvz_buffer_type","text":"","title":"dvz_buffer_type()"},{"location":"api/vklite/#dvz_buffer_usage","text":"","title":"dvz_buffer_usage()"},{"location":"api/vklite/#dvz_buffer_memory","text":"","title":"dvz_buffer_memory()"},{"location":"api/vklite/#dvz_buffer_queue_access","text":"","title":"dvz_buffer_queue_access()"},{"location":"api/vklite/#dvz_buffer_create","text":"","title":"dvz_buffer_create()"},{"location":"api/vklite/#dvz_buffer_resize","text":"","title":"dvz_buffer_resize()"},{"location":"api/vklite/#dvz_buffer_map","text":"","title":"dvz_buffer_map()"},{"location":"api/vklite/#dvz_buffer_unmap","text":"","title":"dvz_buffer_unmap()"},{"location":"api/vklite/#dvz_buffer_download","text":"","title":"dvz_buffer_download()"},{"location":"api/vklite/#dvz_buffer_upload","text":"","title":"dvz_buffer_upload()"},{"location":"api/vklite/#dvz_buffer_destroy","text":"","title":"dvz_buffer_destroy()"},{"location":"api/vklite/#dvz_buffer_regions","text":"","title":"dvz_buffer_regions()"},{"location":"api/vklite/#dvz_buffer_regions_map","text":"","title":"dvz_buffer_regions_map()"},{"location":"api/vklite/#dvz_buffer_regions_unmap","text":"","title":"dvz_buffer_regions_unmap()"},{"location":"api/vklite/#dvz_buffer_regions_upload","text":"","title":"dvz_buffer_regions_upload()"},{"location":"api/vklite/#gpu-images","text":"","title":"GPU images"},{"location":"api/vklite/#dvz_images","text":"","title":"dvz_images()"},{"location":"api/vklite/#dvz_images_format","text":"","title":"dvz_images_format()"},{"location":"api/vklite/#dvz_images_layout","text":"","title":"dvz_images_layout()"},{"location":"api/vklite/#dvz_images_size","text":"","title":"dvz_images_size()"},{"location":"api/vklite/#dvz_images_tiling","text":"","title":"dvz_images_tiling()"},{"location":"api/vklite/#dvz_images_usage","text":"","title":"dvz_images_usage()"},{"location":"api/vklite/#dvz_images_memory","text":"","title":"dvz_images_memory()"},{"location":"api/vklite/#dvz_images_aspect","text":"","title":"dvz_images_aspect()"},{"location":"api/vklite/#dvz_images_queue_access","text":"","title":"dvz_images_queue_access()"},{"location":"api/vklite/#dvz_images_create","text":"","title":"dvz_images_create()"},{"location":"api/vklite/#dvz_images_resize","text":"","title":"dvz_images_resize()"},{"location":"api/vklite/#dvz_images_transition","text":"","title":"dvz_images_transition()"},{"location":"api/vklite/#dvz_images_download","text":"","title":"dvz_images_download()"},{"location":"api/vklite/#dvz_images_destroy","text":"","title":"dvz_images_destroy()"},{"location":"api/vklite/#sampler","text":"","title":"Sampler"},{"location":"api/vklite/#dvz_sampler","text":"","title":"dvz_sampler()"},{"location":"api/vklite/#dvz_sampler_min_filter","text":"","title":"dvz_sampler_min_filter()"},{"location":"api/vklite/#dvz_sampler_mag_filter","text":"","title":"dvz_sampler_mag_filter()"},{"location":"api/vklite/#dvz_sampler_address_mode","text":"","title":"dvz_sampler_address_mode()"},{"location":"api/vklite/#dvz_sampler_create","text":"","title":"dvz_sampler_create()"},{"location":"api/vklite/#dvz_sampler_destroy","text":"","title":"dvz_sampler_destroy()"},{"location":"api/vklite/#pipeline-slots","text":"Vulkan terminology: descriptor set layout.","title":"Pipeline slots"},{"location":"api/vklite/#dvz_slots","text":"","title":"dvz_slots()"},{"location":"api/vklite/#dvz_slots_binding","text":"","title":"dvz_slots_binding()"},{"location":"api/vklite/#dvz_slots_push","text":"","title":"dvz_slots_push()"},{"location":"api/vklite/#dvz_slots_create","text":"","title":"dvz_slots_create()"},{"location":"api/vklite/#dvz_slots_destroy","text":"","title":"dvz_slots_destroy()"},{"location":"api/vklite/#pipeline-bindings","text":"Vulkan terminology: descriptor sets","title":"Pipeline bindings"},{"location":"api/vklite/#dvz_bindings","text":"","title":"dvz_bindings()"},{"location":"api/vklite/#dvz_bindings_buffer","text":"","title":"dvz_bindings_buffer()"},{"location":"api/vklite/#dvz_bindings_texture","text":"","title":"dvz_bindings_texture()"},{"location":"api/vklite/#dvz_bindings_update","text":"","title":"dvz_bindings_update()"},{"location":"api/vklite/#dvz_bindings_destroy","text":"","title":"dvz_bindings_destroy()"},{"location":"api/vklite/#graphics-pipeline","text":"","title":"Graphics pipeline"},{"location":"api/vklite/#dvz_graphics","text":"","title":"dvz_graphics()"},{"location":"api/vklite/#dvz_graphics_renderpass","text":"","title":"dvz_graphics_renderpass()"},{"location":"api/vklite/#dvz_graphics_topology","text":"","title":"dvz_graphics_topology()"},{"location":"api/vklite/#dvz_graphics_shader_glsl","text":"","title":"dvz_graphics_shader_glsl()"},{"location":"api/vklite/#dvz_graphics_shader_spirv","text":"","title":"dvz_graphics_shader_spirv()"},{"location":"api/vklite/#dvz_graphics_shader","text":"","title":"dvz_graphics_shader()"},{"location":"api/vklite/#dvz_graphics_vertex_binding","text":"","title":"dvz_graphics_vertex_binding()"},{"location":"api/vklite/#dvz_graphics_vertex_attr","text":"","title":"dvz_graphics_vertex_attr()"},{"location":"api/vklite/#dvz_graphics_blend","text":"","title":"dvz_graphics_blend()"},{"location":"api/vklite/#dvz_graphics_depth_test","text":"","title":"dvz_graphics_depth_test()"},{"location":"api/vklite/#dvz_graphics_polygon_mode","text":"","title":"dvz_graphics_polygon_mode()"},{"location":"api/vklite/#dvz_graphics_cull_mode","text":"","title":"dvz_graphics_cull_mode()"},{"location":"api/vklite/#dvz_graphics_front_face","text":"","title":"dvz_graphics_front_face()"},{"location":"api/vklite/#dvz_graphics_create","text":"","title":"dvz_graphics_create()"},{"location":"api/vklite/#dvz_graphics_slot","text":"","title":"dvz_graphics_slot()"},{"location":"api/vklite/#dvz_graphics_push","text":"","title":"dvz_graphics_push()"},{"location":"api/vklite/#dvz_graphics_destroy","text":"","title":"dvz_graphics_destroy()"},{"location":"api/vklite/#compute-pipeline","text":"","title":"Compute pipeline"},{"location":"api/vklite/#dvz_compute","text":"","title":"dvz_compute()"},{"location":"api/vklite/#dvz_compute_create","text":"","title":"dvz_compute_create()"},{"location":"api/vklite/#dvz_compute_code","text":"","title":"dvz_compute_code()"},{"location":"api/vklite/#dvz_compute_slot","text":"","title":"dvz_compute_slot()"},{"location":"api/vklite/#dvz_compute_push","text":"","title":"dvz_compute_push()"},{"location":"api/vklite/#dvz_compute_bindings","text":"","title":"dvz_compute_bindings()"},{"location":"api/vklite/#dvz_compute_destroy","text":"","title":"dvz_compute_destroy()"},{"location":"api/vklite/#barrier","text":"","title":"Barrier"},{"location":"api/vklite/#dvz_barrier","text":"","title":"dvz_barrier()"},{"location":"api/vklite/#dvz_barrier_stages","text":"","title":"dvz_barrier_stages()"},{"location":"api/vklite/#dvz_barrier_buffer","text":"","title":"dvz_barrier_buffer()"},{"location":"api/vklite/#dvz_barrier_buffer_queue","text":"","title":"dvz_barrier_buffer_queue()"},{"location":"api/vklite/#dvz_barrier_buffer_access","text":"","title":"dvz_barrier_buffer_access()"},{"location":"api/vklite/#dvz_barrier_images","text":"","title":"dvz_barrier_images()"},{"location":"api/vklite/#dvz_barrier_images_layout","text":"","title":"dvz_barrier_images_layout()"},{"location":"api/vklite/#dvz_barrier_images_queue","text":"","title":"dvz_barrier_images_queue()"},{"location":"api/vklite/#dvz_barrier_images_access","text":"","title":"dvz_barrier_images_access()"},{"location":"api/vklite/#semaphores","text":"","title":"Semaphores"},{"location":"api/vklite/#dvz_semaphores","text":"","title":"dvz_semaphores()"},{"location":"api/vklite/#dvz_semaphores_destroy","text":"","title":"dvz_semaphores_destroy()"},{"location":"api/vklite/#fences","text":"","title":"Fences"},{"location":"api/vklite/#dvz_fences","text":"","title":"dvz_fences()"},{"location":"api/vklite/#dvz_fences_copy","text":"","title":"dvz_fences_copy()"},{"location":"api/vklite/#dvz_fences_wait","text":"","title":"dvz_fences_wait()"},{"location":"api/vklite/#dvz_fences_ready","text":"","title":"dvz_fences_ready()"},{"location":"api/vklite/#dvz_fences_reset","text":"","title":"dvz_fences_reset()"},{"location":"api/vklite/#dvz_fences_destroy","text":"","title":"dvz_fences_destroy()"},{"location":"api/vklite/#renderpass","text":"","title":"Renderpass"},{"location":"api/vklite/#dvz_renderpass","text":"","title":"dvz_renderpass()"},{"location":"api/vklite/#dvz_renderpass_clear","text":"","title":"dvz_renderpass_clear()"},{"location":"api/vklite/#dvz_renderpass_attachment","text":"","title":"dvz_renderpass_attachment()"},{"location":"api/vklite/#dvz_renderpass_attachment_layout","text":"","title":"dvz_renderpass_attachment_layout()"},{"location":"api/vklite/#dvz_renderpass_attachment_ops","text":"","title":"dvz_renderpass_attachment_ops()"},{"location":"api/vklite/#dvz_renderpass_subpass_attachment","text":"","title":"dvz_renderpass_subpass_attachment()"},{"location":"api/vklite/#dvz_renderpass_subpass_dependency","text":"","title":"dvz_renderpass_subpass_dependency()"},{"location":"api/vklite/#dvz_renderpass_subpass_dependency_access","text":"","title":"dvz_renderpass_subpass_dependency_access()"},{"location":"api/vklite/#dvz_renderpass_subpass_dependency_stage","text":"","title":"dvz_renderpass_subpass_dependency_stage()"},{"location":"api/vklite/#dvz_renderpass_create","text":"","title":"dvz_renderpass_create()"},{"location":"api/vklite/#dvz_renderpass_destroy","text":"","title":"dvz_renderpass_destroy()"},{"location":"api/vklite/#framebuffers","text":"","title":"Framebuffers"},{"location":"api/vklite/#dvz_framebuffers","text":"","title":"dvz_framebuffers()"},{"location":"api/vklite/#dvz_framebuffers_attachment","text":"","title":"dvz_framebuffers_attachment()"},{"location":"api/vklite/#dvz_framebuffers_create","text":"","title":"dvz_framebuffers_create()"},{"location":"api/vklite/#dvz_framebuffers_destroy","text":"","title":"dvz_framebuffers_destroy()"},{"location":"api/vklite/#submit","text":"","title":"Submit"},{"location":"api/vklite/#dvz_submit","text":"","title":"dvz_submit()"},{"location":"api/vklite/#dvz_submit_commands","text":"","title":"dvz_submit_commands()"},{"location":"api/vklite/#dvz_submit_wait_semaphores","text":"","title":"dvz_submit_wait_semaphores()"},{"location":"api/vklite/#dvz_submit_signal_semaphores","text":"","title":"dvz_submit_signal_semaphores()"},{"location":"api/vklite/#dvz_submit_send","text":"","title":"dvz_submit_send()"},{"location":"api/vklite/#dvz_submit_reset","text":"","title":"dvz_submit_reset()"},{"location":"api/vklite/#command-buffer-recording","text":"","title":"Command buffer recording"},{"location":"api/vklite/#dvz_cmd_begin_renderpass","text":"","title":"dvz_cmd_begin_renderpass()"},{"location":"api/vklite/#dvz_cmd_end_renderpass","text":"","title":"dvz_cmd_end_renderpass()"},{"location":"api/vklite/#dvz_cmd_compute","text":"","title":"dvz_cmd_compute()"},{"location":"api/vklite/#dvz_cmd_barrier","text":"","title":"dvz_cmd_barrier()"},{"location":"api/vklite/#dvz_cmd_copy_buffer_to_image","text":"","title":"dvz_cmd_copy_buffer_to_image()"},{"location":"api/vklite/#dvz_cmd_copy_image_to_buffer","text":"","title":"dvz_cmd_copy_image_to_buffer()"},{"location":"api/vklite/#dvz_cmd_copy_image","text":"","title":"dvz_cmd_copy_image()"},{"location":"api/vklite/#dvz_cmd_viewport","text":"","title":"dvz_cmd_viewport()"},{"location":"api/vklite/#dvz_cmd_bind_graphics","text":"","title":"dvz_cmd_bind_graphics()"},{"location":"api/vklite/#dvz_cmd_bind_vertex_buffer","text":"","title":"dvz_cmd_bind_vertex_buffer()"},{"location":"api/vklite/#dvz_cmd_bind_index_buffer","text":"","title":"dvz_cmd_bind_index_buffer()"},{"location":"api/vklite/#dvz_cmd_draw","text":"","title":"dvz_cmd_draw()"},{"location":"api/vklite/#dvz_cmd_draw_indexed","text":"","title":"dvz_cmd_draw_indexed()"},{"location":"api/vklite/#dvz_cmd_draw_indirect","text":"","title":"dvz_cmd_draw_indirect()"},{"location":"api/vklite/#dvz_cmd_draw_indexed_indirect","text":"","title":"dvz_cmd_draw_indexed_indirect()"},{"location":"api/vklite/#dvz_cmd_copy_buffer","text":"","title":"dvz_cmd_copy_buffer()"},{"location":"api/vklite/#dvz_cmd_push","text":"","title":"dvz_cmd_push()"},{"location":"discussions/","text":"Discussions \u00b6 In this section: Vulkan crash course : an accessible overview of Vulkan for scientific computing, a must-read requirement before writing custom visuals, Best practices : a review of best practices when using Datoviz and writing custom visuals, Library architecture : an overview of the internal organization of the code, Roadmap : development roadmap, Developer notes : notes for contributors and developers, FAQ : frequently-asqued questions","title":"Index"},{"location":"discussions/#discussions","text":"In this section: Vulkan crash course : an accessible overview of Vulkan for scientific computing, a must-read requirement before writing custom visuals, Best practices : a review of best practices when using Datoviz and writing custom visuals, Library architecture : an overview of the internal organization of the code, Roadmap : development roadmap, Developer notes : notes for contributors and developers, FAQ : frequently-asqued questions","title":"Discussions"},{"location":"discussions/architecture/","text":"Library architecture \u00b6 Coming soon.","title":"Library architecture"},{"location":"discussions/architecture/#library-architecture","text":"Coming soon.","title":"Library architecture"},{"location":"discussions/developer/","text":"Developer notes \u00b6 A few random notes for developers and contributors. This page needs to be reorganized a bit. Manage script \u00b6 A bash script manage.sh script at the repository root provides commands for build, test, doc generation, and so on. Command Description ./manage.sh build recompile the library ./manage.sh doc rebuild the doc website in site/ ./manage.sh docs serve the website on localhost:8000 ./manage.sh cython update the Cython binding definitions and recompile the Python module ./manage.sh test test_array_ run all tests starting with the given string Documentation building \u00b6 We use mkdocs, with material theme, and several markdown, theme, and mkdocs plugins. See mkdocs.yml . The site is generated in the site/ subfolder. We use GitHub Pages to serve the website. Several parts of the documentation are auto-generated, via mkdocs hooks implemented in utils/hooks.py . Building the documentation requires Python dependencies found in utils/requirements-build.txt . In particular, we use the mkdocs-simple-hooks package to make it possible to use custom Python functions as mkdocs plugin hooks. API documentation : the list of functions to document is found in the docs/api/*.md files. At documentation build time, the API doc generation script ( utils/gendoc.py ) parses the library header files, extracts the doxygen docstrings, and inserts them at the right places in the API documentation pages. Enumerations : the documentation file api/enums.md contains a list of headers of enumerations. A script parses the enums in the library header files and inserts them in this file, at documentation build time. Colormaps : colormaps definitions are saved in a CSV file in data/textures/color_texture.csv . This file is parsed by utils/export_colormap.py and the table of all colormaps is automatically generated, using NumPy and Pillow to generate base64-encoded individual colormap images. The table is inserted at the end of docs/user/colormaps.md . Visual documentation : visuals are documented manually. The screenshots are generated by the builtin_visual.c unit tests. Graphics documentation : the item, vertex, params structure fields are automatically generated by a script that parses the relevant struct definitions in the library header files. Code snippets and screenshots : the documentation build script parses <!-- CODE_PYTHON path/to/file.py --> and <!-- IMAGE path/to/image.png --> in documentation sources and inserts the code file contents, or the image. Note The API doc generation uses joblib to save time when live-regenerating the documentation. However the cache in utils/.joblib must be deleted (so that it's automatically recreated) whenever the Datoviz code/API changes. Otherwise, the API doc generation script may fail. Shaders \u00b6 All shaders include common GLSL files found in include/datoviz/glsl/ . This path must be passed to the glslc command with the -I flag. This is what the CMake script is using. Compiled shaders of the builtin graphics are bundled into the library, using a special CMake command. The binary contents of the SPIR-V-compiled shaders are integrated in build/_shaders.c , which is compiled along with the other C source files of the library. Dependencies \u00b6 Dear ImGUI \u00b6 Datoviz integrates Dear ImGUI via a git submodule ( fork in the Datoviz GitHub organization). There's a custom branch based on master , but which an additional patch applied to it in order to support creating GUIs with integrated Datoviz canvases (not yet implemented). C formatting \u00b6 We use clang format to automatically format all C source files. The rules are defined in .clang-format . Command-line tool \u00b6 Datoviz includes an executable that implements test and examples, implemented in the cli/ subfolder. Shaders and binary resource embedding \u00b6 Important binary resources such as SPIR-V compiled shaders of all included graphics, and the colormap texture, are built directly into the compiled library object. A cmake script loads these files and generates big build/_colortex.c and build/_shaders.c files, which are then compiled and linked into the library. Environment variables \u00b6 Environment variable Description DVZ_FPS=1 Show the number of frames per second DVZ_LOG_LEVEL=0 Logging level Vertical synchronization is activated by default. The refresh rate is typically limited to 60 FPS. Deactivating it (which is automatic when using DVZ_FPS=1 ) leads to the event loop running as fast as possible, which is useful for benchmarking. It may lead to high CPU and GPU utilization, whereas vertical synchronization is typically light on CPU cycles. Note also that user interaction seems laggy when vertical synchronization is active (the default). When it comes to GUI interaction (mouse movements, drag and drop, and so on), we're used to lags lower than 10 milliseconds, which a frame rate of 60 FPS cannot achieve. Logging levels : 0=trace, 1=debug, 2=info, 3=warning, 4=error DPI scaling factor : Datoviz natively supports DPI scaling for linewidths, font size, axes, etc. Since automatic cross-platform DPI detection does not seem reliable, Datoviz simply uses sensible defaults but provides an easy way for the user to increase or decrease the DPI via this environment variable. This is useful on high-DPI/Retina monitors.","title":"Developer notes"},{"location":"discussions/developer/#developer-notes","text":"A few random notes for developers and contributors. This page needs to be reorganized a bit.","title":"Developer notes"},{"location":"discussions/developer/#manage-script","text":"A bash script manage.sh script at the repository root provides commands for build, test, doc generation, and so on. Command Description ./manage.sh build recompile the library ./manage.sh doc rebuild the doc website in site/ ./manage.sh docs serve the website on localhost:8000 ./manage.sh cython update the Cython binding definitions and recompile the Python module ./manage.sh test test_array_ run all tests starting with the given string","title":"Manage script"},{"location":"discussions/developer/#documentation-building","text":"We use mkdocs, with material theme, and several markdown, theme, and mkdocs plugins. See mkdocs.yml . The site is generated in the site/ subfolder. We use GitHub Pages to serve the website. Several parts of the documentation are auto-generated, via mkdocs hooks implemented in utils/hooks.py . Building the documentation requires Python dependencies found in utils/requirements-build.txt . In particular, we use the mkdocs-simple-hooks package to make it possible to use custom Python functions as mkdocs plugin hooks. API documentation : the list of functions to document is found in the docs/api/*.md files. At documentation build time, the API doc generation script ( utils/gendoc.py ) parses the library header files, extracts the doxygen docstrings, and inserts them at the right places in the API documentation pages. Enumerations : the documentation file api/enums.md contains a list of headers of enumerations. A script parses the enums in the library header files and inserts them in this file, at documentation build time. Colormaps : colormaps definitions are saved in a CSV file in data/textures/color_texture.csv . This file is parsed by utils/export_colormap.py and the table of all colormaps is automatically generated, using NumPy and Pillow to generate base64-encoded individual colormap images. The table is inserted at the end of docs/user/colormaps.md . Visual documentation : visuals are documented manually. The screenshots are generated by the builtin_visual.c unit tests. Graphics documentation : the item, vertex, params structure fields are automatically generated by a script that parses the relevant struct definitions in the library header files. Code snippets and screenshots : the documentation build script parses <!-- CODE_PYTHON path/to/file.py --> and <!-- IMAGE path/to/image.png --> in documentation sources and inserts the code file contents, or the image. Note The API doc generation uses joblib to save time when live-regenerating the documentation. However the cache in utils/.joblib must be deleted (so that it's automatically recreated) whenever the Datoviz code/API changes. Otherwise, the API doc generation script may fail.","title":"Documentation building"},{"location":"discussions/developer/#shaders","text":"All shaders include common GLSL files found in include/datoviz/glsl/ . This path must be passed to the glslc command with the -I flag. This is what the CMake script is using. Compiled shaders of the builtin graphics are bundled into the library, using a special CMake command. The binary contents of the SPIR-V-compiled shaders are integrated in build/_shaders.c , which is compiled along with the other C source files of the library.","title":"Shaders"},{"location":"discussions/developer/#dependencies","text":"","title":"Dependencies"},{"location":"discussions/developer/#dear-imgui","text":"Datoviz integrates Dear ImGUI via a git submodule ( fork in the Datoviz GitHub organization). There's a custom branch based on master , but which an additional patch applied to it in order to support creating GUIs with integrated Datoviz canvases (not yet implemented).","title":"Dear ImGUI"},{"location":"discussions/developer/#c-formatting","text":"We use clang format to automatically format all C source files. The rules are defined in .clang-format .","title":"C formatting"},{"location":"discussions/developer/#command-line-tool","text":"Datoviz includes an executable that implements test and examples, implemented in the cli/ subfolder.","title":"Command-line tool"},{"location":"discussions/developer/#shaders-and-binary-resource-embedding","text":"Important binary resources such as SPIR-V compiled shaders of all included graphics, and the colormap texture, are built directly into the compiled library object. A cmake script loads these files and generates big build/_colortex.c and build/_shaders.c files, which are then compiled and linked into the library.","title":"Shaders and binary resource embedding"},{"location":"discussions/developer/#environment-variables","text":"Environment variable Description DVZ_FPS=1 Show the number of frames per second DVZ_LOG_LEVEL=0 Logging level Vertical synchronization is activated by default. The refresh rate is typically limited to 60 FPS. Deactivating it (which is automatic when using DVZ_FPS=1 ) leads to the event loop running as fast as possible, which is useful for benchmarking. It may lead to high CPU and GPU utilization, whereas vertical synchronization is typically light on CPU cycles. Note also that user interaction seems laggy when vertical synchronization is active (the default). When it comes to GUI interaction (mouse movements, drag and drop, and so on), we're used to lags lower than 10 milliseconds, which a frame rate of 60 FPS cannot achieve. Logging levels : 0=trace, 1=debug, 2=info, 3=warning, 4=error DPI scaling factor : Datoviz natively supports DPI scaling for linewidths, font size, axes, etc. Since automatic cross-platform DPI detection does not seem reliable, Datoviz simply uses sensible defaults but provides an easy way for the user to increase or decrease the DPI via this environment variable. This is useful on high-DPI/Retina monitors.","title":"Environment variables"},{"location":"discussions/faq/","text":"Frequently asked questions \u00b6 What is the distinction between the scene, the canvas, and the window? \u00b6 Datoviz provides three similar, but different abstractions: the scene , the canvas , the window . The scene provides a relatively high-level plotting interface that allows to arrange panels (subplots) in a grid, define controllers, and add visuals to the panels. The canvas is lower-level object that allows to use Vulkan directly via vklite. While the scene deals with visual elements, the canvas deals with Vulkan objects. The window is an abstraction provided by the backend windowing library, glfw at the moment. It is a bare window that doesn't allow for any kind rendering, unless manually creating a swapchain and so on by using Vulkan or vklite directly. Note With the Python bindings, a scene is automatically created when creating a canvas. Most users will only work at the scene level. Advanced users will use the canvas to create custom applications, interactive animations, or even small video games. Finally, the window is only used internally and will probably never be used directly.","title":"FAQ"},{"location":"discussions/faq/#frequently-asked-questions","text":"","title":"Frequently asked questions"},{"location":"discussions/faq/#what-is-the-distinction-between-the-scene-the-canvas-and-the-window","text":"Datoviz provides three similar, but different abstractions: the scene , the canvas , the window . The scene provides a relatively high-level plotting interface that allows to arrange panels (subplots) in a grid, define controllers, and add visuals to the panels. The canvas is lower-level object that allows to use Vulkan directly via vklite. While the scene deals with visual elements, the canvas deals with Vulkan objects. The window is an abstraction provided by the backend windowing library, glfw at the moment. It is a bare window that doesn't allow for any kind rendering, unless manually creating a swapchain and so on by using Vulkan or vklite directly. Note With the Python bindings, a scene is automatically created when creating a canvas. Most users will only work at the scene level. Advanced users will use the canvas to create custom applications, interactive animations, or even small video games. Finally, the window is only used internally and will probably never be used directly.","title":"What is the distinction between the scene, the canvas, and the window?"},{"location":"discussions/practices/","text":"Best practices \u00b6 Coming soon.","title":"Best practices"},{"location":"discussions/practices/#best-practices","text":"Coming soon.","title":"Best practices"},{"location":"discussions/roadmap/","text":"Roadmap \u00b6 This provisional roadmap may be updated at any time depending on user feedback and internal needs at the International Brain Laboratory. Short term \u00b6 Spring 2021 Proper IPython terminal event-loop integration More visuals Text (graphics already implemented) Segments (graphics already implemented) Arrows 3D surface (reuse mesh code) Alternative colormap mesh with just vec3 pos and float value in vertex struct More GUI controls Table Fixed aspect ratio More robust error handling More robust testing Continuous integration Packaging Finish documentation First public 0.1 release Medium term \u00b6 Fall 2021 \"Infinite zooming\" with axes 2D Axes 3D More visuals PSLG Fake 3D spheres Transformation system Earth geographic coordinates Linked interactivity across panels Cython bindings for custom visuals Qt integration Wrapper proof of concepts MATLAB Benchmark suite Long term \u00b6 2022 and beyond Video reader visual Jupyter backend Remove visualization server CUDA interop","title":"Roadmap"},{"location":"discussions/roadmap/#roadmap","text":"This provisional roadmap may be updated at any time depending on user feedback and internal needs at the International Brain Laboratory.","title":"Roadmap"},{"location":"discussions/roadmap/#short-term","text":"Spring 2021 Proper IPython terminal event-loop integration More visuals Text (graphics already implemented) Segments (graphics already implemented) Arrows 3D surface (reuse mesh code) Alternative colormap mesh with just vec3 pos and float value in vertex struct More GUI controls Table Fixed aspect ratio More robust error handling More robust testing Continuous integration Packaging Finish documentation First public 0.1 release","title":"Short term"},{"location":"discussions/roadmap/#medium-term","text":"Fall 2021 \"Infinite zooming\" with axes 2D Axes 3D More visuals PSLG Fake 3D spheres Transformation system Earth geographic coordinates Linked interactivity across panels Cython bindings for custom visuals Qt integration Wrapper proof of concepts MATLAB Benchmark suite","title":"Medium term"},{"location":"discussions/roadmap/#long-term","text":"2022 and beyond Video reader visual Jupyter backend Remove visualization server CUDA interop","title":"Long term"},{"location":"discussions/vulkan/","text":"Vulkan crash course \u00b6 This page proposes a high-level, simplified overview of Vulkan for beginners. Understanding the basic principles Vulkan is required when writing custom visuals or graphics, but it is not required when using existing visuals . Using the GPU for scientific visualization \u00b6 The GPU is a massively parallel computing unit dedicated to real-time graphics rendering. Vulkan is a low-level graphics API that provides extensive control on the GPU for rendering and compute. It is harder to user than older APIs like OpenGL because it proposes a relatively low level of abstraction. This design choice gives more control to the developer and helps achieving higher performance. The GPU is typically used by video games, which render complex, animated 3D meshes with real-time special effects and low latency. The GPU can also be used for scientific applications, which has quite different requirements. Scenes are typically less dynamic, there is less heterogeneity in the types of objects rendered in the scene, and high visual accuracy is an absolute requirement. How to use the GPU for scientific visualization? At a high level, the user has some scientific data: a set of 2D or 3D points, a graph, a volume, an image, and so on. On the other hand, the GPU can only render three types of primitives : points (1D), lines (2D), triangles (3D). Fortunately, the GPU gives full control on, essentially, two things: the way the data is transformed before the primitive positions are determined, the exact color of each pixel of each primitive. In practice, one specifies this via special programs called shaders , that run in parallel on the GPU. They are typically written in a C-like language called GLSL (OpenGL shading language). Note Vulkan does not work directly with GLSL, but with an intermediate representation called SPIR-V (a bit similar to LLVM). Vulkan and other third-parties propose compilers transforming GLSL code into SPIR-V. In Datoviz, all shader code is written in GLSL. It is theoretically possible to use other languages that compile to SPIR-V. The vertex shader is a GLSL program that runs on every vertex (point) of a given graphics pipeline. It must return the final point position in a reference normalized coordinate system. This point is used when generating the primitive (point, one of the two endpoints of a line, or one of the three corners of a triangle). The fragment shader is a GLSL program that runs on every pixel of every primitive (point, line, or triangle). It must return the RGBA values of that pixel. It can discard pixels altogether (for example, slicing a mesh along a plane). Note Vulkan supports other types of shaders, such as geometry shaders and tesselation shaders. However, hardware support for these more recent and advanced shader types is not universal. For example, geometry shaders are not supported on macOS (Metal). Another important type of shader is the compute shader . Compute shaders are used to implement general-purpose parallel computations on the GPU on possibly the same objects (buffer and textures) used for rendering, which allow for highly complex and custom visualization applications. There are, of course, many other parameters and details related to rendering, but these are the most important principles. 2D and 3D graphics on the GPU \u00b6 Rendering high-quality 2D graphics on the GPU is significantly harder than rendering 3D graphics. 3D rendering \u00b6 Rendering a 3D mesh is relatively straightforward. A mesh is typically defined by a set of 3D points , and a set of faces . Each face is determined by three integers: the indices of the three triangle corners within the set of 3D points. Vertex shader In the simplest case, the vertex shader takes as input the 3D points, and applies 3D transformation matrices to account for the camera position and perspective. By convention, there are generally three transformation 4x4 matrices: model matrix : transformation from the local coordinate system (in which the mesh is defined) into the global coordinate system (the 3D world containing the mesh), view matrix : transformation from the global coordinate system to the camera-local coordinate system, projection matrix : applies perspective with 4D homogeneous coordinates. Understanding the mathematics of these transformations is beyond the scope of this page. There are many explanations online. Here is a trivial example of a vertex shader in GLSL: #version 450 layout ( location = 0 ) in vec3 pos ; void main () { // ... define transformation matrices ... gl_Position = proj * view * model * vec4 ( pos , 1.0 ); } The variable pos is a 3D vector with the coordinates of 1 point. The GPU executes the vertex shader in parallel on many points. The output of the vertex shader is gl_Position , a vec4 vector (the last coordinate is the homogeneous coordinate, used for perspective). Fragment shader A trivial fragment shader could output a constant color, independently from the position of the pixel within a triangle. But typically, the fragment shader for a 3D mesh implements mathematical computations to account for lighting, which gives a much more realistic feel. While many lighting models have been created, Datoviz currently implements a classic technique called Phong shading . Again, the details are beyond the scope of this page. One can also apply a texture on a mesh. Each point comes with a pair of special coordinates called the texture coordinates , noted uv . Normalized between 0 and 1, they refer to a specific pixel position within an associated 2D texture. The fragment shader typically fetches the color of the texture pixel (texel) at this exact position. Importantly, the GPU is able to make efficient linear interpolations of these values for pixels between two vertices of a given primitive. For example, to render an image, one specifies two triangles forming a square, and sets the uv coordinates of each of the six vertices (three per triangle) to the different combinations of (0 or 1, 0 or 1) . For every pixel in the square, the correct uv coordinates will be interpolated and the relevant texel will be fetched from the texture on the GPU. 2D rendering \u00b6 Rendering 2D graphics is much trickier on the GPU. How to render a disc, a polygon patch, a thick line, text by using only points, lines, and triangles? The answer is: by leveraging the vertex shader and, more importantly, the fragment shader . One follows two steps: define the primitive type for the 2D object, which will constitute a sort of \"envelope\" of the final object, use the fragment shader to properly discard pixels that are beyond the boundaries of the final object, and compute the alpha transparency value for pixels lying on the border of the object, thereby implementing antialiasing directly on the GPU. For example, to render a thick line, one must triangulate the path, taking care of regions of high curvature and other details. On each triangle, the fragment shader computes the exact position along the path, and the distance to the border of the path. The alpha transparency value is obtained via a so-called signed distance function (a function of space giving the distance, in pixels, to the border of the object). A similar principle is used for markers and text. For text, signed distance functions of each glyph are stored in a texture and used by the fragment shader. In Datoviz, high-quality antialiased 2D graphics are implemented with GLSL code originally written by Nicolas Rougier in his Glumpy library (GPU implementation of agg , antigrain geometry), and published in computer graphics articles . A fundamental principle of Datoviz is to abstract away these low-level details to the user, who can reuse directly these existing graphics for the most common types of scientific visualizations . Vulkan for scientific visualization \u00b6 We've now seen the basic principles of using the GPU for scientific visualization. Let's turn now to Vulkan. Vulkan is a low-level graphics API that has a high entry barrier given the large number of abstractions provided. These abstractions mostly relate to internal details of GPU hardware, but they are essential when one focuses on achieving high performance, which is the main selling point of Datoviz. Here is, for your information only, the ~30 types of objects used in Vulkan: Diagram by Adam Sawicki, for gpuopen Covering all of these objects in detail is totally out of scope of this page. However, we'll briefly explain the most important objects, and how they are used in Datoviz. Importantly, Datoviz implements its own thin wrapper on top of Vulkan (called vklite), which focuses on the most important concepts only. The wrapper provides an API that is easier to use than the original Vulkan API, although it it slightly less flexible. This is acceptable given that the wrapper targets scientific applications, which are less demanding than 3D video games. We'll classify these objects in five broad categories: storing data on the GPU, defining graphics and compute pipelines with shaders, recording graphics and compute commands for the GPU, running the main rendering loop , synchronization . Storing data on the GPU \u00b6 Scientific data is typically obtained from files, from the network, or generated by simulation programs. In order for the GPU to render it, it needs to be uploaded to the GPU. A GPU typically has dedicated video memory, or shares memory with the host. For example, the NVIDIA GEFORCE RTX 2070 SUPER GPU has 8 GB of video memory. In any case, Vulkan defines several objects to control how memory is organized and how the data is stored in video memory. Datoviz has a very simple model where there are only two types of memory: GPU buffers : a GPU buffer is a memory buffer of a given size, that contains arbitrary binary bytes, GPU textures : a GPU texture is defined by a 1D, 2D, or 3D image of a given shape, by an internally-handled memory buffer with the pixel data in a given format, and a sampler which is a special GPU object that specifies how a texel is accessed and interpolated when fetched by the fragment shader. A GPU buffer abstracts away the following Vulkan objects: Buffer, BufferView. A GPU texture abstracts away the following Vulkan objects: Image, ImageView, Sampler. Uploading data from the host memory to a GPU buffer or texture, and downloading data from the GPU back to the host, are complex operations in Vulkan. Again, Datoviz abstracts these processes away in the transfer API. Note A GPU image may be represented in several ways by the GPU. For example, a texture needs to be stored in a special way in order to achieve high performance, but this internal representation is incompatible with the way the image is typically stored in a file or on the host. Vulkan provides an API to transition the image between these different formats. Uploading an image to the GPU therefore involves transitioning the image from whatever format the GPU has chosen, to a standard linear layout that matches the data uploaded from the CPU. These details are abstracted away in Datoviz. Defining pipeline with shaders \u00b6 Datoviz supports two types of pipelines: graphics pipelines (or just graphics ): for rendering points, lines, or triangles with dedicated vertex and fragment shaders, compute pipelines (or just computes ): for general-purpose GPU computations. A graphics pipeline encompasses many steps. The diagram below exposes the most important steps, and is shown for your information only: Schematic from vulkan-tutorial.com In addition to defining shaders, one needs to define the data input of the vertex shader. In Datoviz, a vertex shader accepts several kinds of inputs: attribute : a part of each vertex to process in parallel (for example, a vec3 position for the point being processed), uniform : small data (parameters) shared across all vertices in the pipeline, texture : a sampler giving a way to fetch any texel from a 1D, 2D, or 3D texture, storage buffer : an arbitrary binary buffer that may be accessed from any vertex thread, push constant : a small parameter that is set when recording the command buffer (see below). Uniforms, textures, storage buffers are special types of so-called Vulkan descriptors , that we could also call GPU resources : they represent essentially GPU buffers or textures. They can be accessed from the vertex shader or, actually, from any kind of shader. By contrast, the push constant is a different type of data that is passed to the GPU when recording a command buffer. An attribute is a type of data that is processed in parallel. Vulkan define several abstractions in order to define the way a shader may access descriptors (uniforms, textures, or storage buffers): descriptor set layouts, pipeline layouts, descriptor pools, descriptor sets... These abstractions mostly make sense when focusing on performance, and they are partly abstracted away in Datoviz. Datoviz proposes the following, simplified model for defining GPU resources accessible by shaders: a slot is defined in GLSL in each shader. It is represented by a resource index within the shader, and a resource type (uniform, storage, sampler). It must also be defined in the Datoviz C API, and the GLSL and C descriptions must imperatively match. a binding is an association of a given GPU object (buffer or texture) with a given slot in a given pipeline (graphics or compute). For example, a pipeline may declare that it expects a uniform at slot 0, and a texture sampler at slot 1. This is defined in the slots . Then, before one can render a pipeline, one also needs to declare what GPU buffer to use for slot 0, and what GPU texture to use for slot 1. This is defined in the bindings . Defining slots is done when creating a graphics or compute. Defining bindings is done when rendering an existing graphics or compute. Pipelines encompass the following Vulkan objects: ShaderModule, Pipeline, PipelineLayout, DescriptorPool, DescriptorSetLayout, DescriptorSet. Recording commands for the GPU \u00b6 Once GPU objects have been created, data has been uploaded, graphics and compute pipelines have been defined, the next step is to record commands for the GPU . This aspect of Vulkan is a significant difference with older graphics APIs such as OpenGL. One does not send commands to the GPU in real time, but one pre-records a linear succession of commands within a so-called command buffer , and submits recorded command buffers to special queues . The GPU watches these queues, receives the command buffers, and processes them in parallel. In Vulkan, recording commands are special commands starting wih vkCmd . In Datoviz, they start with dvz_cmd_ . For example, rendering a single graphics pipeline involves the following recording commands: dvz_cmd_begin (...); // begin recording the command buffer dvz_cmd_begin_renderpass (...); // begin the renderpass dvz_cmd_bind_vertex_buffer (...); // bind an existing GPU buffer as vertex buffer dvz_cmd_bind_index_buffer (...); // bind an existing GPU buffer as index buffer dvz_cmd_bind_graphics (...); // bind an existing graphics pipeline dvz_cmd_viewport (...); // set the viewport dvz_cmd_draw (...); // **perform the actual rendering of the graphics pipeline** dvz_cmd_end_renderpass (...); // end the renderpass dvz_cmd_end (...); // stop recording the command buffer Once called on a command buffer, the command buffer is recorded and can be submitted to a GPU queue. Vulkan leaves to the user the choice of defining the number and types of GPU queues for the application. This is also depends heavily on the hardware. Currently, Datoviz requests four queues, but may end up with less queues if the hardware does not support them (this is all transparent to the user): a transfer queue receives command buffers for buffer/image upload, download, copy, transitions... a compute queue receives command buffers with compute tasks, a render queue receives command buffers with either graphics and/or compute tasks, a present queue is used for the main rendering loop (swapchain). Vulkan has been designed with the idea that command buffers will be constantly recreated and/or reused. Multithreaded applications can record command buffers in parallel, and must use special synchronization primitives to efficiently submit the command buffers to the GPU. Scientific applications are typically much less dynamic than video games. Still, some amount of dynamism is expected in some applications. Therefore, Datoviz assumes that command buffers are typically not recreated at every frame, but allows for relatively efficient command buffer recreation when needed. Here are the different ways the GPU objects may change during the lifetime of an application: changing buffer or texture data : update a GPU buffer or texture, doesn't require command buffer recreation, interactivity (pan and zoom, arcball, and so on): update a uniform buffer, changing the number of vertices in a given graphics pipeline: require command buffer recreation (unless using indirect rendering , in which case this involves updating a GPU buffer), push constant change : require command buffer recreation, resize , panel change , viewport change : require command buffer recreation. Datoviz command buffers and queues are based on the following Vulkan objects: CommandPool, CommandBuffer, Queue. Main rendering loop \u00b6 Once the command buffers have been recorded, one needs to submit them to the GPU and render the scene in a window. This step must be done manually when using the Vulkan API. Again, this is abstracted away in Datoviz, at the level of the canvas . This step is actually one of the most complex ones in Vulkan, especially when there's a need to do it as efficiently as possible. First, one considers that the window size is fixed (until it is not). Resizing is a complex operation that requires destroying and recreating a large number of Vulkan objects, and it needs to be handled correctly in the rendering loop. Second, one needs to create a GPU image (like the object that is associated to a texture, but without a sampler) that the GPU will render to. This image will be presented to the screen. One must also define another special image of the same size for the depth buffer , essential with 3D rendering. Third, one must acquire a surface , a special Vulkan object that is used to render something to the window. Creating a window is an OS-dependent operation. Datoviz uses the glfw window library that abstracts these details away and offers an easy way to create windows and to deal with user inputs (mouse, keyboard). Fourth, one needs to create a swapchain . This object provides a way to implement a technique sometimes called double-buffering, or triple-buffering, depending on the number of images used in the swapchain. The idea is to avoid making the GPU wait while an image is being presented to the screen . For example, with a frame rate of 60 images per second, each image remains on screen during about 16 milliseconds. During this time, the GPU is not expected to render to the same image, unless it makes a copy. That's basically the idea of the swapchain: providing a set of 2, 3 or more images that are almost identical. While the image 0 is presented to the screen, the GPU can render the next frame on image 1. When it finishes, it presents image 1 to the screen, while image 0 is being rewritten for the next frame (double buffering), or while it waits until the GPU requests it. This logic must be, in part, implemented by the developer who uses the Vulkan API directly. Datoviz completely abstracts this process away. Fifth, one needs to define a render pass and a set of framebuffers . The render pass defines the way the GPU renders an image, in one or several steps. The framebuffers represent the links between the GPU images and the render pass steps. The render pass must be specified when recording a rendering command buffer. Sixth, one needs to implement the main rendering loop . This is typically an infinite loop where every iteration represents a frame. At every frame, the rendering loop performs the following (simplified) steps: Examine the user events (mouse, keyboard) that occurred in the window since the last frame, Perform the resize if the window size has changed since the last frame, Implement the scene logic (update internal variables as a function of user events and time), Perform the pending transfers (upload/download of GPU buffers/textures) that have been requested since last frame, possibly from a background thread, Optionally, record new command buffers, Acquire a new swapchain image for rendering, Wait until the previous frame that was using the same swapchain image (might be 2 or 3 frames before) has finished rendering, Submit the command buffers to their associated GPU queues, which will render the image, Present the image to the screen, but only after the GPU has finished rendering it (asynchronous operation). This logic is essentially implemented in: dvz_canvas_frame() dvz_canvas_frame_submit() dvz_app_run() The rendering loop involves the following Vulkan objects: SurfaceKHR, SwapchainKHR, Image, ImageView, RenderPass, Framebuffer. Synchronization \u00b6 The last important topic pertains to synchronization . The GPU should be seen as a partly independent device on which tasks are submitted asynchronously (via command buffers and queues), and that processes them in parallel. In addition, each task may involve a massively parallel architecture (for example, processing thousands of vertices or fragments in parallel). Vulkan provides an API to let the CPU communicate with the GPU. A highly inefficient way would be for the CPU to wait until the GPU is idle before submitting new tasks or uploading data. This would be done via \"hard\" synchronization primitives that are implemented in the Vulkan functions vkQueueWaitIdle() and vkDeviceWaitIdle() , and in the dvz_queue|gpu|app_wait() functions in Datoviz. Doing it this way would work and would not require any other more fine-grained synchronization primitive, but it would result in poor performance. Vulkan provides several more fine-grained synchronization primitives, of which Datoviz currently supports three: Inner command buffer synchronization , provided by barriers, GPU-GPU synchronization , provided by semaphores. CPU-GPU synchronization , provided by fences, A barrier is a way for a command buffer to let the GPU know that some recorded commands should wait for completion of other commands. For example, if a command buffer involves launching a compute shader on a buffer, then rendering to a graphics pipeline, a barrier should be defined if ever the graphics pipeline uses the same buffer as used by the compute shader. The graphics pipeline should not start reading a buffer while the same buffer is being written to by the compute pipeline. A semaphore is a way to introduce dependencies between different submissions of command buffers. They are used in the main rendering loop and swapchain logic. When the GPU has finished rendering an image, then this image should be presented to the screen. This is implemented with a semaphore. A fence is a way for the CPU to wait until the GPU has finished some task. This is also used in the main rendering loop: it wouldn't make much sense to start the current frame until the previous frame has not finished rendering (but it can start while the swapchain presents the previously rendered image to the screen). Synchronization is required when uploading data to the GPU. Since the GPU may be constantly reading from GPU buffers and textures, data should not be uploaded to these GPU objects without proper synchronization. Otherwise, the GPU might use corrupted data while rendering the scene. Technical details \u00b6 The vklite API documentation contains the documentation of all Vulkan-related functions. There is no other documentation of vklite at the moment, however you can look at the unit tests to get an idea of how to use each vklite object. GPU features and limits \u00b6 The DvzGpu structure contains a few fields with native Vulkan structures defining GPU capabilities and limits. We give here a few minimal values that we can reasonably expect on almost all devices (according to this Vulkan database ): Texture dimension Maximum allowed texture size (in any axis) 1D 16384 2D 16384 3D 2048","title":"Vulkan crash course"},{"location":"discussions/vulkan/#vulkan-crash-course","text":"This page proposes a high-level, simplified overview of Vulkan for beginners. Understanding the basic principles Vulkan is required when writing custom visuals or graphics, but it is not required when using existing visuals .","title":"Vulkan crash course"},{"location":"discussions/vulkan/#using-the-gpu-for-scientific-visualization","text":"The GPU is a massively parallel computing unit dedicated to real-time graphics rendering. Vulkan is a low-level graphics API that provides extensive control on the GPU for rendering and compute. It is harder to user than older APIs like OpenGL because it proposes a relatively low level of abstraction. This design choice gives more control to the developer and helps achieving higher performance. The GPU is typically used by video games, which render complex, animated 3D meshes with real-time special effects and low latency. The GPU can also be used for scientific applications, which has quite different requirements. Scenes are typically less dynamic, there is less heterogeneity in the types of objects rendered in the scene, and high visual accuracy is an absolute requirement. How to use the GPU for scientific visualization? At a high level, the user has some scientific data: a set of 2D or 3D points, a graph, a volume, an image, and so on. On the other hand, the GPU can only render three types of primitives : points (1D), lines (2D), triangles (3D). Fortunately, the GPU gives full control on, essentially, two things: the way the data is transformed before the primitive positions are determined, the exact color of each pixel of each primitive. In practice, one specifies this via special programs called shaders , that run in parallel on the GPU. They are typically written in a C-like language called GLSL (OpenGL shading language). Note Vulkan does not work directly with GLSL, but with an intermediate representation called SPIR-V (a bit similar to LLVM). Vulkan and other third-parties propose compilers transforming GLSL code into SPIR-V. In Datoviz, all shader code is written in GLSL. It is theoretically possible to use other languages that compile to SPIR-V. The vertex shader is a GLSL program that runs on every vertex (point) of a given graphics pipeline. It must return the final point position in a reference normalized coordinate system. This point is used when generating the primitive (point, one of the two endpoints of a line, or one of the three corners of a triangle). The fragment shader is a GLSL program that runs on every pixel of every primitive (point, line, or triangle). It must return the RGBA values of that pixel. It can discard pixels altogether (for example, slicing a mesh along a plane). Note Vulkan supports other types of shaders, such as geometry shaders and tesselation shaders. However, hardware support for these more recent and advanced shader types is not universal. For example, geometry shaders are not supported on macOS (Metal). Another important type of shader is the compute shader . Compute shaders are used to implement general-purpose parallel computations on the GPU on possibly the same objects (buffer and textures) used for rendering, which allow for highly complex and custom visualization applications. There are, of course, many other parameters and details related to rendering, but these are the most important principles.","title":"Using the GPU for scientific visualization"},{"location":"discussions/vulkan/#2d-and-3d-graphics-on-the-gpu","text":"Rendering high-quality 2D graphics on the GPU is significantly harder than rendering 3D graphics.","title":"2D and 3D graphics on the GPU"},{"location":"discussions/vulkan/#3d-rendering","text":"Rendering a 3D mesh is relatively straightforward. A mesh is typically defined by a set of 3D points , and a set of faces . Each face is determined by three integers: the indices of the three triangle corners within the set of 3D points.","title":"3D rendering"},{"location":"discussions/vulkan/#2d-rendering","text":"Rendering 2D graphics is much trickier on the GPU. How to render a disc, a polygon patch, a thick line, text by using only points, lines, and triangles? The answer is: by leveraging the vertex shader and, more importantly, the fragment shader . One follows two steps: define the primitive type for the 2D object, which will constitute a sort of \"envelope\" of the final object, use the fragment shader to properly discard pixels that are beyond the boundaries of the final object, and compute the alpha transparency value for pixels lying on the border of the object, thereby implementing antialiasing directly on the GPU. For example, to render a thick line, one must triangulate the path, taking care of regions of high curvature and other details. On each triangle, the fragment shader computes the exact position along the path, and the distance to the border of the path. The alpha transparency value is obtained via a so-called signed distance function (a function of space giving the distance, in pixels, to the border of the object). A similar principle is used for markers and text. For text, signed distance functions of each glyph are stored in a texture and used by the fragment shader. In Datoviz, high-quality antialiased 2D graphics are implemented with GLSL code originally written by Nicolas Rougier in his Glumpy library (GPU implementation of agg , antigrain geometry), and published in computer graphics articles . A fundamental principle of Datoviz is to abstract away these low-level details to the user, who can reuse directly these existing graphics for the most common types of scientific visualizations .","title":"2D rendering"},{"location":"discussions/vulkan/#vulkan-for-scientific-visualization","text":"We've now seen the basic principles of using the GPU for scientific visualization. Let's turn now to Vulkan. Vulkan is a low-level graphics API that has a high entry barrier given the large number of abstractions provided. These abstractions mostly relate to internal details of GPU hardware, but they are essential when one focuses on achieving high performance, which is the main selling point of Datoviz. Here is, for your information only, the ~30 types of objects used in Vulkan: Diagram by Adam Sawicki, for gpuopen Covering all of these objects in detail is totally out of scope of this page. However, we'll briefly explain the most important objects, and how they are used in Datoviz. Importantly, Datoviz implements its own thin wrapper on top of Vulkan (called vklite), which focuses on the most important concepts only. The wrapper provides an API that is easier to use than the original Vulkan API, although it it slightly less flexible. This is acceptable given that the wrapper targets scientific applications, which are less demanding than 3D video games. We'll classify these objects in five broad categories: storing data on the GPU, defining graphics and compute pipelines with shaders, recording graphics and compute commands for the GPU, running the main rendering loop , synchronization .","title":"Vulkan for scientific visualization"},{"location":"discussions/vulkan/#storing-data-on-the-gpu","text":"Scientific data is typically obtained from files, from the network, or generated by simulation programs. In order for the GPU to render it, it needs to be uploaded to the GPU. A GPU typically has dedicated video memory, or shares memory with the host. For example, the NVIDIA GEFORCE RTX 2070 SUPER GPU has 8 GB of video memory. In any case, Vulkan defines several objects to control how memory is organized and how the data is stored in video memory. Datoviz has a very simple model where there are only two types of memory: GPU buffers : a GPU buffer is a memory buffer of a given size, that contains arbitrary binary bytes, GPU textures : a GPU texture is defined by a 1D, 2D, or 3D image of a given shape, by an internally-handled memory buffer with the pixel data in a given format, and a sampler which is a special GPU object that specifies how a texel is accessed and interpolated when fetched by the fragment shader. A GPU buffer abstracts away the following Vulkan objects: Buffer, BufferView. A GPU texture abstracts away the following Vulkan objects: Image, ImageView, Sampler. Uploading data from the host memory to a GPU buffer or texture, and downloading data from the GPU back to the host, are complex operations in Vulkan. Again, Datoviz abstracts these processes away in the transfer API. Note A GPU image may be represented in several ways by the GPU. For example, a texture needs to be stored in a special way in order to achieve high performance, but this internal representation is incompatible with the way the image is typically stored in a file or on the host. Vulkan provides an API to transition the image between these different formats. Uploading an image to the GPU therefore involves transitioning the image from whatever format the GPU has chosen, to a standard linear layout that matches the data uploaded from the CPU. These details are abstracted away in Datoviz.","title":"Storing data on the GPU"},{"location":"discussions/vulkan/#defining-pipeline-with-shaders","text":"Datoviz supports two types of pipelines: graphics pipelines (or just graphics ): for rendering points, lines, or triangles with dedicated vertex and fragment shaders, compute pipelines (or just computes ): for general-purpose GPU computations. A graphics pipeline encompasses many steps. The diagram below exposes the most important steps, and is shown for your information only: Schematic from vulkan-tutorial.com In addition to defining shaders, one needs to define the data input of the vertex shader. In Datoviz, a vertex shader accepts several kinds of inputs: attribute : a part of each vertex to process in parallel (for example, a vec3 position for the point being processed), uniform : small data (parameters) shared across all vertices in the pipeline, texture : a sampler giving a way to fetch any texel from a 1D, 2D, or 3D texture, storage buffer : an arbitrary binary buffer that may be accessed from any vertex thread, push constant : a small parameter that is set when recording the command buffer (see below). Uniforms, textures, storage buffers are special types of so-called Vulkan descriptors , that we could also call GPU resources : they represent essentially GPU buffers or textures. They can be accessed from the vertex shader or, actually, from any kind of shader. By contrast, the push constant is a different type of data that is passed to the GPU when recording a command buffer. An attribute is a type of data that is processed in parallel. Vulkan define several abstractions in order to define the way a shader may access descriptors (uniforms, textures, or storage buffers): descriptor set layouts, pipeline layouts, descriptor pools, descriptor sets... These abstractions mostly make sense when focusing on performance, and they are partly abstracted away in Datoviz. Datoviz proposes the following, simplified model for defining GPU resources accessible by shaders: a slot is defined in GLSL in each shader. It is represented by a resource index within the shader, and a resource type (uniform, storage, sampler). It must also be defined in the Datoviz C API, and the GLSL and C descriptions must imperatively match. a binding is an association of a given GPU object (buffer or texture) with a given slot in a given pipeline (graphics or compute). For example, a pipeline may declare that it expects a uniform at slot 0, and a texture sampler at slot 1. This is defined in the slots . Then, before one can render a pipeline, one also needs to declare what GPU buffer to use for slot 0, and what GPU texture to use for slot 1. This is defined in the bindings . Defining slots is done when creating a graphics or compute. Defining bindings is done when rendering an existing graphics or compute. Pipelines encompass the following Vulkan objects: ShaderModule, Pipeline, PipelineLayout, DescriptorPool, DescriptorSetLayout, DescriptorSet.","title":"Defining pipeline with shaders"},{"location":"discussions/vulkan/#recording-commands-for-the-gpu","text":"Once GPU objects have been created, data has been uploaded, graphics and compute pipelines have been defined, the next step is to record commands for the GPU . This aspect of Vulkan is a significant difference with older graphics APIs such as OpenGL. One does not send commands to the GPU in real time, but one pre-records a linear succession of commands within a so-called command buffer , and submits recorded command buffers to special queues . The GPU watches these queues, receives the command buffers, and processes them in parallel. In Vulkan, recording commands are special commands starting wih vkCmd . In Datoviz, they start with dvz_cmd_ . For example, rendering a single graphics pipeline involves the following recording commands: dvz_cmd_begin (...); // begin recording the command buffer dvz_cmd_begin_renderpass (...); // begin the renderpass dvz_cmd_bind_vertex_buffer (...); // bind an existing GPU buffer as vertex buffer dvz_cmd_bind_index_buffer (...); // bind an existing GPU buffer as index buffer dvz_cmd_bind_graphics (...); // bind an existing graphics pipeline dvz_cmd_viewport (...); // set the viewport dvz_cmd_draw (...); // **perform the actual rendering of the graphics pipeline** dvz_cmd_end_renderpass (...); // end the renderpass dvz_cmd_end (...); // stop recording the command buffer Once called on a command buffer, the command buffer is recorded and can be submitted to a GPU queue. Vulkan leaves to the user the choice of defining the number and types of GPU queues for the application. This is also depends heavily on the hardware. Currently, Datoviz requests four queues, but may end up with less queues if the hardware does not support them (this is all transparent to the user): a transfer queue receives command buffers for buffer/image upload, download, copy, transitions... a compute queue receives command buffers with compute tasks, a render queue receives command buffers with either graphics and/or compute tasks, a present queue is used for the main rendering loop (swapchain). Vulkan has been designed with the idea that command buffers will be constantly recreated and/or reused. Multithreaded applications can record command buffers in parallel, and must use special synchronization primitives to efficiently submit the command buffers to the GPU. Scientific applications are typically much less dynamic than video games. Still, some amount of dynamism is expected in some applications. Therefore, Datoviz assumes that command buffers are typically not recreated at every frame, but allows for relatively efficient command buffer recreation when needed. Here are the different ways the GPU objects may change during the lifetime of an application: changing buffer or texture data : update a GPU buffer or texture, doesn't require command buffer recreation, interactivity (pan and zoom, arcball, and so on): update a uniform buffer, changing the number of vertices in a given graphics pipeline: require command buffer recreation (unless using indirect rendering , in which case this involves updating a GPU buffer), push constant change : require command buffer recreation, resize , panel change , viewport change : require command buffer recreation. Datoviz command buffers and queues are based on the following Vulkan objects: CommandPool, CommandBuffer, Queue.","title":"Recording commands for the GPU"},{"location":"discussions/vulkan/#main-rendering-loop","text":"Once the command buffers have been recorded, one needs to submit them to the GPU and render the scene in a window. This step must be done manually when using the Vulkan API. Again, this is abstracted away in Datoviz, at the level of the canvas . This step is actually one of the most complex ones in Vulkan, especially when there's a need to do it as efficiently as possible. First, one considers that the window size is fixed (until it is not). Resizing is a complex operation that requires destroying and recreating a large number of Vulkan objects, and it needs to be handled correctly in the rendering loop. Second, one needs to create a GPU image (like the object that is associated to a texture, but without a sampler) that the GPU will render to. This image will be presented to the screen. One must also define another special image of the same size for the depth buffer , essential with 3D rendering. Third, one must acquire a surface , a special Vulkan object that is used to render something to the window. Creating a window is an OS-dependent operation. Datoviz uses the glfw window library that abstracts these details away and offers an easy way to create windows and to deal with user inputs (mouse, keyboard). Fourth, one needs to create a swapchain . This object provides a way to implement a technique sometimes called double-buffering, or triple-buffering, depending on the number of images used in the swapchain. The idea is to avoid making the GPU wait while an image is being presented to the screen . For example, with a frame rate of 60 images per second, each image remains on screen during about 16 milliseconds. During this time, the GPU is not expected to render to the same image, unless it makes a copy. That's basically the idea of the swapchain: providing a set of 2, 3 or more images that are almost identical. While the image 0 is presented to the screen, the GPU can render the next frame on image 1. When it finishes, it presents image 1 to the screen, while image 0 is being rewritten for the next frame (double buffering), or while it waits until the GPU requests it. This logic must be, in part, implemented by the developer who uses the Vulkan API directly. Datoviz completely abstracts this process away. Fifth, one needs to define a render pass and a set of framebuffers . The render pass defines the way the GPU renders an image, in one or several steps. The framebuffers represent the links between the GPU images and the render pass steps. The render pass must be specified when recording a rendering command buffer. Sixth, one needs to implement the main rendering loop . This is typically an infinite loop where every iteration represents a frame. At every frame, the rendering loop performs the following (simplified) steps: Examine the user events (mouse, keyboard) that occurred in the window since the last frame, Perform the resize if the window size has changed since the last frame, Implement the scene logic (update internal variables as a function of user events and time), Perform the pending transfers (upload/download of GPU buffers/textures) that have been requested since last frame, possibly from a background thread, Optionally, record new command buffers, Acquire a new swapchain image for rendering, Wait until the previous frame that was using the same swapchain image (might be 2 or 3 frames before) has finished rendering, Submit the command buffers to their associated GPU queues, which will render the image, Present the image to the screen, but only after the GPU has finished rendering it (asynchronous operation). This logic is essentially implemented in: dvz_canvas_frame() dvz_canvas_frame_submit() dvz_app_run() The rendering loop involves the following Vulkan objects: SurfaceKHR, SwapchainKHR, Image, ImageView, RenderPass, Framebuffer.","title":"Main rendering loop"},{"location":"discussions/vulkan/#synchronization","text":"The last important topic pertains to synchronization . The GPU should be seen as a partly independent device on which tasks are submitted asynchronously (via command buffers and queues), and that processes them in parallel. In addition, each task may involve a massively parallel architecture (for example, processing thousands of vertices or fragments in parallel). Vulkan provides an API to let the CPU communicate with the GPU. A highly inefficient way would be for the CPU to wait until the GPU is idle before submitting new tasks or uploading data. This would be done via \"hard\" synchronization primitives that are implemented in the Vulkan functions vkQueueWaitIdle() and vkDeviceWaitIdle() , and in the dvz_queue|gpu|app_wait() functions in Datoviz. Doing it this way would work and would not require any other more fine-grained synchronization primitive, but it would result in poor performance. Vulkan provides several more fine-grained synchronization primitives, of which Datoviz currently supports three: Inner command buffer synchronization , provided by barriers, GPU-GPU synchronization , provided by semaphores. CPU-GPU synchronization , provided by fences, A barrier is a way for a command buffer to let the GPU know that some recorded commands should wait for completion of other commands. For example, if a command buffer involves launching a compute shader on a buffer, then rendering to a graphics pipeline, a barrier should be defined if ever the graphics pipeline uses the same buffer as used by the compute shader. The graphics pipeline should not start reading a buffer while the same buffer is being written to by the compute pipeline. A semaphore is a way to introduce dependencies between different submissions of command buffers. They are used in the main rendering loop and swapchain logic. When the GPU has finished rendering an image, then this image should be presented to the screen. This is implemented with a semaphore. A fence is a way for the CPU to wait until the GPU has finished some task. This is also used in the main rendering loop: it wouldn't make much sense to start the current frame until the previous frame has not finished rendering (but it can start while the swapchain presents the previously rendered image to the screen). Synchronization is required when uploading data to the GPU. Since the GPU may be constantly reading from GPU buffers and textures, data should not be uploaded to these GPU objects without proper synchronization. Otherwise, the GPU might use corrupted data while rendering the scene.","title":"Synchronization"},{"location":"discussions/vulkan/#technical-details","text":"The vklite API documentation contains the documentation of all Vulkan-related functions. There is no other documentation of vklite at the moment, however you can look at the unit tests to get an idea of how to use each vklite object.","title":"Technical details"},{"location":"discussions/vulkan/#gpu-features-and-limits","text":"The DvzGpu structure contains a few fields with native Vulkan structures defining GPU capabilities and limits. We give here a few minimal values that we can reasonably expect on almost all devices (according to this Vulkan database ): Texture dimension Maximum allowed texture size (in any axis) 1D 16384 2D 16384 3D 2048","title":"GPU features and limits"},{"location":"examples/","text":"Examples gallery \u00b6 Python examples and screenshots.","title":"Overview"},{"location":"examples/#examples-gallery","text":"Python examples and screenshots.","title":"Examples gallery"},{"location":"examples/brain/","text":"3D brain mesh \u00b6 # from `bindings/python/examples/brain.py` import numpy as np from nilearn import datasets from nilearn.surface import load_surf_data , load_surf_mesh , vol_to_surf from nilearn import plotting from datoviz import canvas , run , colormap # Get the data: fsaverage = datasets . fetch_surf_fsaverage () # Left hemisphere. mesh = load_surf_mesh ( fsaverage [ 'pial_left' ]) coords , faces = mesh [ 0 ], mesh [ 1 ] bg_data = load_surf_data ( fsaverage [ 'sulc_left' ]) # Right hemisphere. mesh = load_surf_mesh ( fsaverage [ 'pial_right' ]) coords2 , faces2 = mesh [ 0 ], mesh [ 1 ] bg_data2 = load_surf_data ( fsaverage [ 'sulc_right' ]) # Concatenate. coords = np . vstack (( coords , coords2 )) faces = np . vstack (( faces , faces2 + faces . max () + 1 )) bg_data = np . concatenate (( bg_data , bg_data2 )) # Depth background data. bg_data = ( bg_data - bg_data . min ()) / ( bg_data . max () - bg_data . min ()) N = bg_data . shape [ 0 ] # HACK: uv tex coords to fetch the right colormap value. To be improved cmap = 0 uv = np . c_ [ bg_data , np . ones ( N ) * cmap / 256.0 + . 5 / 256.0 ] # Plot the data: c = canvas ( show_fps = False , width = 1024 , height = 768 ) panel = c . panel ( controller = 'arcball' ) visual = panel . visual ( 'mesh' , transform = 'auto' ) visual . data ( 'pos' , coords ) visual . data ( 'texcoords' , uv ) visual . data ( 'index' , faces . ravel ()) # Light parameters light_params = np . zeros (( 4 , 4 )) # up to 4 lights # ambient, diffuse, specular, specular exponent light_params [ 0 , :] = ( . 4 , . 4 , . 2 , 64 ) visual . data ( 'light_params' , light_params ) gui = c . gui ( \"GUI\" ) @gui . control ( \"slider_float\" , \"glossy\" , value =. 2 , vmin = 0 , vmax = 1 ) def on_change ( value ): light_params [ 0 , 2 ] = value visual . data ( 'light_params' , light_params ) run ()","title":"3D brain mesh"},{"location":"examples/brain/#3d-brain-mesh","text":"# from `bindings/python/examples/brain.py` import numpy as np from nilearn import datasets from nilearn.surface import load_surf_data , load_surf_mesh , vol_to_surf from nilearn import plotting from datoviz import canvas , run , colormap # Get the data: fsaverage = datasets . fetch_surf_fsaverage () # Left hemisphere. mesh = load_surf_mesh ( fsaverage [ 'pial_left' ]) coords , faces = mesh [ 0 ], mesh [ 1 ] bg_data = load_surf_data ( fsaverage [ 'sulc_left' ]) # Right hemisphere. mesh = load_surf_mesh ( fsaverage [ 'pial_right' ]) coords2 , faces2 = mesh [ 0 ], mesh [ 1 ] bg_data2 = load_surf_data ( fsaverage [ 'sulc_right' ]) # Concatenate. coords = np . vstack (( coords , coords2 )) faces = np . vstack (( faces , faces2 + faces . max () + 1 )) bg_data = np . concatenate (( bg_data , bg_data2 )) # Depth background data. bg_data = ( bg_data - bg_data . min ()) / ( bg_data . max () - bg_data . min ()) N = bg_data . shape [ 0 ] # HACK: uv tex coords to fetch the right colormap value. To be improved cmap = 0 uv = np . c_ [ bg_data , np . ones ( N ) * cmap / 256.0 + . 5 / 256.0 ] # Plot the data: c = canvas ( show_fps = False , width = 1024 , height = 768 ) panel = c . panel ( controller = 'arcball' ) visual = panel . visual ( 'mesh' , transform = 'auto' ) visual . data ( 'pos' , coords ) visual . data ( 'texcoords' , uv ) visual . data ( 'index' , faces . ravel ()) # Light parameters light_params = np . zeros (( 4 , 4 )) # up to 4 lights # ambient, diffuse, specular, specular exponent light_params [ 0 , :] = ( . 4 , . 4 , . 2 , 64 ) visual . data ( 'light_params' , light_params ) gui = c . gui ( \"GUI\" ) @gui . control ( \"slider_float\" , \"glossy\" , value =. 2 , vmin = 0 , vmax = 1 ) def on_change ( value ): light_params [ 0 , 2 ] = value visual . data ( 'light_params' , light_params ) run ()","title":"3D brain mesh"},{"location":"examples/brain_highres/","text":"3D high-res brain mesh \u00b6 Showing a ultra-high resolution mesh of a human brain, acquired with a 7 Tesla MRI. The data is not yet publicly available. Data courtesy of Anneke Alkemade et al.: Alkemade A, Pine K, Kirilina E, Keuken MC, Mulder MJ, Balesar R, Groot JM, Bleys RLAW, Trampel R, Weiskopf N, Herrler A, M\u00f6ller HE, Bazin P-L and Forstmann BU (2020) *7 Tesla MRI Followed by Histological 3D Reconstructions in Whole-Brain Specimens* Front. Neuroanat. 14:536838 doi: 10.3389/fnana.2020.536838 Acknowledgements to Pierre-Louis Bazin and Julia Huntenburg for data access. # from `bindings/python/examples/brain_highres.py` from pathlib import Path import numpy as np from datoviz import canvas , run , colormap c = canvas ( show_fps = True , width = 1024 , height = 768 ) panel = c . panel ( controller = 'arcball' ) visual = panel . visual ( 'mesh' , transform = 'auto' ) ROOT = Path ( __file__ ) . parent . parent . parent . parent pos = np . load ( ROOT / \"data/mesh/brain_highres.vert.npy\" ) faces = np . load ( ROOT / \"data/mesh/brain_highres.faces.npy\" ) assert pos . ndim == 2 assert pos . shape [ 1 ] == 3 assert faces . ndim == 2 assert faces . shape [ 1 ] == 3 print ( f \"Mesh has { len ( faces ) } triangles and { len ( pos ) } vertices\" ) visual . data ( 'pos' , pos ) visual . data ( 'index' , faces . ravel ()) visual . data ( 'clip' , np . array ([ 0 , 0 , 1 , 1 ])) gui = c . gui ( \"GUI\" ) @gui . control ( \"slider_float\" , \"clip\" , vmin =- 1 , vmax =+ 1 , value =+ 1 ) def on_change ( value ): visual . data ( 'clip' , np . array ([ 0 , 0 , 1 , value ])) run ()","title":"3D high-res brain mesh"},{"location":"examples/brain_highres/#3d-high-res-brain-mesh","text":"Showing a ultra-high resolution mesh of a human brain, acquired with a 7 Tesla MRI. The data is not yet publicly available. Data courtesy of Anneke Alkemade et al.: Alkemade A, Pine K, Kirilina E, Keuken MC, Mulder MJ, Balesar R, Groot JM, Bleys RLAW, Trampel R, Weiskopf N, Herrler A, M\u00f6ller HE, Bazin P-L and Forstmann BU (2020) *7 Tesla MRI Followed by Histological 3D Reconstructions in Whole-Brain Specimens* Front. Neuroanat. 14:536838 doi: 10.3389/fnana.2020.536838 Acknowledgements to Pierre-Louis Bazin and Julia Huntenburg for data access. # from `bindings/python/examples/brain_highres.py` from pathlib import Path import numpy as np from datoviz import canvas , run , colormap c = canvas ( show_fps = True , width = 1024 , height = 768 ) panel = c . panel ( controller = 'arcball' ) visual = panel . visual ( 'mesh' , transform = 'auto' ) ROOT = Path ( __file__ ) . parent . parent . parent . parent pos = np . load ( ROOT / \"data/mesh/brain_highres.vert.npy\" ) faces = np . load ( ROOT / \"data/mesh/brain_highres.faces.npy\" ) assert pos . ndim == 2 assert pos . shape [ 1 ] == 3 assert faces . ndim == 2 assert faces . shape [ 1 ] == 3 print ( f \"Mesh has { len ( faces ) } triangles and { len ( pos ) } vertices\" ) visual . data ( 'pos' , pos ) visual . data ( 'index' , faces . ravel ()) visual . data ( 'clip' , np . array ([ 0 , 0 , 1 , 1 ])) gui = c . gui ( \"GUI\" ) @gui . control ( \"slider_float\" , \"clip\" , vmin =- 1 , vmax =+ 1 , value =+ 1 ) def on_change ( value ): visual . data ( 'clip' , np . array ([ 0 , 0 , 1 , value ])) run ()","title":"3D high-res brain mesh"},{"location":"examples/france/","text":"France departements as polygons \u00b6 # from `bindings/python/examples/france.py` from pathlib import Path import numpy as np import numpy.random as nr from datoviz import canvas , run , colormap ROOT = Path ( __file__ ) . resolve () . parent . parent . parent . parent pos = np . fromfile ( ROOT / \"data/misc/departements.polypoints.bin\" , dtype = np . float64 ) pos = pos . reshape (( - 1 , 2 )) pos = np . c_ [ pos [:, 1 ], pos [:, 0 ], np . zeros ( pos . shape [ 0 ])] # latitude, longitude, 0 # Web Mercator projection lat , lon , _ = pos . T lonrad = lon / 180.0 * np . pi latrad = lat / 180.0 * np . pi zoom = 1 c = 256 / 2 * np . pi * 2 ** zoom x = c * ( lonrad + np . pi ) y = - c * ( np . pi - np . log ( np . tan ( np . pi / 4.0 + latrad / 2.0 ))) pos = np . c_ [ x , y , _ ] length = np . fromfile ( ROOT / \"data/misc/departements.polylengths.bin\" , dtype = np . uint32 ) N = len ( length ) color = colormap ( nr . rand ( N ), vmin = 0 , vmax = 1 , cmap = 'viridis' ) c = canvas ( show_fps = False ) panel = c . panel ( controller = 'axes' ) visual = panel . visual ( 'polygon' ) visual . data ( 'pos' , pos ) visual . data ( 'length' , length ) visual . data ( 'color' , color ) run ()","title":"France departements"},{"location":"examples/france/#france-departements-as-polygons","text":"# from `bindings/python/examples/france.py` from pathlib import Path import numpy as np import numpy.random as nr from datoviz import canvas , run , colormap ROOT = Path ( __file__ ) . resolve () . parent . parent . parent . parent pos = np . fromfile ( ROOT / \"data/misc/departements.polypoints.bin\" , dtype = np . float64 ) pos = pos . reshape (( - 1 , 2 )) pos = np . c_ [ pos [:, 1 ], pos [:, 0 ], np . zeros ( pos . shape [ 0 ])] # latitude, longitude, 0 # Web Mercator projection lat , lon , _ = pos . T lonrad = lon / 180.0 * np . pi latrad = lat / 180.0 * np . pi zoom = 1 c = 256 / 2 * np . pi * 2 ** zoom x = c * ( lonrad + np . pi ) y = - c * ( np . pi - np . log ( np . tan ( np . pi / 4.0 + latrad / 2.0 ))) pos = np . c_ [ x , y , _ ] length = np . fromfile ( ROOT / \"data/misc/departements.polylengths.bin\" , dtype = np . uint32 ) N = len ( length ) color = colormap ( nr . rand ( N ), vmin = 0 , vmax = 1 , cmap = 'viridis' ) c = canvas ( show_fps = False ) panel = c . panel ( controller = 'axes' ) visual = panel . visual ( 'polygon' ) visual . data ( 'pos' , pos ) visual . data ( 'length' , length ) visual . data ( 'color' , color ) run ()","title":"France departements as polygons"},{"location":"examples/image/","text":"Image \u00b6 # from `bindings/python/examples/image.py` from pathlib import Path import numpy as np import numpy.random as nr import imageio from datoviz import canvas , run , colormap ROOT = Path ( __file__ ) . parent . parent . parent . parent c = canvas ( show_fps = True ) panel = c . panel ( controller = 'panzoom' ) visual = panel . visual ( 'image' ) # Top left, top right, bottom right, bottom left visual . data ( 'pos' , np . array ([[ - 1 , + 1 , 0 ]]), idx = 0 ) visual . data ( 'pos' , np . array ([[ + 1 , + 1 , 0 ]]), idx = 1 ) visual . data ( 'pos' , np . array ([[ + 1 , - 1 , 0 ]]), idx = 2 ) visual . data ( 'pos' , np . array ([[ - 1 , - 1 , 0 ]]), idx = 3 ) visual . data ( 'texcoords' , np . atleast_2d ([ 0 , 0 ]), idx = 0 ) visual . data ( 'texcoords' , np . atleast_2d ([ 1 , 0 ]), idx = 1 ) visual . data ( 'texcoords' , np . atleast_2d ([ 1 , 1 ]), idx = 2 ) visual . data ( 'texcoords' , np . atleast_2d ([ 0 , 1 ]), idx = 3 ) # First texture. img = imageio . imread ( ROOT / 'data/textures/earth.jpg' ) img = np . dstack (( img , 255 * np . ones ( img . shape [: 2 ]))) img = np . transpose ( img , ( 1 , 0 , 2 )) img = img . astype ( np . uint8 ) visual . image ( img , filtering = 'nearest' , idx = 0 ) # Second texture. n = 256 t = np . linspace ( - 1 , + 1 , n ) x , y = np . meshgrid ( t , t ) z = np . exp ( - 2 * ( x * x + y * y )) z = ( z * 255 ) . astype ( np . uint8 ) img = np . dstack (( z , z , z , 255 * np . ones_like ( z ))) . astype ( np . uint8 ) visual . image ( img , filtering = 'nearest' , idx = 1 ) visual . data ( 'texcoefs' , np . array ([ 1 , . 5 , 0 , 0 ]) . astype ( np . float32 )) # Control the blending via a GUI. gui = c . gui ( \"GUI\" ) @gui . control ( \"slider_float\" , \"blending\" , vmin = 0 , vmax = 1 ) def on_change ( value ): visual . data ( 'texcoefs' , np . array ([ 1 - value , value , 0 , 0 ])) run ()","title":"Image"},{"location":"examples/image/#image","text":"# from `bindings/python/examples/image.py` from pathlib import Path import numpy as np import numpy.random as nr import imageio from datoviz import canvas , run , colormap ROOT = Path ( __file__ ) . parent . parent . parent . parent c = canvas ( show_fps = True ) panel = c . panel ( controller = 'panzoom' ) visual = panel . visual ( 'image' ) # Top left, top right, bottom right, bottom left visual . data ( 'pos' , np . array ([[ - 1 , + 1 , 0 ]]), idx = 0 ) visual . data ( 'pos' , np . array ([[ + 1 , + 1 , 0 ]]), idx = 1 ) visual . data ( 'pos' , np . array ([[ + 1 , - 1 , 0 ]]), idx = 2 ) visual . data ( 'pos' , np . array ([[ - 1 , - 1 , 0 ]]), idx = 3 ) visual . data ( 'texcoords' , np . atleast_2d ([ 0 , 0 ]), idx = 0 ) visual . data ( 'texcoords' , np . atleast_2d ([ 1 , 0 ]), idx = 1 ) visual . data ( 'texcoords' , np . atleast_2d ([ 1 , 1 ]), idx = 2 ) visual . data ( 'texcoords' , np . atleast_2d ([ 0 , 1 ]), idx = 3 ) # First texture. img = imageio . imread ( ROOT / 'data/textures/earth.jpg' ) img = np . dstack (( img , 255 * np . ones ( img . shape [: 2 ]))) img = np . transpose ( img , ( 1 , 0 , 2 )) img = img . astype ( np . uint8 ) visual . image ( img , filtering = 'nearest' , idx = 0 ) # Second texture. n = 256 t = np . linspace ( - 1 , + 1 , n ) x , y = np . meshgrid ( t , t ) z = np . exp ( - 2 * ( x * x + y * y )) z = ( z * 255 ) . astype ( np . uint8 ) img = np . dstack (( z , z , z , 255 * np . ones_like ( z ))) . astype ( np . uint8 ) visual . image ( img , filtering = 'nearest' , idx = 1 ) visual . data ( 'texcoefs' , np . array ([ 1 , . 5 , 0 , 0 ]) . astype ( np . float32 )) # Control the blending via a GUI. gui = c . gui ( \"GUI\" ) @gui . control ( \"slider_float\" , \"blending\" , vmin = 0 , vmax = 1 ) def on_change ( value ): visual . data ( 'texcoefs' , np . array ([ 1 - value , value , 0 , 0 ])) run ()","title":"Image"},{"location":"examples/molecule/","text":"Molecule 3D mesh \u00b6 # from `bindings/python/examples/molecule.py` import numpy as np import numpy.random as nr from datoviz import canvas , run , colormap c = canvas ( show_fps = False ) panel = c . panel ( controller = 'arcball' ) visual = panel . visual ( 'mesh' ) visual . load_obj ( \"data/mesh/cas9_guide.obj\" ) run ()","title":"Molecule 3D mesh"},{"location":"examples/molecule/#molecule-3d-mesh","text":"# from `bindings/python/examples/molecule.py` import numpy as np import numpy.random as nr from datoviz import canvas , run , colormap c = canvas ( show_fps = False ) panel = c . panel ( controller = 'arcball' ) visual = panel . visual ( 'mesh' ) visual . load_obj ( \"data/mesh/cas9_guide.obj\" ) run ()","title":"Molecule 3D mesh"},{"location":"examples/quickstart/","text":"Simple test with Python \u00b6 # from `bindings/python/examples/quickstart.py` import time import numpy as np import numpy.random as nr # Import the library. from datoviz import canvas , run , colormap # Create a new canvas and scene. There's only 1 subplot (panel) by default. c = canvas ( show_fps = True ) # Get a panel (by default, the one spanning the entire canvas) # We specify the type of controller we want. Here, we want 2D axes. panel = c . panel ( controller = 'axes' ) # We create a new \"marker\" visual. visual = panel . visual ( 'marker' ) # We prepare the visual properties. Here, we set the marker positions, colors (RGBA bytes), # and size (in pixels). N = 100_000 pos = nr . randn ( N , 3 ) ms = nr . uniform ( low = 2 , high = 35 , size = N ) color_values = nr . rand ( N ) # Use a built-in colormap color = colormap ( color_values , vmin = 0 , vmax = 1 , alpha =. 75 * np . ones ( N ), cmap = 'viridis' ) # Set the visual props. visual . data ( 'pos' , pos ) visual . data ( 'color' , color ) visual . data ( 'ms' , ms ) @c . connect def on_mouse_click ( x , y , button , modifiers = ()): # x, y are in pixel coordinates # First, we find the picked panel p = c . panel_at ( x , y ) if p : # Then, we transform into the data coordinate system # Supported coordinate systems # target_cds='data' / 'scene' / 'vulkan' / 'framebuffer' / 'window' xd , yd = p . pick ( x , y ) print ( f \"Pick at ( { xd : .4f } , { yd : .4f } ), modifiers= { modifiers } \" ) # We create a GUI dialog. gui = c . gui ( \"hello world\" ) # We add a control, a slider controlling a float between 0 and 1 (by default) @gui . control ( \"slider_float\" , \"my float slider\" ) def on_change ( value ): # Every time the slider value changes, we update the marker size visual . data ( 'ms' , ms * value ) # We add another control, a slider controlling an int between 1 and 4 cmaps = [ 'viridis' , 'cividis' , 'autumn' , 'winter' ] @gui . control ( \"slider_int\" , \"my int slider\" , vmin = 0 , vmax = 3 ) def on_change ( value ): color = colormap ( color_values , vmin = 0 , vmax = 1 , alpha =. 75 * np . ones ( N ), cmap = cmaps [ value ]) visual . data ( 'color' , color ) # We add a button to regenerate the marker positions @gui . control ( \"button\" , \"regenerate\" ) def on_change ( value ): pos = nr . randn ( N , 3 ) visual . data ( 'pos' , pos ) # We run the main event loop, which will display the canvas until Escape is pressed or the # window is closed. run ()","title":"Simple test with Python"},{"location":"examples/quickstart/#simple-test-with-python","text":"# from `bindings/python/examples/quickstart.py` import time import numpy as np import numpy.random as nr # Import the library. from datoviz import canvas , run , colormap # Create a new canvas and scene. There's only 1 subplot (panel) by default. c = canvas ( show_fps = True ) # Get a panel (by default, the one spanning the entire canvas) # We specify the type of controller we want. Here, we want 2D axes. panel = c . panel ( controller = 'axes' ) # We create a new \"marker\" visual. visual = panel . visual ( 'marker' ) # We prepare the visual properties. Here, we set the marker positions, colors (RGBA bytes), # and size (in pixels). N = 100_000 pos = nr . randn ( N , 3 ) ms = nr . uniform ( low = 2 , high = 35 , size = N ) color_values = nr . rand ( N ) # Use a built-in colormap color = colormap ( color_values , vmin = 0 , vmax = 1 , alpha =. 75 * np . ones ( N ), cmap = 'viridis' ) # Set the visual props. visual . data ( 'pos' , pos ) visual . data ( 'color' , color ) visual . data ( 'ms' , ms ) @c . connect def on_mouse_click ( x , y , button , modifiers = ()): # x, y are in pixel coordinates # First, we find the picked panel p = c . panel_at ( x , y ) if p : # Then, we transform into the data coordinate system # Supported coordinate systems # target_cds='data' / 'scene' / 'vulkan' / 'framebuffer' / 'window' xd , yd = p . pick ( x , y ) print ( f \"Pick at ( { xd : .4f } , { yd : .4f } ), modifiers= { modifiers } \" ) # We create a GUI dialog. gui = c . gui ( \"hello world\" ) # We add a control, a slider controlling a float between 0 and 1 (by default) @gui . control ( \"slider_float\" , \"my float slider\" ) def on_change ( value ): # Every time the slider value changes, we update the marker size visual . data ( 'ms' , ms * value ) # We add another control, a slider controlling an int between 1 and 4 cmaps = [ 'viridis' , 'cividis' , 'autumn' , 'winter' ] @gui . control ( \"slider_int\" , \"my int slider\" , vmin = 0 , vmax = 3 ) def on_change ( value ): color = colormap ( color_values , vmin = 0 , vmax = 1 , alpha =. 75 * np . ones ( N ), cmap = cmaps [ value ]) visual . data ( 'color' , color ) # We add a button to regenerate the marker positions @gui . control ( \"button\" , \"regenerate\" ) def on_change ( value ): pos = nr . randn ( N , 3 ) visual . data ( 'pos' , pos ) # We run the main event loop, which will display the canvas until Escape is pressed or the # window is closed. run ()","title":"Simple test with Python"},{"location":"examples/raster/","text":"Raster plot \u00b6 # from `bindings/python/examples/raster.py` from pathlib import Path ROOT = Path ( __file__ ) . parent . parent . parent . parent import numpy as np import numpy.random as nr from datoviz import canvas , run , colormap c = canvas ( show_fps = True ) panel = c . panel ( controller = 'axes' ) visual = panel . visual ( 'point' ) amps = np . load ( ROOT / \"data/misc/spikes.amps.npy\" ) spike_clusters = np . load ( ROOT / \"data/misc/spikes.clusters.npy\" ) spike_depths = np . load ( ROOT / \"data/misc/spikes.depths.npy\" ) spike_times = np . load ( ROOT / \"data/misc/spikes.times.npy\" ) N = len ( spike_times ) print ( f \" { N } spikes\" ) pos = np . c_ [ spike_times , spike_depths , np . zeros ( N )] color = colormap ( spike_clusters . astype ( np . float64 ), cmap = 'glasbey' , alpha =. 5 ) visual . data ( 'pos' , pos ) visual . data ( 'color' , color ) visual . data ( 'ms' , np . array ([ 2. ])) run ()","title":"Raster plot"},{"location":"examples/raster/#raster-plot","text":"# from `bindings/python/examples/raster.py` from pathlib import Path ROOT = Path ( __file__ ) . parent . parent . parent . parent import numpy as np import numpy.random as nr from datoviz import canvas , run , colormap c = canvas ( show_fps = True ) panel = c . panel ( controller = 'axes' ) visual = panel . visual ( 'point' ) amps = np . load ( ROOT / \"data/misc/spikes.amps.npy\" ) spike_clusters = np . load ( ROOT / \"data/misc/spikes.clusters.npy\" ) spike_depths = np . load ( ROOT / \"data/misc/spikes.depths.npy\" ) spike_times = np . load ( ROOT / \"data/misc/spikes.times.npy\" ) N = len ( spike_times ) print ( f \" { N } spikes\" ) pos = np . c_ [ spike_times , spike_depths , np . zeros ( N )] color = colormap ( spike_clusters . astype ( np . float64 ), cmap = 'glasbey' , alpha =. 5 ) visual . data ( 'pos' , pos ) visual . data ( 'color' , color ) visual . data ( 'ms' , np . array ([ 2. ])) run ()","title":"Raster plot"},{"location":"examples/signals/","text":"Digital signals \u00b6 # from `bindings/python/examples/signals.py` import numpy as np import numpy.random as nr from datoviz import canvas , run , colormap c = canvas ( show_fps = True ) panel = c . panel ( controller = 'panzoom' ) visual = panel . visual ( 'line_strip' ) n_signals = 200 n_points = 2000 n_vert = n_signals * n_points t = np . linspace ( 0 , 5 , n_points ) x = np . tile ( t , ( n_signals ,)) assert x . ndim == 1 y = . 2 * nr . randn ( n_signals , n_points ) offsets = np . tile ( np . arange ( n_signals )[:, np . newaxis ], ( 1 , n_points )) y += offsets pos = np . c_ [ x , y . ravel (), np . zeros ( n_vert )] color = np . repeat ( colormap ( np . linspace ( 0 , 1 , n_signals ), cmap = 'glasbey' ), n_points , axis = 0 ) length = np . repeat ( np . array ([ n_points ]), n_signals ) assert pos . shape == ( n_vert , 3 ) assert color . shape == ( n_vert , 4 ) assert length . shape == ( n_signals ,) visual . data ( 'pos' , pos ) visual . data ( 'color' , color ) visual . data ( 'length' , length ) i = 0 k = 50 def f (): global i yk = . 2 * nr . randn ( n_signals , k ) offsets = np . tile ( np . arange ( n_signals )[:, np . newaxis ], ( 1 , k )) yk += offsets y [:, i * k :( i + 1 ) * k ] = yk pos [:, 1 ] = y . ravel () visual . data ( 'pos' , pos ) i += 1 i = i % ( n_points // k ) c . _connect ( 'timer' , f , . 05 ) run ()","title":"Digital signals"},{"location":"examples/signals/#digital-signals","text":"# from `bindings/python/examples/signals.py` import numpy as np import numpy.random as nr from datoviz import canvas , run , colormap c = canvas ( show_fps = True ) panel = c . panel ( controller = 'panzoom' ) visual = panel . visual ( 'line_strip' ) n_signals = 200 n_points = 2000 n_vert = n_signals * n_points t = np . linspace ( 0 , 5 , n_points ) x = np . tile ( t , ( n_signals ,)) assert x . ndim == 1 y = . 2 * nr . randn ( n_signals , n_points ) offsets = np . tile ( np . arange ( n_signals )[:, np . newaxis ], ( 1 , n_points )) y += offsets pos = np . c_ [ x , y . ravel (), np . zeros ( n_vert )] color = np . repeat ( colormap ( np . linspace ( 0 , 1 , n_signals ), cmap = 'glasbey' ), n_points , axis = 0 ) length = np . repeat ( np . array ([ n_points ]), n_signals ) assert pos . shape == ( n_vert , 3 ) assert color . shape == ( n_vert , 4 ) assert length . shape == ( n_signals ,) visual . data ( 'pos' , pos ) visual . data ( 'color' , color ) visual . data ( 'length' , length ) i = 0 k = 50 def f (): global i yk = . 2 * nr . randn ( n_signals , k ) offsets = np . tile ( np . arange ( n_signals )[:, np . newaxis ], ( 1 , k )) yk += offsets y [:, i * k :( i + 1 ) * k ] = yk pos [:, 1 ] = y . ravel () visual . data ( 'pos' , pos ) i += 1 i = i % ( n_points // k ) c . _connect ( 'timer' , f , . 05 ) run ()","title":"Digital signals"},{"location":"examples/twopanels/","text":"Test with two panels and different controllers \u00b6 # from `bindings/python/examples/twopanels.py` import numpy as np import numpy.random as nr from datoviz import canvas , run , colormap N = 100_000 pos = nr . randn ( N , 3 ) ms = nr . uniform ( low = 2 , high = 40 , size = N ) color = colormap ( nr . rand ( N ), vmin = 0 , vmax = 1 , alpha =. 75 * np . ones ( N )) c = canvas ( rows = 1 , cols = 2 , show_fps = True ) panel0 = c . panel ( 0 , 0 , controller = 'axes' ) panel1 = c . panel ( 0 , 1 , controller = 'arcball' ) visual = panel0 . visual ( 'marker' ) visual . data ( 'pos' , pos ) visual . data ( 'color' , color ) visual . data ( 'ms' , ms ) visual1 = panel1 . visual ( 'marker' , depth_test = True ) visual1 . data ( 'pos' , pos ) visual1 . data ( 'color' , color ) visual1 . data ( 'ms' , ms ) run ()","title":"Two panels"},{"location":"examples/twopanels/#test-with-two-panels-and-different-controllers","text":"# from `bindings/python/examples/twopanels.py` import numpy as np import numpy.random as nr from datoviz import canvas , run , colormap N = 100_000 pos = nr . randn ( N , 3 ) ms = nr . uniform ( low = 2 , high = 40 , size = N ) color = colormap ( nr . rand ( N ), vmin = 0 , vmax = 1 , alpha =. 75 * np . ones ( N )) c = canvas ( rows = 1 , cols = 2 , show_fps = True ) panel0 = c . panel ( 0 , 0 , controller = 'axes' ) panel1 = c . panel ( 0 , 1 , controller = 'arcball' ) visual = panel0 . visual ( 'marker' ) visual . data ( 'pos' , pos ) visual . data ( 'color' , color ) visual . data ( 'ms' , ms ) visual1 = panel1 . visual ( 'marker' , depth_test = True ) visual1 . data ( 'pos' , pos ) visual1 . data ( 'color' , color ) visual1 . data ( 'ms' , ms ) run ()","title":"Test with two panels and different controllers"},{"location":"howto/","text":"How to guides \u00b6 This section provides step-by-step how-to guides for intermediate to advanced users: How to write a standalone C app using the scene API, How to write a custom visual in C, by reusing existing shaders and without writing custom shaders (Python bindings coming soon), How to write a custom graphics in C and GLSL, by writing custom shaders , How to use the canvas C API to create standalone applications that do not use the high-level scene API, How to use the vklite C API to create standalone offscreen and/or compute applications that only use the vklite API (thin wrapper on top of the Vulkan C API).","title":"Index"},{"location":"howto/#how-to-guides","text":"This section provides step-by-step how-to guides for intermediate to advanced users: How to write a standalone C app using the scene API, How to write a custom visual in C, by reusing existing shaders and without writing custom shaders (Python bindings coming soon), How to write a custom graphics in C and GLSL, by writing custom shaders , How to use the canvas C API to create standalone applications that do not use the high-level scene API, How to use the vklite C API to create standalone offscreen and/or compute applications that only use the vklite API (thin wrapper on top of the Vulkan C API).","title":"How to guides"},{"location":"howto/graphics/","text":"How to write a custom graphics \u00b6 Warning You need to go through the Vulkan crash course before following this guide. You also need to go through the how to write a custom visual guide first. In this section, we'll show how to create a custom graphics by writing custom GLSL shaders . This is an advanced topic as it requires understanding the basic of GPU graphics programming. Note Only the C API supports custom graphics at the moment. Python bindings for custom graphics will come in an upcoming version. Datoviz already includes the code necessary to compile GLSL shaders to SPIR-V on the fly (based on Khronos glslang ). The full source code for this example can be found in examples/custom_graphics.c . As a toy example, we'll create a graphics with square points of various size and color . Specifically, we'll require each point to have a different size (which is not supported in the existing point graphics), and a color depending directly on the size and computed directly on the GPU. We'll use a single byte per vertex to store the vertex size (between 0 and 255 pixels), and we'll use no memory for the color since it will be determined directly by this value. This kind of memory optimization is one of the use-cases of writing custom graphics. Limiting memory usage is important when dealing with extremely large datasets (tens or hundreds of millions of points). Writing a custom graphics involves the following steps: Choosing the primitive type. Defining the vertex data structure and corresponding vertex shader attributes. Writing the vertex shader. Writing the fragment shader. Compiling the GLSL shaders to SPIR-V (may be done automatically in the future). Writing the custom graphics. Writing the custom visual. Writing a simple test. Note Here, we show how to create a custom graphics compatible with the scene API, so that it can be used added to the scene the same way as builtin visuals. One could however decide not to use the scene API at all, and leverage instead the vklite API directly. This allows one to create entirely custom and standalone applications. This is not documented at the moment, and one should look at the vklite unit tests to understand how to use the vklite API directly. Choosing the graphics primitive type \u00b6 Vulkan supports six major types of primitives: point list : square points with an arbitrary size, line list : disjoint aliased line segments, line strip : joined aliased line segments, triangle strip : joined triangles consecutively sharing an edge, triangle fan : joined triangles all sharing a single corner (the first vertex), triangle list : disjoint triangles, Other less common primitive types are described in the Vulkan specification . Warning Triangle fans are not supported on macOS. Schematic from the Vulkan Cookbook by Pawel Lapinski, O'Reilly Note The circular arrows in the triangles above indicate the orientation of the triangles, which is taken into account by the GPU. It is good practice to always ensure that all triangles constituting a given object are consistent. For example, when triangulating a square with two triangles, the order of the vertices should be chosen such that both triangles are directly oriented. The graphics pipeline can be configured to handle triangle orientation in a specific way. For example, one can make a graphics pipeline where indirectly oriented triangles are automatically discarded. Line primitives typically have a width of 1 pixel, although some hardware supports thicker lines. In Datoviz, thick, high-quality lines are implemented with triangles instead (line triangulation), and the antialiased thick line is drawn directly in the fragment shader. \"Basic\" line primitives are only used for testing and for special high-performance applications where scalability and performance are more important than visual quality. The most commonly-used primitive types in scientific visualization are essentially point lists and triangle lists (and to a lesser extent, triangle strips). In this example, we'll use a point list , where each point will correspond to one square. Defining the vertex data structure \u00b6 Once we know we'll use a point list as a primitive, the next step is to define the vertex data structure / vertex shader attributes. Here, we want the following attributes for each vertex: the point position (center of the square), as a vec3 , the point size (edge length of the square, in pixels), as an uint8_t . The point size will be limited to a maximum of 255 pixels in this toy example. We could have simply used a floating-point for the edge length, but that would have used four bytes instead of one. This is also to show the versatility of Vulkan and GPU programming, where we have full control on the data types used on both the GPU and CPU. Here is our vertex C structure: // We define the vertex structure for our graphics pipeline. typedef struct { vec3 pos ; // 3D point position uint8_t size ; // point size, in pixels, between 0 and 255. } PointVertex ; We also need to define the attributes in GLSL: layout ( location = 0 ) in vec3 pos ; // x, y, z positions layout ( location = 1 ) in float size ; // point size, between 0.0 and 255.0 Attribute format \u00b6 In this example, we use a uint8_t (byte) on the CPU, but a float on the GPU. Vulkan provides a way to specify the way we want to do the casting . This is done when creating the graphics pipeline below. We can just say here that we'll use the Vulkan format VK_FORMAT_R8_USCALED . This format means that: Number of components . There is a single component (scalar), the R in R8 means \"red\" component: this is the component used for scalar values. If using a pair of components, we would have both a R and G component (green). This naming convention using color names comes from the fact that these GPU formats are typically used to represent colors, but they can actually represent any type of numerical data. Number of bits per component . The 8 in R8 means that our red component is represented with 8 bits. Signedness . The U in USCALED means that we use an unsigned format. Scaling . The SCALED means that the byte, a number between 0 and 255, will be converted to a float without normalization: 255 becomes 255.0. Here are the other common scaling options provided by Vulkan when using bytes: Scaling GPU number type Range UNORM float [0.0, 1.0] SNORM float [-1.0, 1.0] USCALED float [0.0, 255.0] SSCALED float [-128.0, +127.0] (?) UINT int [0, 255] SINT int [-128, 127] (?) You'll find more information here: Full list of formats in the Vulkan spec , Hardware support of the formats : some formats are not supported by all existing GPUs. This website is quite useful to know what formats have the best hardware support. Writing the vertex shader \u00b6 We're now ready to write the vertex shader. Here is the full source code of the vertex shader: A few comments can be found below. Common shader resources \u00b6 The #include directive is provided for convenience by the GLSL to SPIR-V compiler glslc . It is used extensively in Datoviz. It provides a simple way of making all graphics shaders share some functions and resources. In particular, we make the convention that, in order to be compatible with the scene API, all graphics pipelines in Datoviz must have the following common slots : layout (std140, binding = 0) uniform MVP {...}; layout (std140, binding = 1) uniform Viewport {...}; Binding Descriptor type Description 0 uniform structure with the model-view-proj matrices 1 uniform structure with viewport information This is done just by including common.glsl in every shader. Note that this is not required when using completely standalone graphics pipelines that will never be used with the scene API. When one needs to add another bindings, the numbering should start at USER_BINDING , a special GLSL macro that is currently 2. Uniform alignment The std140 keyword refers to the alignment of the fields within the uniform structure. We're not going into the details, but we'll present some relatively arbitrary rules that we follow in Datoviz so that we don't need to think too much about memory alignment considerations when working with uniform structures: always use the std140 keyword when defining uniform resources in the shaders, NEVER use the following data types in a uniform structure : vec3 , ivec3 , mat3 , and any format with a 3 in it. If you need a vector with three components, use a vec4 and leave the last component alone. This only affects uniforms, NOT attributes. Varying variables \u00b6 The vertex shader may pass some variables to the next stage, typically the fragment shader, using varying variables . A crucial difference between the vertex and fragment shaders is that the vertex shader executes on every vertex , whereas the fragment shader executes on every pixel . When the vertex shader passes a value to the fragment shader, the GPU makes a linear interpolation for all pixels between two vertices in the same primitive. This is specific to line and triangle primitives, not points. For example, passing 0 to one vertex and 1 to the next vertex in a graphics with a line primitive would result in all pixels being passed a real value between 0 and 1 depending on the position of that pixel within that line segment. This system is used in particular with: Color gradients, Texture coordinates. Position transformation \u00b6 The common.glsl file defines (several overloaded versions of) the function transform() , which applies the model-view-proj matrices stored in the first bound resource, the MVP uniform. The associated C structure is DvzMVP : struct DvzMVP { mat4 model ; // model 4x4 matrix mat4 view ; // view 4x4 matrix mat4 proj ; // projection 4x4 matrix float time ; // elapsed time since the start of the application, in seconds }; The transformation is basically gl_Position = to_vulkan(proj * view * model * position) . The to_vulkan() internal function transforms the position from the OpenGL/Datoviz 3D coordinate system (y up, z toward the user) to the Vulkan coordinate system (y down, z toward the inside of the screen). Special GLSL variables \u00b6 We use a few special GLSL variables in the vertex shader: Name Type Description gl_Position vec4 final vertex position in normalized device coordinates gl_PointSize float size of the point, when using the point list primitive There are many more special variables available in each type of shader. You'll find the list of special GLSL variables here. Colormaps \u00b6 The colormap() function is implemented in colormaps.glsl . It provides a GLSL implementation of a few common colormaps, which allows to compute the color as a function of a scalar value without using a texture . There are a few advantages to this, mostly related to performance and avoiding edge effects when using nearest interpolation in the texture. Writing the fragment shader \u00b6 We now pass to the fragment shader. Here is the code: Here are a few comments. Clipping \u00b6 Datoviz panels may have margins around them. We make the distinction between: Viewport Description inner viewport area of the panel excluding the margins outer viewport area of the panel including the margins We follow the convention that the normalized coordinates [-1, +1] correspond to the inner viewport . Vertices beyond these limits will be rendered in the margins. Clipping allows to hide all pixels outside the inner viewport. This is how we implement the axes controller: graphics should not be rendered in the margins which contain the tick labels. Any graphics that should be compatible with the axes controller should therefore have the special CLIP macro at the beginning of the fragments shader (implemented in common.glsl ). This macro detects whether the current pixel is beyond the inner viewport, and if the graphics has been configured so, it discards that pixel. Special variables \u00b6 We haven't used any special variable in this fragment shader, but here are two useful variables that we could use: Name Type Description gl_FragCoord vec4 pixel position in window space gl_PointCoord vec2 coordinates of the pixel within the point primitive The gl_PointCoord variable is reserved to graphics pipelines using a point primitive. It provides the coordinates, in pixels, of the current pixel within the square. The origin is at the upper-left corner of the square. Compiling the shaders to SPIR-V \u00b6 Once the shaders have been written, they should be compiled to SPIR-V (unless using Datoviz to compile shaders provided as GLSL code, but this is a work in progress). Here is how to compile the shaders on the command-line: glslc custom_point.vert -o custom_point.vert.spv glslc custom_point.frag -o custom_point.frag.spv The glslc command is provided by the Vulkan SDK. Writing the custom graphics \u00b6 Here is the code to create a custom graphics compatible with the scene API, and to add it to a custom visual: // We create a blank graphics that will become our custom graphics. DvzGraphics * graphics = dvz_blank_graphics ( scene , 0 ); // Custom graphics creation. // The first step is to define the vertex and fragment shaders. When using // dvz_graphics_shader(), one must specify a path to the compiled SPIR-V shaders. // When writing the shaders in GLSL, it is thus necessary to compile them separately with // glslc. dvz_graphics_shader ( graphics , VK_SHADER_STAGE_VERTEX_BIT , \"custom_point.vert.spv\" ); dvz_graphics_shader ( graphics , VK_SHADER_STAGE_FRAGMENT_BIT , \"custom_point.frag.spv\" ); // We define the graphics pipeline topology: point list here. dvz_graphics_topology ( graphics , VK_PRIMITIVE_TOPOLOGY_POINT_LIST ); // Next, we declare the size of our vertex structure. dvz_graphics_vertex_binding ( graphics , 0 , sizeof ( PointVertex )); // We now declare the vertex shader attributes, that should correspond to the different // structure fields in the vertex structure. // The first attribute is a vec3 in GLSL, and a vec3 in C too. dvz_graphics_vertex_attr ( graphics , 0 , 0 , VK_FORMAT_R32G32B32_SFLOAT , offsetof ( PointVertex , pos )); // The second attribute is a float in GLSL, and a byte (uint8) in C. We use the special // format VK_FORMAT_R8_USCALED to declare this. dvz_graphics_vertex_attr ( graphics , 0 , 1 , VK_FORMAT_R8_USCALED , offsetof ( PointVertex , size )); // Now that we've set up the graphics, we create it. dvz_graphics_create ( graphics ); // We add our custom graphics to a custom visual. dvz_custom_graphics ( visual , graphics ); As shown here, the creation of a custom graphics involves the following steps: Defining the shaders, Defining the primitive type, Declaring the vertex structure, the format of each field, and the casting between the GPU and GPU. There are further functions in the vklite API to customize the creation of the graphics pipeline. Refer to the Datoviz C API reference for more details. Writing the custom visual \u00b6 Here is how to add a custom graphics to a custom visual: // We create a blank visual, to which we'll add our custom visual. DvzVisual * visual = dvz_blank_visual ( scene , DVZ_VISUAL_FLAGS_TRANSFORM_NONE ); // ... creating the custom graphics... // We add our custom graphics to a custom visual. dvz_custom_graphics ( visual , graphics ); At this point, the custom visual contains common sources (vertex, MVP, viewport) and props. One could then define more sources and props. Writing a simple test \u00b6 Finally, here is how to use our custom visual with a custom graphics: // ... creating the custom visual with the custom graphics... // We add the custom visual to the panel. dvz_custom_visual ( panel , visual ); // Now, we prepare the vertex data. We could have defined and used props, but we'll show // another method instead. We create the vertex buffer directly, using the PointVertex // structure we've created. const uint32_t N = 64 ; // number of points PointVertex * vertices = calloc ( N , sizeof ( PointVertex )); // vertex buffer float t = 0 ; for ( uint32_t i = 0 ; i < N ; i ++ ) { t = i / ( float )( N - 1 ); // vertex position vertices [ i ]. pos [ 0 ] = - .75 + 1.25 * t * t ; vertices [ i ]. pos [ 1 ] = + .75 - 1.25 * t ; // vertex size, in byte, between 0 and 255. vertices [ i ]. size = 4 * i + 1 ; } // Here is the crucial bit: we bind the GPU vertex buffer with our struct array. dvz_visual_data_source ( visual , DVZ_SOURCE_TYPE_VERTEX , 0 , 0 , N , N , vertices ); FREE ( vertices ); Here are a few comments: Instead of using visual props, we have defined the vertex structure directly. The whole point of the visual props is to avoid doing this, which may not be particularly easy with complex vertex shaders with many attributes. Also, setting the vertex buffer directly requires to understand how the graphics is implemented on the GPU. If some form of triangulation is required, it is up to the user to do it. In this particular example, there is no triangulation as we have a one-to-one correspondance between the squares and the vertices. Note that the data buffers passed to the dvz_visual_data*() functions make a copy of the data, so we're fine freeing it. Other topics \u00b6 We didn't yet cover these other aspects: Other resources (textures, other uniforms), Other shaders (geometry shader, tesselation shader), Indexed rendering, Indirect rendering, Fixed functions, Depth testing, Dynamic states, Specialization constants, Push constants, Custom command buffers, Interaction with compute shaders, And many other topics... Checklist for Datoviz contributors \u00b6 This is a handy checklist for developers when adding new graphics to the library (via a pull request). graphics.h : Add the typedef of the vertex, item, params structures Add new section and structs for vertex, item, params structures Make sure all fields in the params struct have a byte size that is not 3 or divisible by 3 Put comments after each struct field (used by automatic documentation generator) vklite.h : Make sure the DvzGraphicsType enum exists, or create a new one Shaders: graphics_xxx.vert|frag : Don't forget to import common.glsl in all shaders The first user binding should be params, should match exactly the struct The next bindings should be numbered with USER_BINDING+1 etc The body of the main fragment shader function should always begin with CLIP graphics.c : Add new section, with _graphics_xxx() and _graphics_xxx_callback() if there is a non-default graphics callback function Write the main graphics function Specify the shaders Specify the primitive type Specify the vertex attributes, should match the vertex shader Add the common slots Add slots for params/textures Specify the graphics callback function (optional) Write the graphics callback function This function is called when a new item is added to the graphics. What an \"item\" is is up to the create of a graphics. It's typically the smallest bit of data that has a meaning in the context of the graphics. The graphics callback is mostly used for pre-upload CPU-side \"triangulation\" of the data, so that the visuals that reuse this graphics don't have to know the details of the triangulation. Add the switch case in dvz_graphics_builtin() test_graphics.h : Add the new graphics test declaration test_graphics.c : Write the body of the test function Set to save the screenshot with the graphics name main.c : Add the new test to the list of test functions Test the graphics without interactivity and save the graphics screenshot Add graphics section in docs/reference/graphics.md","title":"How to write a custom graphics"},{"location":"howto/graphics/#how-to-write-a-custom-graphics","text":"Warning You need to go through the Vulkan crash course before following this guide. You also need to go through the how to write a custom visual guide first. In this section, we'll show how to create a custom graphics by writing custom GLSL shaders . This is an advanced topic as it requires understanding the basic of GPU graphics programming. Note Only the C API supports custom graphics at the moment. Python bindings for custom graphics will come in an upcoming version. Datoviz already includes the code necessary to compile GLSL shaders to SPIR-V on the fly (based on Khronos glslang ). The full source code for this example can be found in examples/custom_graphics.c . As a toy example, we'll create a graphics with square points of various size and color . Specifically, we'll require each point to have a different size (which is not supported in the existing point graphics), and a color depending directly on the size and computed directly on the GPU. We'll use a single byte per vertex to store the vertex size (between 0 and 255 pixels), and we'll use no memory for the color since it will be determined directly by this value. This kind of memory optimization is one of the use-cases of writing custom graphics. Limiting memory usage is important when dealing with extremely large datasets (tens or hundreds of millions of points). Writing a custom graphics involves the following steps: Choosing the primitive type. Defining the vertex data structure and corresponding vertex shader attributes. Writing the vertex shader. Writing the fragment shader. Compiling the GLSL shaders to SPIR-V (may be done automatically in the future). Writing the custom graphics. Writing the custom visual. Writing a simple test. Note Here, we show how to create a custom graphics compatible with the scene API, so that it can be used added to the scene the same way as builtin visuals. One could however decide not to use the scene API at all, and leverage instead the vklite API directly. This allows one to create entirely custom and standalone applications. This is not documented at the moment, and one should look at the vklite unit tests to understand how to use the vklite API directly.","title":"How to write a custom graphics"},{"location":"howto/graphics/#choosing-the-graphics-primitive-type","text":"Vulkan supports six major types of primitives: point list : square points with an arbitrary size, line list : disjoint aliased line segments, line strip : joined aliased line segments, triangle strip : joined triangles consecutively sharing an edge, triangle fan : joined triangles all sharing a single corner (the first vertex), triangle list : disjoint triangles, Other less common primitive types are described in the Vulkan specification . Warning Triangle fans are not supported on macOS. Schematic from the Vulkan Cookbook by Pawel Lapinski, O'Reilly Note The circular arrows in the triangles above indicate the orientation of the triangles, which is taken into account by the GPU. It is good practice to always ensure that all triangles constituting a given object are consistent. For example, when triangulating a square with two triangles, the order of the vertices should be chosen such that both triangles are directly oriented. The graphics pipeline can be configured to handle triangle orientation in a specific way. For example, one can make a graphics pipeline where indirectly oriented triangles are automatically discarded. Line primitives typically have a width of 1 pixel, although some hardware supports thicker lines. In Datoviz, thick, high-quality lines are implemented with triangles instead (line triangulation), and the antialiased thick line is drawn directly in the fragment shader. \"Basic\" line primitives are only used for testing and for special high-performance applications where scalability and performance are more important than visual quality. The most commonly-used primitive types in scientific visualization are essentially point lists and triangle lists (and to a lesser extent, triangle strips). In this example, we'll use a point list , where each point will correspond to one square.","title":"Choosing the graphics primitive type"},{"location":"howto/graphics/#defining-the-vertex-data-structure","text":"Once we know we'll use a point list as a primitive, the next step is to define the vertex data structure / vertex shader attributes. Here, we want the following attributes for each vertex: the point position (center of the square), as a vec3 , the point size (edge length of the square, in pixels), as an uint8_t . The point size will be limited to a maximum of 255 pixels in this toy example. We could have simply used a floating-point for the edge length, but that would have used four bytes instead of one. This is also to show the versatility of Vulkan and GPU programming, where we have full control on the data types used on both the GPU and CPU. Here is our vertex C structure: // We define the vertex structure for our graphics pipeline. typedef struct { vec3 pos ; // 3D point position uint8_t size ; // point size, in pixels, between 0 and 255. } PointVertex ; We also need to define the attributes in GLSL: layout ( location = 0 ) in vec3 pos ; // x, y, z positions layout ( location = 1 ) in float size ; // point size, between 0.0 and 255.0","title":"Defining the vertex data structure"},{"location":"howto/graphics/#attribute-format","text":"In this example, we use a uint8_t (byte) on the CPU, but a float on the GPU. Vulkan provides a way to specify the way we want to do the casting . This is done when creating the graphics pipeline below. We can just say here that we'll use the Vulkan format VK_FORMAT_R8_USCALED . This format means that: Number of components . There is a single component (scalar), the R in R8 means \"red\" component: this is the component used for scalar values. If using a pair of components, we would have both a R and G component (green). This naming convention using color names comes from the fact that these GPU formats are typically used to represent colors, but they can actually represent any type of numerical data. Number of bits per component . The 8 in R8 means that our red component is represented with 8 bits. Signedness . The U in USCALED means that we use an unsigned format. Scaling . The SCALED means that the byte, a number between 0 and 255, will be converted to a float without normalization: 255 becomes 255.0. Here are the other common scaling options provided by Vulkan when using bytes: Scaling GPU number type Range UNORM float [0.0, 1.0] SNORM float [-1.0, 1.0] USCALED float [0.0, 255.0] SSCALED float [-128.0, +127.0] (?) UINT int [0, 255] SINT int [-128, 127] (?) You'll find more information here: Full list of formats in the Vulkan spec , Hardware support of the formats : some formats are not supported by all existing GPUs. This website is quite useful to know what formats have the best hardware support.","title":"Attribute format"},{"location":"howto/graphics/#writing-the-vertex-shader","text":"We're now ready to write the vertex shader. Here is the full source code of the vertex shader: A few comments can be found below.","title":"Writing the vertex shader"},{"location":"howto/graphics/#common-shader-resources","text":"The #include directive is provided for convenience by the GLSL to SPIR-V compiler glslc . It is used extensively in Datoviz. It provides a simple way of making all graphics shaders share some functions and resources. In particular, we make the convention that, in order to be compatible with the scene API, all graphics pipelines in Datoviz must have the following common slots : layout (std140, binding = 0) uniform MVP {...}; layout (std140, binding = 1) uniform Viewport {...}; Binding Descriptor type Description 0 uniform structure with the model-view-proj matrices 1 uniform structure with viewport information This is done just by including common.glsl in every shader. Note that this is not required when using completely standalone graphics pipelines that will never be used with the scene API. When one needs to add another bindings, the numbering should start at USER_BINDING , a special GLSL macro that is currently 2.","title":"Common shader resources"},{"location":"howto/graphics/#varying-variables","text":"The vertex shader may pass some variables to the next stage, typically the fragment shader, using varying variables . A crucial difference between the vertex and fragment shaders is that the vertex shader executes on every vertex , whereas the fragment shader executes on every pixel . When the vertex shader passes a value to the fragment shader, the GPU makes a linear interpolation for all pixels between two vertices in the same primitive. This is specific to line and triangle primitives, not points. For example, passing 0 to one vertex and 1 to the next vertex in a graphics with a line primitive would result in all pixels being passed a real value between 0 and 1 depending on the position of that pixel within that line segment. This system is used in particular with: Color gradients, Texture coordinates.","title":"Varying variables"},{"location":"howto/graphics/#position-transformation","text":"The common.glsl file defines (several overloaded versions of) the function transform() , which applies the model-view-proj matrices stored in the first bound resource, the MVP uniform. The associated C structure is DvzMVP : struct DvzMVP { mat4 model ; // model 4x4 matrix mat4 view ; // view 4x4 matrix mat4 proj ; // projection 4x4 matrix float time ; // elapsed time since the start of the application, in seconds }; The transformation is basically gl_Position = to_vulkan(proj * view * model * position) . The to_vulkan() internal function transforms the position from the OpenGL/Datoviz 3D coordinate system (y up, z toward the user) to the Vulkan coordinate system (y down, z toward the inside of the screen).","title":"Position transformation"},{"location":"howto/graphics/#special-glsl-variables","text":"We use a few special GLSL variables in the vertex shader: Name Type Description gl_Position vec4 final vertex position in normalized device coordinates gl_PointSize float size of the point, when using the point list primitive There are many more special variables available in each type of shader. You'll find the list of special GLSL variables here.","title":"Special GLSL variables"},{"location":"howto/graphics/#colormaps","text":"The colormap() function is implemented in colormaps.glsl . It provides a GLSL implementation of a few common colormaps, which allows to compute the color as a function of a scalar value without using a texture . There are a few advantages to this, mostly related to performance and avoiding edge effects when using nearest interpolation in the texture.","title":"Colormaps"},{"location":"howto/graphics/#writing-the-fragment-shader","text":"We now pass to the fragment shader. Here is the code: Here are a few comments.","title":"Writing the fragment shader"},{"location":"howto/graphics/#clipping","text":"Datoviz panels may have margins around them. We make the distinction between: Viewport Description inner viewport area of the panel excluding the margins outer viewport area of the panel including the margins We follow the convention that the normalized coordinates [-1, +1] correspond to the inner viewport . Vertices beyond these limits will be rendered in the margins. Clipping allows to hide all pixels outside the inner viewport. This is how we implement the axes controller: graphics should not be rendered in the margins which contain the tick labels. Any graphics that should be compatible with the axes controller should therefore have the special CLIP macro at the beginning of the fragments shader (implemented in common.glsl ). This macro detects whether the current pixel is beyond the inner viewport, and if the graphics has been configured so, it discards that pixel.","title":"Clipping"},{"location":"howto/graphics/#special-variables","text":"We haven't used any special variable in this fragment shader, but here are two useful variables that we could use: Name Type Description gl_FragCoord vec4 pixel position in window space gl_PointCoord vec2 coordinates of the pixel within the point primitive The gl_PointCoord variable is reserved to graphics pipelines using a point primitive. It provides the coordinates, in pixels, of the current pixel within the square. The origin is at the upper-left corner of the square.","title":"Special variables"},{"location":"howto/graphics/#compiling-the-shaders-to-spir-v","text":"Once the shaders have been written, they should be compiled to SPIR-V (unless using Datoviz to compile shaders provided as GLSL code, but this is a work in progress). Here is how to compile the shaders on the command-line: glslc custom_point.vert -o custom_point.vert.spv glslc custom_point.frag -o custom_point.frag.spv The glslc command is provided by the Vulkan SDK.","title":"Compiling the shaders to SPIR-V"},{"location":"howto/graphics/#writing-the-custom-graphics","text":"Here is the code to create a custom graphics compatible with the scene API, and to add it to a custom visual: // We create a blank graphics that will become our custom graphics. DvzGraphics * graphics = dvz_blank_graphics ( scene , 0 ); // Custom graphics creation. // The first step is to define the vertex and fragment shaders. When using // dvz_graphics_shader(), one must specify a path to the compiled SPIR-V shaders. // When writing the shaders in GLSL, it is thus necessary to compile them separately with // glslc. dvz_graphics_shader ( graphics , VK_SHADER_STAGE_VERTEX_BIT , \"custom_point.vert.spv\" ); dvz_graphics_shader ( graphics , VK_SHADER_STAGE_FRAGMENT_BIT , \"custom_point.frag.spv\" ); // We define the graphics pipeline topology: point list here. dvz_graphics_topology ( graphics , VK_PRIMITIVE_TOPOLOGY_POINT_LIST ); // Next, we declare the size of our vertex structure. dvz_graphics_vertex_binding ( graphics , 0 , sizeof ( PointVertex )); // We now declare the vertex shader attributes, that should correspond to the different // structure fields in the vertex structure. // The first attribute is a vec3 in GLSL, and a vec3 in C too. dvz_graphics_vertex_attr ( graphics , 0 , 0 , VK_FORMAT_R32G32B32_SFLOAT , offsetof ( PointVertex , pos )); // The second attribute is a float in GLSL, and a byte (uint8) in C. We use the special // format VK_FORMAT_R8_USCALED to declare this. dvz_graphics_vertex_attr ( graphics , 0 , 1 , VK_FORMAT_R8_USCALED , offsetof ( PointVertex , size )); // Now that we've set up the graphics, we create it. dvz_graphics_create ( graphics ); // We add our custom graphics to a custom visual. dvz_custom_graphics ( visual , graphics ); As shown here, the creation of a custom graphics involves the following steps: Defining the shaders, Defining the primitive type, Declaring the vertex structure, the format of each field, and the casting between the GPU and GPU. There are further functions in the vklite API to customize the creation of the graphics pipeline. Refer to the Datoviz C API reference for more details.","title":"Writing the custom graphics"},{"location":"howto/graphics/#writing-the-custom-visual","text":"Here is how to add a custom graphics to a custom visual: // We create a blank visual, to which we'll add our custom visual. DvzVisual * visual = dvz_blank_visual ( scene , DVZ_VISUAL_FLAGS_TRANSFORM_NONE ); // ... creating the custom graphics... // We add our custom graphics to a custom visual. dvz_custom_graphics ( visual , graphics ); At this point, the custom visual contains common sources (vertex, MVP, viewport) and props. One could then define more sources and props.","title":"Writing the custom visual"},{"location":"howto/graphics/#writing-a-simple-test","text":"Finally, here is how to use our custom visual with a custom graphics: // ... creating the custom visual with the custom graphics... // We add the custom visual to the panel. dvz_custom_visual ( panel , visual ); // Now, we prepare the vertex data. We could have defined and used props, but we'll show // another method instead. We create the vertex buffer directly, using the PointVertex // structure we've created. const uint32_t N = 64 ; // number of points PointVertex * vertices = calloc ( N , sizeof ( PointVertex )); // vertex buffer float t = 0 ; for ( uint32_t i = 0 ; i < N ; i ++ ) { t = i / ( float )( N - 1 ); // vertex position vertices [ i ]. pos [ 0 ] = - .75 + 1.25 * t * t ; vertices [ i ]. pos [ 1 ] = + .75 - 1.25 * t ; // vertex size, in byte, between 0 and 255. vertices [ i ]. size = 4 * i + 1 ; } // Here is the crucial bit: we bind the GPU vertex buffer with our struct array. dvz_visual_data_source ( visual , DVZ_SOURCE_TYPE_VERTEX , 0 , 0 , N , N , vertices ); FREE ( vertices ); Here are a few comments: Instead of using visual props, we have defined the vertex structure directly. The whole point of the visual props is to avoid doing this, which may not be particularly easy with complex vertex shaders with many attributes. Also, setting the vertex buffer directly requires to understand how the graphics is implemented on the GPU. If some form of triangulation is required, it is up to the user to do it. In this particular example, there is no triangulation as we have a one-to-one correspondance between the squares and the vertices. Note that the data buffers passed to the dvz_visual_data*() functions make a copy of the data, so we're fine freeing it.","title":"Writing a simple test"},{"location":"howto/graphics/#other-topics","text":"We didn't yet cover these other aspects: Other resources (textures, other uniforms), Other shaders (geometry shader, tesselation shader), Indexed rendering, Indirect rendering, Fixed functions, Depth testing, Dynamic states, Specialization constants, Push constants, Custom command buffers, Interaction with compute shaders, And many other topics...","title":"Other topics"},{"location":"howto/graphics/#checklist-for-datoviz-contributors","text":"This is a handy checklist for developers when adding new graphics to the library (via a pull request). graphics.h : Add the typedef of the vertex, item, params structures Add new section and structs for vertex, item, params structures Make sure all fields in the params struct have a byte size that is not 3 or divisible by 3 Put comments after each struct field (used by automatic documentation generator) vklite.h : Make sure the DvzGraphicsType enum exists, or create a new one Shaders: graphics_xxx.vert|frag : Don't forget to import common.glsl in all shaders The first user binding should be params, should match exactly the struct The next bindings should be numbered with USER_BINDING+1 etc The body of the main fragment shader function should always begin with CLIP graphics.c : Add new section, with _graphics_xxx() and _graphics_xxx_callback() if there is a non-default graphics callback function Write the main graphics function Specify the shaders Specify the primitive type Specify the vertex attributes, should match the vertex shader Add the common slots Add slots for params/textures Specify the graphics callback function (optional) Write the graphics callback function This function is called when a new item is added to the graphics. What an \"item\" is is up to the create of a graphics. It's typically the smallest bit of data that has a meaning in the context of the graphics. The graphics callback is mostly used for pre-upload CPU-side \"triangulation\" of the data, so that the visuals that reuse this graphics don't have to know the details of the triangulation. Add the switch case in dvz_graphics_builtin() test_graphics.h : Add the new graphics test declaration test_graphics.c : Write the body of the test function Set to save the screenshot with the graphics name main.c : Add the new test to the list of test functions Test the graphics without interactivity and save the graphics screenshot Add graphics section in docs/reference/graphics.md","title":"Checklist for Datoviz contributors"},{"location":"howto/standalone_canvas/","text":"How to use the canvas C API \u00b6 This example shows how to write a standalone C app using only the canvas API , not the scene API. We'll render a triangle without using existing graphics, visuals, panels, and so on. We'll follow these steps: Creating a graphics with custom shaders, Creating a function callback for command buffer refill, Creating a vertex buffer manually.","title":"How to use the canvas C API"},{"location":"howto/standalone_canvas/#how-to-use-the-canvas-c-api","text":"This example shows how to write a standalone C app using only the canvas API , not the scene API. We'll render a triangle without using existing graphics, visuals, panels, and so on. We'll follow these steps: Creating a graphics with custom shaders, Creating a function callback for command buffer refill, Creating a vertex buffer manually.","title":"How to use the canvas C API"},{"location":"howto/standalone_scene/","text":"How to write a standalone C app \u00b6 This example shows how to make a scatter plot using the scene C API in a standalone application. Note A basic build script is provided in the examples/ folder. A better practice would probably be to use cmake but this is not documented yet. Help appreciated! Full source code \u00b6","title":"How to write a standalone C app"},{"location":"howto/standalone_scene/#how-to-write-a-standalone-c-app","text":"This example shows how to make a scatter plot using the scene C API in a standalone application. Note A basic build script is provided in the examples/ folder. A better practice would probably be to use cmake but this is not documented yet. Help appreciated!","title":"How to write a standalone C app"},{"location":"howto/standalone_scene/#full-source-code","text":"","title":"Full source code"},{"location":"howto/standalone_vklite/","text":"How to use the vklite C API \u00b6 This example shows how to write a standalone C app using only the vklite API (thin wrapper on top of the Vulkan C API), not the canvas or scene API. Creating a Vulkan-aware window is a complex operation as it requires creating a swapchain and implementing a rendering loop using proper CPU-GPU and GPU-GPU synchronization. The canvas abstracts that complexity away and there are probably few reasons not to use it. Therefore, the main reason to use the vklite API directly is probably when doing offscreen rendering and/or compute , and when reusing existing visuals and graphics in Datoviz is not desirable. On this page, we'll show how to make an offscreen render of a triangle using only the vklite API. We'll cover the following steps: Creating a GPU with custom queues. Creating a render pass. Creating GPU images for rendering. Creating framebuffers. Creating a graphics pipeline. Creating pipeline bindings. Creating a vertex buffer. Creating and recording a command buffer. Submitting a command buffer to the GPU and waiting until it has completed. Making a screenshot by creating a staging GPU image, and submitting a command buffer with transition barriers and a GPU image copy. This \"hello world\" script is about 250 lines long (without comments), about 4x smaller than by using the raw Vulkan C API.","title":"How to use the vklite C API"},{"location":"howto/standalone_vklite/#how-to-use-the-vklite-c-api","text":"This example shows how to write a standalone C app using only the vklite API (thin wrapper on top of the Vulkan C API), not the canvas or scene API. Creating a Vulkan-aware window is a complex operation as it requires creating a swapchain and implementing a rendering loop using proper CPU-GPU and GPU-GPU synchronization. The canvas abstracts that complexity away and there are probably few reasons not to use it. Therefore, the main reason to use the vklite API directly is probably when doing offscreen rendering and/or compute , and when reusing existing visuals and graphics in Datoviz is not desirable. On this page, we'll show how to make an offscreen render of a triangle using only the vklite API. We'll cover the following steps: Creating a GPU with custom queues. Creating a render pass. Creating GPU images for rendering. Creating framebuffers. Creating a graphics pipeline. Creating pipeline bindings. Creating a vertex buffer. Creating and recording a command buffer. Submitting a command buffer to the GPU and waiting until it has completed. Making a screenshot by creating a staging GPU image, and submitting a command buffer with transition barriers and a GPU image copy. This \"hello world\" script is about 250 lines long (without comments), about 4x smaller than by using the raw Vulkan C API.","title":"How to use the vklite C API"},{"location":"howto/visual/","text":"How to write a custom visual \u00b6 Warning You need to go through the Vulkan crash course before following this guide. In this section, we'll show how to create a custom visual based on an existing graphics pipeline, so without writing custom GLSL shaders . Note Only the C API supports custom visuals at the moment. Python bindings for custom visuals will come in an upcoming version. The full source code for this example can be found in examples/custom_visual.c . We'll make a square visual that makes it easy to add uniformly-colored squares to the scene. What is a visual? \u00b6 The visual is the most important abstraction in Datoviz. It abstracts away the internal details related to GPU rendering and proposes a user-friendly interface to set up visual elements. Generally speaking, adding a new visual to a panel involves the following steps: choosing one of the existing visuals on the visual reference page, preparing the data to match the format expected by the visual properties, setting the visual properties with the data. Visual properties, or \"props\" \u00b6 Each predefined visual comes with a set of predefined visual properties, also called props . For example, the marker visual has props for: point position, color, marker size, marker type, angle, and so on. The visual reference page presents the list of all predefined visuals along with their sets of props. You'll probably refer a lot to this page since it contains the most important information you'll need for your visualizations. Each prop is defined by: a name, a data type (for example float32 , uint8 ), a description of how the prop is used for rendering. Note The Python API takes care of converting each prop to the correct data type using NumPy ndarray.astype() . Most props accepting floating-point numbers require single-precision format since this is the optimal format for GPUs. The notable exception is the POS prop (position), which requires double-precision data. Datoviz provides an internal CPU-based data transformation system that requires double precision (single-precision would not be acceptable for scientific data handling). Also, visuals that implement triangulation require double precision. Datoviz converts the transformed position to single-precision at the last moment before uploading it to the GPU. Batch rendering \u00b6 Another crucial notion related to visuals is batch rendering . For performance reasons, it is recommended to use as few visuals as possible in a given scene . For example: to display a scatter plot with 100 points, use a single marker visual with 100 points, instead of 100 visuals with one point , to display 100 polygons, use a single polygon visual with 100 items (each containing an arbitrary number of points), instead of 100 visuals , similarly with paths, images, meshes, text, and so on. This allows the GPU to render all of these different objects of the same type in a single draw command (with the same GPU transformation matrices). To define multiple objects with various sizes in a given visual (for example, displaying multiple paths with the same visual), one typically concatenates all points and properties in big arrays (total size is the sum of all object sizes), and use the special prop length to define the length of each object (vector with as many elements as there are different objects). Distinction between graphics and visuals \u00b6 Datoviz makes the distinction between a graphics (graphics pipeline) and a visual : a graphics is a GPU-level object. It is defined by a vertex shader, a fragment shader, a primitive type (point, line, triangle), and other details. a visual is a user-level object. It encapsulates a particular type of visual element and abstracts away the GPU implementation details. A visual is defined by one or several graphics pipelines, optional compute pipelines, and a set of visual properties that allow the user to specify the visual's data. Importantly, the user doesn't need to know the internal implementation details of a visual to use it. It is normally sufficient to know the props specification. Making a custom visual based on existing graphics \u00b6 In this section, we'll show how to create a custom visual by reusing an existing graphics (without writing custom shaders). The main use-case for this scenario is making a visual with a custom CPU data transformation pipeline . An example is polygon triangulation: since the GPU can only render triangles, one needs to triangulate an arbitrary shape before rendering it. The triangulation is implemented at the level of the visual, so that the user can pass the polygon points without having to triangulate it manually. The visual makes the triangulation internally, and generates the triangles required by the underlying graphics pipeline. In the simple example below, we'll implement a simple square visual with a trivial triangulation (two triangles per square). The custom square visual will provide the following props: Type Index Type Description pos 0 dvec3 square center position color 0 cvec4 square color length 0 float edge length The underlying graphics will be the triangle graphics, where three successive vertices define a single independent triangle. Each square will be triangulated into two triangles, or six vertices. This square visual will be easier to use than the triangle one, since the user won't have to deal with triangulation manually. Here is the code to define the three visual props: // pos prop, dvec3 data type dvz_visual_prop ( visual , DVZ_PROP_POS , 0 , DVZ_DTYPE_DVEC3 , DVZ_SOURCE_TYPE_VERTEX , 0 ); // color prop, cvec4 data type dvz_visual_prop ( visual , DVZ_PROP_COLOR , 0 , DVZ_DTYPE_CVEC4 , DVZ_SOURCE_TYPE_VERTEX , 0 ); // length prop, float data type dvz_visual_prop ( visual , DVZ_PROP_LENGTH , 0 , DVZ_DTYPE_FLOAT , DVZ_SOURCE_TYPE_VERTEX , 0 ); Vertex shader attributes, vertex buffer, vertex structure \u00b6 To each graphics is associated a particular vertex shader. A vertex shader defines a list of attributes , which correspond to the different vertex inputs. For example, the triangle graphics we will use in this example has two attributes in its vertex shader: // This GLSL code is at the beginning of the triangle graphics vertex shader. // Two attributes: layout ( location = 0 ) in vec3 pos ; // vertex position layout ( location = 1 ) in vec4 color ; // vertex color In Datoviz, the input data feeding these vertex shader attributes is stored in a GPU buffer called the vertex buffer . The GPU buffer contains a contiguous array of structure elements that match exactly these attributes. Here, the vertex structure of the triangle graphics, and of the other basic graphics, is the standard DvzVertex structure: struct DvzVertex { vec3 pos ; // three single-precision floating-point numbers for x, y, z position cvec4 color ; // four uint8 bytes for the r, g, b, a color components } The vertex shader executes in parallel over all structure elements stored in the vertex buffer. We'll see in the custom graphics page more details about how we link this C structure to the GLSL attributes. The main role of the visual is to copy the user-specified props data into the vertex buffer . This is sometimes straightforward, like in the marker visual, where each vertex corresponds to one marker, but it is often less trivial. In the example covered in this page, where we need to transform squares into triangles, our custom visual will need to create six vertices in the vertex buffer for every square passed by the user . This is implemented in the visual baking function . Visual baking function \u00b6 Once the props are defined, the most important step when creating a custom visual is to implement the baking function . The baking function takes the props data as input, and fills in the vertex buffer , as well as, possibly, other GPU data sources (uniforms, storage buffers, textures). Here, the square baking function must recover the pos and length props in order to compute the four corner position of each square, and make the triangulation with two triangles per square. The props and the vertex buffer (and other sources) come with DvzArray instances, which are thin wrappers around 1D arrays of homogeneous data types. They support few features: no multidimensional arrays, no vectorized operations. // The following code snippet contains the body of the baking function. static void _bake_callback ( DvzVisual * visual , DvzVisualDataEvent ev ) { ASSERT ( visual != NULL ); // First, we obtain the array instances holding the prop data as specified by the user. DvzArray * arr_pos = dvz_prop_array ( visual , DVZ_PROP_POS , 0 ); DvzArray * arr_color = dvz_prop_array ( visual , DVZ_PROP_COLOR , 0 ); DvzArray * arr_length = dvz_prop_array ( visual , DVZ_PROP_LENGTH , 0 ); // We also get the array of the vertex buffer, which we'll need to fill with the triangulation. DvzArray * arr_vertex = dvz_source_array ( visual , DVZ_SOURCE_TYPE_VERTEX , 0 ); // The number of rows in the 1D position array (set by the user) is the number of squares // requested by the user. uint32_t square_count = arr_pos -> item_count ; // We resize the vertex buffer array so that it holds six vertices per square (two triangles). dvz_array_resize ( arr_vertex , 6 * square_count ); // Pointers to the input data. dvec3 * pos = NULL ; cvec4 * color = NULL ; float * length = NULL ; // Pointer to the output vertex. DvzVertex * vertex = ( DvzVertex * ) arr_vertex -> data ; // Here, we triangulate each square by computing the position of each square corner. float hl = 0 ; for ( uint32_t i = 0 ; i < square_count ; i ++ ) { // We get a pointer to the current item in each prop array. pos = dvz_array_item ( arr_pos , i ); color = dvz_array_item ( arr_color , i ); length = dvz_array_item ( arr_length , i ); // This is the half of the square size. hl = ( * length ) / 2 ; // First triangle: // Bottom-left corner. vertex [ 6 * i + 0 ]. pos [ 0 ] = pos [ 0 ][ 0 ] - hl ; vertex [ 6 * i + 0 ]. pos [ 1 ] = pos [ 0 ][ 1 ] - hl ; // Bottom-right corner. vertex [ 6 * i + 1 ]. pos [ 0 ] = pos [ 0 ][ 0 ] + hl ; vertex [ 6 * i + 1 ]. pos [ 1 ] = pos [ 0 ][ 1 ] - hl ; // Top-right corner. vertex [ 6 * i + 2 ]. pos [ 0 ] = pos [ 0 ][ 0 ] + hl ; vertex [ 6 * i + 2 ]. pos [ 1 ] = pos [ 0 ][ 1 ] + hl ; // Second triangle: // Top-right corner again. vertex [ 6 * i + 3 ]. pos [ 0 ] = pos [ 0 ][ 0 ] + hl ; vertex [ 6 * i + 3 ]. pos [ 1 ] = pos [ 0 ][ 1 ] + hl ; // Top-left corner. vertex [ 6 * i + 4 ]. pos [ 0 ] = pos [ 0 ][ 0 ] - hl ; vertex [ 6 * i + 4 ]. pos [ 1 ] = pos [ 0 ][ 1 ] + hl ; // Bottom-left corner (again). vertex [ 6 * i + 5 ]. pos [ 0 ] = pos [ 0 ][ 0 ] - hl ; vertex [ 6 * i + 5 ]. pos [ 1 ] = pos [ 0 ][ 1 ] - hl ; // We copy the square color to each of the six vertices making the current square. // This is a choice made in this example, and it is up to the custom visual creator // to define how the user data, passed via props, will be used to fill in the vertices. for ( uint32_t j = 0 ; j < 6 ; j ++ ) memcpy ( vertex [ 6 * i + j ]. color , color , sizeof ( cvec4 )); } } Putting everything together \u00b6 Here is the code to create the custom visual. // We create a blank visual in the scene. // For demo purposes, we disable the automatic position normalization. DvzVisual * visual = dvz_blank_visual ( scene , DVZ_VISUAL_FLAGS_TRANSFORM_NONE ); // We add the existing graphics triangle graphics pipeline. dvz_visual_graphics ( visual , dvz_graphics_builtin ( canvas , DVZ_GRAPHICS_TRIANGLE , 0 )); // We add the vertex buffer source, and we must specify the same vertex struct type // as the one used by the graphics pipeline (standard vertex structure, with pos and color). dvz_visual_source ( visual , DVZ_SOURCE_TYPE_VERTEX , 0 , DVZ_PIPELINE_GRAPHICS , 0 , 0 , sizeof ( DvzVertex ), 0 ); // We specify the visual props. dvz_visual_prop ( visual , DVZ_PROP_POS , 0 , DVZ_DTYPE_DVEC3 , DVZ_SOURCE_TYPE_VERTEX , 0 ); dvz_visual_prop ( visual , DVZ_PROP_COLOR , 0 , DVZ_DTYPE_CVEC4 , DVZ_SOURCE_TYPE_VERTEX , 0 ); dvz_visual_prop ( visual , DVZ_PROP_LENGTH , 0 , DVZ_DTYPE_FLOAT , DVZ_SOURCE_TYPE_VERTEX , 0 ); // We declare our custom baking function. dvz_visual_callback_bake ( visual , _bake_callback ); // Finally, once the custom visual has been created, we can add it to the panel. dvz_custom_visual ( panel , visual ); Once the custom visual has been created and added to the scene, the last step consists of setting some data, as usual: // We define three squares. dvz_visual_data ( visual , DVZ_PROP_POS , 0 , 3 , ( dvec3 []){{ - .5 , 0 , 0 }, { 0 , 0 , 0 }, { + .5 , 0 , 0 }}); // We use a different color for each square. dvz_visual_data ( visual , DVZ_PROP_COLOR , 0 , 3 , ( cvec4 []){{ 255 , 0 , 0 , 255 }, { 0 , 255 , 0 , 255 }, { 0 , 0 , 255 , 255 }}); // NOTE: we use the same length for all squares. dvz_visual_data ( visual , DVZ_PROP_LENGTH , 0 , 1 , ( float []){ .25 }); Note that we used a single value for the last prop (edge length). Datoviz uses the convention that a prop may have less values than objects, in which case the last value is repeated over . In particular, defining a prop with a single element means using the same value for all items in the visual. This is a sort of \"broadcasting\" rule (following NumPy's terminology). Other topics \u00b6 We didn't yet cover these other aspects: Position data transformation, Visual parameters stored in the uniform buffer, Visual flags. Checklist for Datoviz contributors \u00b6 This is a handy checklist for developers when adding new visuals to the library (via a pull request). Determine the graphics you'll be using, and the list of source and props builtin_visuals.h : Make sure the DvzVisualType enum exists, or create a new one visuals.h : Check that all prop types exist, otherwise add them builtin_visuals.c : Make a new section for the visual code Write the main visual function Specify the graphics pipeline(s) Specify the sources: Vertex buffer Index buffer (optional) Param buffer (optional) Textures (optional) Specify the props The cast and copy spec are not mandatory if there is a custom baking callback function Note that the special POS prop is DVEC3 and is typically cast to VEC3, but only after baking and panel transform Add a length prop if there are multiple objects Add the common props Add the param props Specify the baking callback function (optional) Write the baking callback function Add a new switch case in dvz_visual_builtin() test_builtin_visuals.h : Add a new visual test declaration test_builtin_visuals.c : Write the body of the test function main.c : Add the new test to the list of test functions Test the new visual without interactivity and save a visual screenshot Add a section in the visual reference docs/reference/visuals.md , document the props, sources, etc.","title":"How to write a custom visual"},{"location":"howto/visual/#how-to-write-a-custom-visual","text":"Warning You need to go through the Vulkan crash course before following this guide. In this section, we'll show how to create a custom visual based on an existing graphics pipeline, so without writing custom GLSL shaders . Note Only the C API supports custom visuals at the moment. Python bindings for custom visuals will come in an upcoming version. The full source code for this example can be found in examples/custom_visual.c . We'll make a square visual that makes it easy to add uniformly-colored squares to the scene.","title":"How to write a custom visual"},{"location":"howto/visual/#what-is-a-visual","text":"The visual is the most important abstraction in Datoviz. It abstracts away the internal details related to GPU rendering and proposes a user-friendly interface to set up visual elements. Generally speaking, adding a new visual to a panel involves the following steps: choosing one of the existing visuals on the visual reference page, preparing the data to match the format expected by the visual properties, setting the visual properties with the data.","title":"What is a visual?"},{"location":"howto/visual/#visual-properties-or-props","text":"Each predefined visual comes with a set of predefined visual properties, also called props . For example, the marker visual has props for: point position, color, marker size, marker type, angle, and so on. The visual reference page presents the list of all predefined visuals along with their sets of props. You'll probably refer a lot to this page since it contains the most important information you'll need for your visualizations. Each prop is defined by: a name, a data type (for example float32 , uint8 ), a description of how the prop is used for rendering. Note The Python API takes care of converting each prop to the correct data type using NumPy ndarray.astype() . Most props accepting floating-point numbers require single-precision format since this is the optimal format for GPUs. The notable exception is the POS prop (position), which requires double-precision data. Datoviz provides an internal CPU-based data transformation system that requires double precision (single-precision would not be acceptable for scientific data handling). Also, visuals that implement triangulation require double precision. Datoviz converts the transformed position to single-precision at the last moment before uploading it to the GPU.","title":"Visual properties, or \"props\""},{"location":"howto/visual/#batch-rendering","text":"Another crucial notion related to visuals is batch rendering . For performance reasons, it is recommended to use as few visuals as possible in a given scene . For example: to display a scatter plot with 100 points, use a single marker visual with 100 points, instead of 100 visuals with one point , to display 100 polygons, use a single polygon visual with 100 items (each containing an arbitrary number of points), instead of 100 visuals , similarly with paths, images, meshes, text, and so on. This allows the GPU to render all of these different objects of the same type in a single draw command (with the same GPU transformation matrices). To define multiple objects with various sizes in a given visual (for example, displaying multiple paths with the same visual), one typically concatenates all points and properties in big arrays (total size is the sum of all object sizes), and use the special prop length to define the length of each object (vector with as many elements as there are different objects).","title":"Batch rendering"},{"location":"howto/visual/#distinction-between-graphics-and-visuals","text":"Datoviz makes the distinction between a graphics (graphics pipeline) and a visual : a graphics is a GPU-level object. It is defined by a vertex shader, a fragment shader, a primitive type (point, line, triangle), and other details. a visual is a user-level object. It encapsulates a particular type of visual element and abstracts away the GPU implementation details. A visual is defined by one or several graphics pipelines, optional compute pipelines, and a set of visual properties that allow the user to specify the visual's data. Importantly, the user doesn't need to know the internal implementation details of a visual to use it. It is normally sufficient to know the props specification.","title":"Distinction between graphics and visuals"},{"location":"howto/visual/#making-a-custom-visual-based-on-existing-graphics","text":"In this section, we'll show how to create a custom visual by reusing an existing graphics (without writing custom shaders). The main use-case for this scenario is making a visual with a custom CPU data transformation pipeline . An example is polygon triangulation: since the GPU can only render triangles, one needs to triangulate an arbitrary shape before rendering it. The triangulation is implemented at the level of the visual, so that the user can pass the polygon points without having to triangulate it manually. The visual makes the triangulation internally, and generates the triangles required by the underlying graphics pipeline. In the simple example below, we'll implement a simple square visual with a trivial triangulation (two triangles per square). The custom square visual will provide the following props: Type Index Type Description pos 0 dvec3 square center position color 0 cvec4 square color length 0 float edge length The underlying graphics will be the triangle graphics, where three successive vertices define a single independent triangle. Each square will be triangulated into two triangles, or six vertices. This square visual will be easier to use than the triangle one, since the user won't have to deal with triangulation manually. Here is the code to define the three visual props: // pos prop, dvec3 data type dvz_visual_prop ( visual , DVZ_PROP_POS , 0 , DVZ_DTYPE_DVEC3 , DVZ_SOURCE_TYPE_VERTEX , 0 ); // color prop, cvec4 data type dvz_visual_prop ( visual , DVZ_PROP_COLOR , 0 , DVZ_DTYPE_CVEC4 , DVZ_SOURCE_TYPE_VERTEX , 0 ); // length prop, float data type dvz_visual_prop ( visual , DVZ_PROP_LENGTH , 0 , DVZ_DTYPE_FLOAT , DVZ_SOURCE_TYPE_VERTEX , 0 );","title":"Making a custom visual based on existing graphics"},{"location":"howto/visual/#vertex-shader-attributes-vertex-buffer-vertex-structure","text":"To each graphics is associated a particular vertex shader. A vertex shader defines a list of attributes , which correspond to the different vertex inputs. For example, the triangle graphics we will use in this example has two attributes in its vertex shader: // This GLSL code is at the beginning of the triangle graphics vertex shader. // Two attributes: layout ( location = 0 ) in vec3 pos ; // vertex position layout ( location = 1 ) in vec4 color ; // vertex color In Datoviz, the input data feeding these vertex shader attributes is stored in a GPU buffer called the vertex buffer . The GPU buffer contains a contiguous array of structure elements that match exactly these attributes. Here, the vertex structure of the triangle graphics, and of the other basic graphics, is the standard DvzVertex structure: struct DvzVertex { vec3 pos ; // three single-precision floating-point numbers for x, y, z position cvec4 color ; // four uint8 bytes for the r, g, b, a color components } The vertex shader executes in parallel over all structure elements stored in the vertex buffer. We'll see in the custom graphics page more details about how we link this C structure to the GLSL attributes. The main role of the visual is to copy the user-specified props data into the vertex buffer . This is sometimes straightforward, like in the marker visual, where each vertex corresponds to one marker, but it is often less trivial. In the example covered in this page, where we need to transform squares into triangles, our custom visual will need to create six vertices in the vertex buffer for every square passed by the user . This is implemented in the visual baking function .","title":"Vertex shader attributes, vertex buffer, vertex structure"},{"location":"howto/visual/#visual-baking-function","text":"Once the props are defined, the most important step when creating a custom visual is to implement the baking function . The baking function takes the props data as input, and fills in the vertex buffer , as well as, possibly, other GPU data sources (uniforms, storage buffers, textures). Here, the square baking function must recover the pos and length props in order to compute the four corner position of each square, and make the triangulation with two triangles per square. The props and the vertex buffer (and other sources) come with DvzArray instances, which are thin wrappers around 1D arrays of homogeneous data types. They support few features: no multidimensional arrays, no vectorized operations. // The following code snippet contains the body of the baking function. static void _bake_callback ( DvzVisual * visual , DvzVisualDataEvent ev ) { ASSERT ( visual != NULL ); // First, we obtain the array instances holding the prop data as specified by the user. DvzArray * arr_pos = dvz_prop_array ( visual , DVZ_PROP_POS , 0 ); DvzArray * arr_color = dvz_prop_array ( visual , DVZ_PROP_COLOR , 0 ); DvzArray * arr_length = dvz_prop_array ( visual , DVZ_PROP_LENGTH , 0 ); // We also get the array of the vertex buffer, which we'll need to fill with the triangulation. DvzArray * arr_vertex = dvz_source_array ( visual , DVZ_SOURCE_TYPE_VERTEX , 0 ); // The number of rows in the 1D position array (set by the user) is the number of squares // requested by the user. uint32_t square_count = arr_pos -> item_count ; // We resize the vertex buffer array so that it holds six vertices per square (two triangles). dvz_array_resize ( arr_vertex , 6 * square_count ); // Pointers to the input data. dvec3 * pos = NULL ; cvec4 * color = NULL ; float * length = NULL ; // Pointer to the output vertex. DvzVertex * vertex = ( DvzVertex * ) arr_vertex -> data ; // Here, we triangulate each square by computing the position of each square corner. float hl = 0 ; for ( uint32_t i = 0 ; i < square_count ; i ++ ) { // We get a pointer to the current item in each prop array. pos = dvz_array_item ( arr_pos , i ); color = dvz_array_item ( arr_color , i ); length = dvz_array_item ( arr_length , i ); // This is the half of the square size. hl = ( * length ) / 2 ; // First triangle: // Bottom-left corner. vertex [ 6 * i + 0 ]. pos [ 0 ] = pos [ 0 ][ 0 ] - hl ; vertex [ 6 * i + 0 ]. pos [ 1 ] = pos [ 0 ][ 1 ] - hl ; // Bottom-right corner. vertex [ 6 * i + 1 ]. pos [ 0 ] = pos [ 0 ][ 0 ] + hl ; vertex [ 6 * i + 1 ]. pos [ 1 ] = pos [ 0 ][ 1 ] - hl ; // Top-right corner. vertex [ 6 * i + 2 ]. pos [ 0 ] = pos [ 0 ][ 0 ] + hl ; vertex [ 6 * i + 2 ]. pos [ 1 ] = pos [ 0 ][ 1 ] + hl ; // Second triangle: // Top-right corner again. vertex [ 6 * i + 3 ]. pos [ 0 ] = pos [ 0 ][ 0 ] + hl ; vertex [ 6 * i + 3 ]. pos [ 1 ] = pos [ 0 ][ 1 ] + hl ; // Top-left corner. vertex [ 6 * i + 4 ]. pos [ 0 ] = pos [ 0 ][ 0 ] - hl ; vertex [ 6 * i + 4 ]. pos [ 1 ] = pos [ 0 ][ 1 ] + hl ; // Bottom-left corner (again). vertex [ 6 * i + 5 ]. pos [ 0 ] = pos [ 0 ][ 0 ] - hl ; vertex [ 6 * i + 5 ]. pos [ 1 ] = pos [ 0 ][ 1 ] - hl ; // We copy the square color to each of the six vertices making the current square. // This is a choice made in this example, and it is up to the custom visual creator // to define how the user data, passed via props, will be used to fill in the vertices. for ( uint32_t j = 0 ; j < 6 ; j ++ ) memcpy ( vertex [ 6 * i + j ]. color , color , sizeof ( cvec4 )); } }","title":"Visual baking function"},{"location":"howto/visual/#putting-everything-together","text":"Here is the code to create the custom visual. // We create a blank visual in the scene. // For demo purposes, we disable the automatic position normalization. DvzVisual * visual = dvz_blank_visual ( scene , DVZ_VISUAL_FLAGS_TRANSFORM_NONE ); // We add the existing graphics triangle graphics pipeline. dvz_visual_graphics ( visual , dvz_graphics_builtin ( canvas , DVZ_GRAPHICS_TRIANGLE , 0 )); // We add the vertex buffer source, and we must specify the same vertex struct type // as the one used by the graphics pipeline (standard vertex structure, with pos and color). dvz_visual_source ( visual , DVZ_SOURCE_TYPE_VERTEX , 0 , DVZ_PIPELINE_GRAPHICS , 0 , 0 , sizeof ( DvzVertex ), 0 ); // We specify the visual props. dvz_visual_prop ( visual , DVZ_PROP_POS , 0 , DVZ_DTYPE_DVEC3 , DVZ_SOURCE_TYPE_VERTEX , 0 ); dvz_visual_prop ( visual , DVZ_PROP_COLOR , 0 , DVZ_DTYPE_CVEC4 , DVZ_SOURCE_TYPE_VERTEX , 0 ); dvz_visual_prop ( visual , DVZ_PROP_LENGTH , 0 , DVZ_DTYPE_FLOAT , DVZ_SOURCE_TYPE_VERTEX , 0 ); // We declare our custom baking function. dvz_visual_callback_bake ( visual , _bake_callback ); // Finally, once the custom visual has been created, we can add it to the panel. dvz_custom_visual ( panel , visual ); Once the custom visual has been created and added to the scene, the last step consists of setting some data, as usual: // We define three squares. dvz_visual_data ( visual , DVZ_PROP_POS , 0 , 3 , ( dvec3 []){{ - .5 , 0 , 0 }, { 0 , 0 , 0 }, { + .5 , 0 , 0 }}); // We use a different color for each square. dvz_visual_data ( visual , DVZ_PROP_COLOR , 0 , 3 , ( cvec4 []){{ 255 , 0 , 0 , 255 }, { 0 , 255 , 0 , 255 }, { 0 , 0 , 255 , 255 }}); // NOTE: we use the same length for all squares. dvz_visual_data ( visual , DVZ_PROP_LENGTH , 0 , 1 , ( float []){ .25 }); Note that we used a single value for the last prop (edge length). Datoviz uses the convention that a prop may have less values than objects, in which case the last value is repeated over . In particular, defining a prop with a single element means using the same value for all items in the visual. This is a sort of \"broadcasting\" rule (following NumPy's terminology).","title":"Putting everything together"},{"location":"howto/visual/#other-topics","text":"We didn't yet cover these other aspects: Position data transformation, Visual parameters stored in the uniform buffer, Visual flags.","title":"Other topics"},{"location":"howto/visual/#checklist-for-datoviz-contributors","text":"This is a handy checklist for developers when adding new visuals to the library (via a pull request). Determine the graphics you'll be using, and the list of source and props builtin_visuals.h : Make sure the DvzVisualType enum exists, or create a new one visuals.h : Check that all prop types exist, otherwise add them builtin_visuals.c : Make a new section for the visual code Write the main visual function Specify the graphics pipeline(s) Specify the sources: Vertex buffer Index buffer (optional) Param buffer (optional) Textures (optional) Specify the props The cast and copy spec are not mandatory if there is a custom baking callback function Note that the special POS prop is DVEC3 and is typically cast to VEC3, but only after baking and panel transform Add a length prop if there are multiple objects Add the common props Add the param props Specify the baking callback function (optional) Write the baking callback function Add a new switch case in dvz_visual_builtin() test_builtin_visuals.h : Add a new visual test declaration test_builtin_visuals.c : Write the body of the test function main.c : Add the new test to the list of test functions Test the new visual without interactivity and save a visual screenshot Add a section in the visual reference docs/reference/visuals.md , document the props, sources, etc.","title":"Checklist for Datoviz contributors"},{"location":"reference/","text":"Reference \u00b6 This section provides a reference of: All included visuals , All included graphics , All included colormaps , All included controllers , and the list of supported event types.","title":"Index"},{"location":"reference/#reference","text":"This section provides a reference of: All included visuals , All included graphics , All included colormaps , All included controllers , and the list of supported event types.","title":"Reference"},{"location":"reference/colormaps/","text":"Colormaps \u00b6 Datoviz natively includes a collection of common colormaps, both continuous and discrete (color palettes). These colormaps come from the following sources: matplotlib bokeh colorcet Kenneth Moreland's page These colormaps are stored in a 256x256 texture. Each row contains either: a 256-color continuous colormap, a 256-color discrete color palette, eight discrete 32-color palettes. Unused space may be used for future or user-defined colormaps. The texture is always loaded both in CPU and GPU memory. It is shared between all visuals and canvases. Datoviz provides a few functions to easily make colors out of scalar values: Python import numpy as np from datoviz import colormap values = np . random . rand ( 1000 ) colors = colormap ( values , vmin = 0 , vmax = 1 , cmap = 'viridis' ) print ( colors ) # output: # [[126 210 78 255] # [ 64 68 135 255] # [ 36 170 130 255] # ... # [ 36 132 141 255] # [ 61 75 137 255] # [ 31 148 139 255]] C DvzColormap cmap = DVZ_CMAP_VIRIDIS ; cvec4 color = { 0 }; uint8_t value = 128 ; double dvalue = .5 ; // Get a single color from a byte. dvz_colormap ( cmap , 128 , color ); // Get a single color from a double, with a custom vmin-vmax range. dvz_colormap_scale ( cmap , dvalue , 0 , 1 , color ); // Get an array of colors from an array of values. const uint32_t N = 10 ; double * values = calloc ( N , sizeof ( double )); cvec4 * colors = calloc ( N , sizeof ( cvec4 )); dvz_colormap_array ( cmap , N , values , 0 , 1 , colors ); FREE ( values ); FREE ( colors ); List of colormaps and color palettes \u00b6 Note The row and col give the offset of the colormap or color palette within the 256x256 colormap texture.","title":"List of colormaps"},{"location":"reference/colormaps/#colormaps","text":"Datoviz natively includes a collection of common colormaps, both continuous and discrete (color palettes). These colormaps come from the following sources: matplotlib bokeh colorcet Kenneth Moreland's page These colormaps are stored in a 256x256 texture. Each row contains either: a 256-color continuous colormap, a 256-color discrete color palette, eight discrete 32-color palettes. Unused space may be used for future or user-defined colormaps. The texture is always loaded both in CPU and GPU memory. It is shared between all visuals and canvases. Datoviz provides a few functions to easily make colors out of scalar values: Python import numpy as np from datoviz import colormap values = np . random . rand ( 1000 ) colors = colormap ( values , vmin = 0 , vmax = 1 , cmap = 'viridis' ) print ( colors ) # output: # [[126 210 78 255] # [ 64 68 135 255] # [ 36 170 130 255] # ... # [ 36 132 141 255] # [ 61 75 137 255] # [ 31 148 139 255]] C DvzColormap cmap = DVZ_CMAP_VIRIDIS ; cvec4 color = { 0 }; uint8_t value = 128 ; double dvalue = .5 ; // Get a single color from a byte. dvz_colormap ( cmap , 128 , color ); // Get a single color from a double, with a custom vmin-vmax range. dvz_colormap_scale ( cmap , dvalue , 0 , 1 , color ); // Get an array of colors from an array of values. const uint32_t N = 10 ; double * values = calloc ( N , sizeof ( double )); cvec4 * colors = calloc ( N , sizeof ( cvec4 )); dvz_colormap_array ( cmap , N , values , 0 , 1 , colors ); FREE ( values ); FREE ( colors );","title":"Colormaps"},{"location":"reference/colormaps/#list-of-colormaps-and-color-palettes","text":"Note The row and col give the offset of the colormap or color palette within the 256x256 colormap texture.","title":"List of colormaps and color palettes"},{"location":"reference/controllers/","text":"Controllers \u00b6 List of controllers \u00b6 When creating a new panel, one needs to specify a Controller . This object defines how the user interacts with the panel. The controllers currently implemented are: static : no interactivity, panzoom : pan and zoom with the mouse, axes : axes with ticks, tick labels, grid, and interactivity with pan and zoom, arcbcall : static 3D camera, model rotation with the mouse, camera : first-person 3D camera. More controllers will be implemented in the future. The C interface used to create custom controllers will be refined too. Panzoom \u00b6 The panzoom controller provides mouse interaction patterns for panning and zooming: Mouse dragging with left button : pan Mouse dragging with right button : zoom in x and y axis independently Mouse wheel : zoom in and out in both axes simultaneously Double-click with left button : reset to initial view Axes 2D \u00b6 The axes 2D controller displays ticks, tick labels, grid and provides panzoom interaction. Arcball \u00b6 The arcball controller is used to rotate a 3D object in all directions using the mouse. It is implemented with quaternions. First-person camera \u00b6 Left-dragging controls the camera, the arrow keys control the position, the Z is controlled by the mouse wheel. List of event types \u00b6 Name Description init called at the beginning of the first frame frame called at every frame refill called when the command buffers need to be recreated (e.g. window resize) resize called when the window is resized timer called in the main loop at regular time intervals mouse_press called when a mouse button is pressed mouse_release called when a mouse button is released mouse_move called when the mouse moves mouse_wheel called when the mouse wheel moves mouse_drag_begin called when a mouse drag operation begins mouse_drag_end called when a mouse drag operation ends mouse_click called after a single click mouse_double_click called after a double click key_press called when a key is pressed key_release called when a key is released","title":"List of controllers"},{"location":"reference/controllers/#controllers","text":"","title":"Controllers"},{"location":"reference/controllers/#list-of-controllers","text":"When creating a new panel, one needs to specify a Controller . This object defines how the user interacts with the panel. The controllers currently implemented are: static : no interactivity, panzoom : pan and zoom with the mouse, axes : axes with ticks, tick labels, grid, and interactivity with pan and zoom, arcbcall : static 3D camera, model rotation with the mouse, camera : first-person 3D camera. More controllers will be implemented in the future. The C interface used to create custom controllers will be refined too.","title":"List of controllers"},{"location":"reference/controllers/#panzoom","text":"The panzoom controller provides mouse interaction patterns for panning and zooming: Mouse dragging with left button : pan Mouse dragging with right button : zoom in x and y axis independently Mouse wheel : zoom in and out in both axes simultaneously Double-click with left button : reset to initial view","title":"Panzoom"},{"location":"reference/controllers/#axes-2d","text":"The axes 2D controller displays ticks, tick labels, grid and provides panzoom interaction.","title":"Axes 2D"},{"location":"reference/controllers/#arcball","text":"The arcball controller is used to rotate a 3D object in all directions using the mouse. It is implemented with quaternions.","title":"Arcball"},{"location":"reference/controllers/#first-person-camera","text":"Left-dragging controls the camera, the arrow keys control the position, the Z is controlled by the mouse wheel.","title":"First-person camera"},{"location":"reference/controllers/#list-of-event-types","text":"Name Description init called at the beginning of the first frame frame called at every frame refill called when the command buffers need to be recreated (e.g. window resize) resize called when the window is resized timer called in the main loop at regular time intervals mouse_press called when a mouse button is pressed mouse_release called when a mouse button is released mouse_move called when the mouse moves mouse_wheel called when the mouse wheel moves mouse_drag_begin called when a mouse drag operation begins mouse_drag_end called when a mouse drag operation ends mouse_click called after a single click mouse_double_click called after a double click key_press called when a key is pressed key_release called when a key is released","title":"List of event types"},{"location":"reference/graphics/","text":"Graphics library \u00b6 This page lists all included graphics. The list is divided into: 2D graphics : high-quality antialiased 2D graphical elements, 3D graphics : meshes and volumes, Basic graphics : basic, low-quality, aliased, but fast basic primitives (points, lines, triangles), useful for demo, testing, and when dealing with tens of millions of points Note If not specified, the default vertex structure is DvzVertex : Field Type Description pos vec3 position color cvec4 color RGBA (four bytes) 2D graphics \u00b6 Marker \u00b6 Marker Segment \u00b6 Segment Path \u00b6 Path Text \u00b6 Text Image \u00b6 Image 3D graphics \u00b6 Mesh \u00b6 Mesh The mesh graphics supports the following features: Phong shading Up to four textures Customizable texture blending coefficients Transparency (but does not play well with depth test) Support for arbitrary RGB values (via cvec3 packing into vec2) Customizable plane clipping Plane clipping: when the clip vector is non-zero, the fragment shader implements the following test. If the dot product of the clip vector with the vertex position (in scene coordinates) is negative, the fragment is discarded. This feature allows to cut the mesh along any arbitrary affine plane. Volume slice \u00b6 VolumeSlice Volume \u00b6 Volume Basic graphics \u00b6 Points \u00b6 Point Lines \u00b6 Lines Line strip \u00b6 Triangles \u00b6 Triangle strip \u00b6 Triangle fan \u00b6 Warning Triangle fan graphics is not supported on macOS and should therefore be avoided if macOS compatibility is desirable.","title":"List of graphics"},{"location":"reference/graphics/#graphics-library","text":"This page lists all included graphics. The list is divided into: 2D graphics : high-quality antialiased 2D graphical elements, 3D graphics : meshes and volumes, Basic graphics : basic, low-quality, aliased, but fast basic primitives (points, lines, triangles), useful for demo, testing, and when dealing with tens of millions of points Note If not specified, the default vertex structure is DvzVertex : Field Type Description pos vec3 position color cvec4 color RGBA (four bytes)","title":"Graphics library"},{"location":"reference/graphics/#2d-graphics","text":"","title":"2D graphics"},{"location":"reference/graphics/#marker","text":"Marker","title":"Marker"},{"location":"reference/graphics/#segment","text":"Segment","title":"Segment"},{"location":"reference/graphics/#path","text":"Path","title":"Path"},{"location":"reference/graphics/#text","text":"Text","title":"Text"},{"location":"reference/graphics/#image","text":"Image","title":"Image"},{"location":"reference/graphics/#3d-graphics","text":"","title":"3D graphics"},{"location":"reference/graphics/#mesh","text":"Mesh The mesh graphics supports the following features: Phong shading Up to four textures Customizable texture blending coefficients Transparency (but does not play well with depth test) Support for arbitrary RGB values (via cvec3 packing into vec2) Customizable plane clipping Plane clipping: when the clip vector is non-zero, the fragment shader implements the following test. If the dot product of the clip vector with the vertex position (in scene coordinates) is negative, the fragment is discarded. This feature allows to cut the mesh along any arbitrary affine plane.","title":"Mesh"},{"location":"reference/graphics/#volume-slice","text":"VolumeSlice","title":"Volume slice"},{"location":"reference/graphics/#volume","text":"Volume","title":"Volume"},{"location":"reference/graphics/#basic-graphics","text":"","title":"Basic graphics"},{"location":"reference/graphics/#points","text":"Point","title":"Points"},{"location":"reference/graphics/#lines","text":"Lines","title":"Lines"},{"location":"reference/graphics/#line-strip","text":"","title":"Line strip"},{"location":"reference/graphics/#triangles","text":"","title":"Triangles"},{"location":"reference/graphics/#triangle-strip","text":"","title":"Triangle strip"},{"location":"reference/graphics/#triangle-fan","text":"Warning Triangle fan graphics is not supported on macOS and should therefore be avoided if macOS compatibility is desirable.","title":"Triangle fan"},{"location":"reference/visuals/","text":"Visuals \u00b6 This page lists all visuals currently implemented in the library. Technical notes \u00b6 In a given visual, a prop is entirely defined by its type and its index. A visual may have multiple props of the same type. For example, the segment visual has a first prop pos with the segment start position, and another with the segment end position. The tables below specify the role of these indices in each case. A visual source corresponds to a GPU object holding the data for the visual. Common source types include: vertex buffer, index buffer, uniform buffer, texture. In a given visual, a source is entirely defined by its type and its index. Each prop is typically linked to a given source. Most props correspond either to shader attributes, in which case they are associated with the vertex buffer, or to global variables, in which case they are associated with uniform buffers. A visual is composed of one or several pipelines : graphics pipelines (or just graphics ), and optionally compute pipelines (or just computes ). A graphics pipeline corresponds to a vertex shader, a fragment shader, and possibly other shaders. In a given visual, each pipeline is entirely defined by its type (graphics or compute) and its index. The tables below specify the different pipelines when there are several of them in a given visual. For example, the axes visual contains a segment graphics for tick segments, and a text graphics for tick labels. Props marked uniform below can only receive a single value. They correspond to struct fields in a uniform buffer, and they are thus shared across all vertices of a given visual. 2D visuals \u00b6 Marker \u00b6 Props Type Index Type Description pos 0 dvec3 marker position color 0 cvec4 marker color marker_size 0 float marker size marker_type 0 char marker type angle 0 char marker angle, between 0 (0) and 256 ( M_2PI ) excluded transform 0 char transform enum color 1 vec4 edge color ( uniform ) line_width 0 float edge line width ( uniform ) Marker types Note Marker shapes are computed in real-time in the fragment shader (vector graphics rather than bitmaps). The GLSL code is from Rougier 2014, Antialiased 2D Grid, Marker, and Arrow Shaders . Marker Value Image disc 0 asterisk 1 chevron 2 clover 3 club 4 cross 5 diamond 6 arrow 7 ellipse 8 hbar 9 heart 10 infinity 11 pin 12 ring 13 spade 14 square 15 tag 16 triangle 17 vbar 18 Path \u00b6 Props Type Index Type Description pos 0 dvec3 all path points, concatenated length 0 uint path lengths, one integer per path color 0 cvec4 point colors, one per point topology 0 DvzPathTopology (int) topology of each path (0=open, 1=closed) line_width 0 float line width of all paths ( uniform ) cap_type 0 DvzCapType (int) cap type ( uniform ) join_type 0 DvzJoinType (int) join type ( uniform ) Polygon \u00b6 This visual currently only uses a basic triangle underlying graphics. It performs a triangulation of the polygons with the earcut C++ library by mapbox. Several arbitrary-sized polygons can be specified in the same visual. Props Type Index Type Description pos 0 dvec3 all polygon points, concatenated length 0 uint polygon lengths, one integer per polygon color 0 cvec4 polygon colors, one per polygon Image \u00b6 Props Type Index Type Description pos 0 dvec3 top left position pos 1 dvec3 top right position pos 2 dvec3 bottom right position pos 3 dvec3 bottom left position texcoords 0 vec3 top left texture coordinates texcoords 1 vec3 top right texture coordinates texcoords 2 vec3 bottom right texture coordinates texcoords 3 vec3 bottom left texture coordinates Sources Type Index Description vertex 0 vertex buffer param 0 parameter struct image 0..3 2D texture with image #i Scalar image with colormap \u00b6 This visual is similar to the image visual, except that it accepts scalar images and colormaps computed on the GPU. Props Type Index Type Description pos 0 dvec3 top left position pos 1 dvec3 top right position pos 2 dvec3 bottom right position pos 3 dvec3 bottom left position texcoords 0 vec3 top left texture coordinates texcoords 1 vec3 top right texture coordinates texcoords 2 vec3 bottom right texture coordinates texcoords 3 vec3 bottom left texture coordinates vrange 0 vec2 colormap range ( uniform ) cmap 0 int colormap number ( uniform ) Sources Type Index Description vertex 0 vertex buffer param 0 parameter struct color_texture 0 colormap texture image 0 2D texture with image Axes \u00b6 Tick level Index Level Description 0 minor minor ticks 1 major major ticks 2 grid grid 3 lim axes delimiters Graphics Index Graphics Description 0 segment ticks (minor, major, grid, lim) 1 text tick labels Props Type Index Type Graphics Description pos any level double segment tick positions in data coordinates color any level cvec4 segment tick colors line_width any level float segment tick line width length minor float segment minor tick length length major float segment major tick length text 0 str text tick labels text text_size 0 float text tick labels font size Sources Type Index Graphics Description vertex 0 segment vertex buffer for ticks index 0 segment index buffer for ticks vertex 1 text vertex buffer for labels index 1 text index buffer for labels font_atlas 0 text font atlas for labels 3D visuals \u00b6 Mesh \u00b6 Features: Up to four blendable textures Up to four lights Automatic computation of normals (cross product of face vertices) if the normal prop is not provided Props Type Index Type Description pos 0 dvec3 vertex position normal 0 vec3 vertex normal texcoords 0 vec2 texture coordinates color 0 cvec4 color as RGB 3-bytes alpha 0 char alpha transparency value index 0 uint32 faces, as vertex indices light_pos 0 mat4 light positions ( uniform ) light_params 0 mat4 light coefficients ( uniform ) texcoefs 0 vec4 texture blending coefficients ( uniform ) clip 0 vec4 clip vector ( uniform ) Warning The texcoords and color props are mutually exclusive. The color has precedence over the texcoords. The mesh vertex struct has no color field, only a texcoord field. When the color prop is set, special texcoords values are computed (packing 3 bytes into the second texture coordinate floating-point number). Sources Type Index Description vertex 0 vertex buffer (vertices) index 0 index buffer (faces) param 0 parameter struct image 0..3 2D texture with image #i Volume \u00b6 Props Type Index Type Description pos 0 dvec3 first corner pos 1 dvec3 opposite corner texcoords 0 vec3 texture coordinates of the first corner texcoords 1 vec3 texture coordinates of the opposite corner colormap 0 int colormap enum ( uniform ) Sources Type Index Description vertex 0 vertex buffer param 0 parameter struct color_texture 0 2D texture with the colormap texture volume 0 3D texture with the volume Volume slice \u00b6 Props Type Index Type Description pos 0 dvec3 top left position pos 1 dvec3 top right position pos 2 dvec3 bottom right position pos 3 dvec3 bottom left position texcoords 0 vec3 top left texture coordinates texcoords 1 vec3 top right texture coordinates texcoords 2 vec3 bottom right texture coordinates texcoords 3 vec3 bottom left texture coordinates transfer_x 0 vec4 colormap transfer function, x values ( uniform ) transfer_y 0 vec4 colormap transfer function, y values ( uniform ) transfer_x 1 vec4 alpha transfer function, x values ( uniform ) transfer_y 1 vec4 alpha transfer function, y values ( uniform ) colormap 0 int colormap enum ( uniform ) scale 0 float volume value scaling factor ( uniform ) Sources Type Index Description vertex 0 vertex buffer param 0 parameter struct color_texture 0 2D texture with the colormap texture volume 0 3D texture with the volume Basic visuals \u00b6 The basic visuals are simpler and more efficient, but they do not support antialiasing. Point \u00b6 The point visual is a trimmed-downed version of the marker visual. It is based on the point primitive. Props Type Index Type Description pos 0 dvec3 point position color 0 cvec4 point color marker_size 0 float point size ( uniform ) Line \u00b6 Props Type Index Type Description pos 0 dvec3 line start position pos 1 dvec3 line end position color 0 color line color Line strip \u00b6 Props Type Index Type Description pos 0 dvec3 point position color 0 cvec4 point color length 0 uint32 number of points in each line strip Triangle \u00b6 Props Type Index Type Description pos 0 dvec3 triangle position 0 pos 1 dvec3 triangle position 1 pos 2 dvec3 triangle position 2 color 0 color triangle color Triangle strip \u00b6 Props Type Index Type Description pos 0 dvec3 point position color 0 color point color Triangle fan \u00b6 Warning This visual is not supported on macOS. Props Type Index Type Description pos 0 dvec3 point position color 0 color point color Common data \u00b6 The sources and props below are shared by all builtin visuals. Common sources \u00b6 Type Index Description mvp 0 DvzMVP structure with model-view-proj matrices viewport 0 DvzViewport structure with viewport info Common props \u00b6 Type Index Type Source Description model 0 mat4 mvp model transformation matrix view 0 mat4 mvp view transformation matrix proj 0 mat4 mvp proj transformation matrix time 0 float mvp time since app start, in seconds Common enums \u00b6 Visual transform \u00b6 Visual clip \u00b6 Data types \u00b6 Data type Component size Type size Description char 8 8 1 byte (unsigned byte) cvec2 8 16 2 bytes cvec3 8 24 3 bytes cvec4 8 32 4 bytes ushort 16 16 1 unsigned short integer usvec2 16 32 2 ushort usvec3 16 48 3 ushort usvec4 16 64 4 ushort short 16 16 1 signed short integer svec2 16 32 2 short svec3 16 48 3 short svec4 16 64 4 short uint 32 32 1 unsigned long integer uvec2 32 64 2 uint uvec3 32 96 3 uint uvec4 32 128 4 uint int 32 32 1 long integer ivec2 32 64 2 int ivec3 32 96 3 int ivec4 32 128 4 int float 32 32 1 single-precision floating-point number vec2 32 64 2 float vec3 32 96 3 float vec4 32 128 4 float double 64 64 1 double-precision floating-point number dvec2 64 128 2 double dvec3 64 192 3 double dvec4 64 256 4 double mat2 32 128 2x2 matrix of floats mat3 32 288 3x3 matrix of floats mat4 32 512 4x4 matrix of floats custom - - used by structured/record arrays (heterogeneous types) str 64 64 pointer to char","title":"List of visuals"},{"location":"reference/visuals/#visuals","text":"This page lists all visuals currently implemented in the library.","title":"Visuals"},{"location":"reference/visuals/#technical-notes","text":"In a given visual, a prop is entirely defined by its type and its index. A visual may have multiple props of the same type. For example, the segment visual has a first prop pos with the segment start position, and another with the segment end position. The tables below specify the role of these indices in each case. A visual source corresponds to a GPU object holding the data for the visual. Common source types include: vertex buffer, index buffer, uniform buffer, texture. In a given visual, a source is entirely defined by its type and its index. Each prop is typically linked to a given source. Most props correspond either to shader attributes, in which case they are associated with the vertex buffer, or to global variables, in which case they are associated with uniform buffers. A visual is composed of one or several pipelines : graphics pipelines (or just graphics ), and optionally compute pipelines (or just computes ). A graphics pipeline corresponds to a vertex shader, a fragment shader, and possibly other shaders. In a given visual, each pipeline is entirely defined by its type (graphics or compute) and its index. The tables below specify the different pipelines when there are several of them in a given visual. For example, the axes visual contains a segment graphics for tick segments, and a text graphics for tick labels. Props marked uniform below can only receive a single value. They correspond to struct fields in a uniform buffer, and they are thus shared across all vertices of a given visual.","title":"Technical notes"},{"location":"reference/visuals/#2d-visuals","text":"","title":"2D visuals"},{"location":"reference/visuals/#marker","text":"","title":"Marker"},{"location":"reference/visuals/#path","text":"","title":"Path"},{"location":"reference/visuals/#polygon","text":"This visual currently only uses a basic triangle underlying graphics. It performs a triangulation of the polygons with the earcut C++ library by mapbox. Several arbitrary-sized polygons can be specified in the same visual.","title":"Polygon"},{"location":"reference/visuals/#image","text":"","title":"Image"},{"location":"reference/visuals/#scalar-image-with-colormap","text":"This visual is similar to the image visual, except that it accepts scalar images and colormaps computed on the GPU.","title":"Scalar image with colormap"},{"location":"reference/visuals/#axes","text":"","title":"Axes"},{"location":"reference/visuals/#3d-visuals","text":"","title":"3D visuals"},{"location":"reference/visuals/#mesh","text":"Features: Up to four blendable textures Up to four lights Automatic computation of normals (cross product of face vertices) if the normal prop is not provided","title":"Mesh"},{"location":"reference/visuals/#volume","text":"","title":"Volume"},{"location":"reference/visuals/#volume-slice","text":"","title":"Volume slice"},{"location":"reference/visuals/#basic-visuals","text":"The basic visuals are simpler and more efficient, but they do not support antialiasing.","title":"Basic visuals"},{"location":"reference/visuals/#point","text":"The point visual is a trimmed-downed version of the marker visual. It is based on the point primitive.","title":"Point"},{"location":"reference/visuals/#line","text":"","title":"Line"},{"location":"reference/visuals/#line-strip","text":"","title":"Line strip"},{"location":"reference/visuals/#triangle","text":"","title":"Triangle"},{"location":"reference/visuals/#triangle-strip","text":"","title":"Triangle strip"},{"location":"reference/visuals/#triangle-fan","text":"Warning This visual is not supported on macOS.","title":"Triangle fan"},{"location":"reference/visuals/#common-data","text":"The sources and props below are shared by all builtin visuals.","title":"Common data"},{"location":"reference/visuals/#common-sources","text":"Type Index Description mvp 0 DvzMVP structure with model-view-proj matrices viewport 0 DvzViewport structure with viewport info","title":"Common sources"},{"location":"reference/visuals/#common-props","text":"Type Index Type Source Description model 0 mat4 mvp model transformation matrix view 0 mat4 mvp view transformation matrix proj 0 mat4 mvp proj transformation matrix time 0 float mvp time since app start, in seconds","title":"Common props"},{"location":"reference/visuals/#common-enums","text":"","title":"Common enums"},{"location":"reference/visuals/#visual-transform","text":"","title":"Visual transform"},{"location":"reference/visuals/#visual-clip","text":"","title":"Visual clip"},{"location":"reference/visuals/#data-types","text":"Data type Component size Type size Description char 8 8 1 byte (unsigned byte) cvec2 8 16 2 bytes cvec3 8 24 3 bytes cvec4 8 32 4 bytes ushort 16 16 1 unsigned short integer usvec2 16 32 2 ushort usvec3 16 48 3 ushort usvec4 16 64 4 ushort short 16 16 1 signed short integer svec2 16 32 2 short svec3 16 48 3 short svec4 16 64 4 short uint 32 32 1 unsigned long integer uvec2 32 64 2 uint uvec3 32 96 3 uint uvec4 32 128 4 uint int 32 32 1 long integer ivec2 32 64 2 int ivec3 32 96 3 int ivec4 32 128 4 int float 32 32 1 single-precision floating-point number vec2 32 64 2 float vec3 32 96 3 float vec4 32 128 4 float double 64 64 1 double-precision floating-point number dvec2 64 128 2 double dvec3 64 192 3 double dvec4 64 256 4 double mat2 32 128 2x2 matrix of floats mat3 32 288 3x3 matrix of floats mat4 32 512 4x4 matrix of floats custom - - used by structured/record arrays (heterogeneous types) str 64 64 pointer to char","title":"Data types"},{"location":"tutorials/","text":"Tutorials \u00b6 This section proposes the following guides: Installing Datoviz Datoviz quickstart with Python","title":"Index"},{"location":"tutorials/#tutorials","text":"This section proposes the following guides: Installing Datoviz Datoviz quickstart with Python","title":"Tutorials"},{"location":"tutorials/install/","text":"Installation \u00b6 Warning Datoviz is at an early stage of development and doesn't yet provide precompiled packages. Manual compilation is required. Datoviz should work on most systems. It has been developed on Linux (Ubuntu 20.04), tested on macOS (Intel). Windows support is preliminary, help appreciated on this platform. This section describes how to compile Datoviz on different operating systems. Python bindings \u00b6 Datoviz comprises two parts: A full featured C library , Light Python bindings written in Cython. The philosophy of Datoviz is to implement all the logic and functionality in C , and provide minimal bindings in high-level languages around this functionality. This design choice ensures that future bindings that might be developed in different languages would all share the same functionality (Julia, R, MATLAB, Rust, C#, and so on). Note Most efforts were so far dedicated to the C library, whereas the Python bindings are at still an early stage of development. Dependencies of the Python bindings are: NumPy IPython Building the Cython bindings manually (which is mandatory at the moment) requires the following additional dependencies: cython pyparsing colorcet imageio Dependencies of the C library \u00b6 Mandatory dependencies that need to be installed before compiling the C library are: LunarG Vulkan SDK cmake 3.16+ (build) ninja (build) Optional dependencies are: freetype (optional) libpng (optional) ffmpeg (optional) Qt5 (optional, not supported on Ubuntu strictly below 20.04) The other dependencies below are already included in the library, so you don't need to install them manually : glfw3 3.3+ : cross-platform windowing system cglm : basic types and math computations on vectors and matrices stb_image : image file input and output Dear ImGui : rich graphical user interfaces earcut.hpp : triangulation of polygons triangle : triangulation of complex polygons and planar straight-line graphs (PSLG) tiny_obj_loader : loading of .obj mesh files Manual compilation \u00b6 Ubuntu 20.04+ \u00b6 Install the latest graphics drivers. Install the build tools: sudo apt install build-essential cmake ninja-build xcb libx11-xcb-dev libglfw3-dev Install the optional dependencies: sudo apt install libpng-dev libavcodec-dev libavformat-dev libavfilter-dev libavutil-dev libswresample-dev libvncserver-dev xtightvncviewer libqt5opengl5-dev libfreetype6-dev libassimp-dev Install the latest Lunarg Vulkan SDK (tarball SDK), for example in ~/vulkan cd ~/vulkan ./vulkansdk samples (build the Vulkan samples) ./samples/build/Sample-Programs/Hologram/Hologram (test an example) Important : add source ~/vulkan/setup-env.sh to your ~/.bashrc so that the $VULKAN_SDK environment variable and other variables are properly set in your terminal. Clone the Datoviz repository and build the library: git clone --recursive git@github.com:datoviz/datoviz.git cd datoviz ./manage.sh build Check that the compilation worked by running an example: ./manage.sh demo Note: this will only work if Vulkan SDK's setup-env.sh file is source-ed in the terminal. Compile the Cython module: ./manage.sh cython Export the shared library path in your environment: source setup-env.sh Try a Python example: python bindings/cython/examples/quickstart.py macOS \u00b6 Install the dependencies Type git in a terminal to install it. Install Xcode Install git-lfs (to download large test/example datasets) Install Homebrew if you don't have it already Type brew install cmake ninja Install Vulkan Install the latest Vulkan SDK cd /Volumes/vulkansdk-macos-1.2.154.0 (replace by appropriate version) ./install_vulkan.py Install Datoviz git clone --recursive git@github.com:datoviz/datoviz.git cd datoviz ./manage.sh build ./manage.sh demo Build the Cython module Compile the Cython module: ./manage.sh cython Export the shared library path in your environment: source setup-env.sh Try a Python example: python bindings/cython/examples/quickstart.py Windows 10 \u00b6 Warning Only mingw-w64 is supported at the moment. Microsoft Visual C++ is not yet supported. Help needed to fill in the details below : Install the latest graphics drivers for your system and hardware. Install Winlibs , a Windows port of gcc, using mingw-w64. Make sure the mingw executable is in the PATH. Install CMake for Windows Install the latest Lunarg Vulkan SDK ( .exe executable). Install the Windows Universal C Runtime https://stackoverflow.com/a/52329698 Clone the repository: git clone --recursive git@github.com:datoviz/datoviz.git Enter the following commands within the repository's directory (using the Windows terminal, not in Windows for Linux subsystem): mkdir build cd build cmake .. -G \"MinGW Makefiles\" mingw32-make cd .. To run an example (the batch script will only work in cmd.exe, not Powershell): setup-env.bat set DVZ_INTERACT=1 build\\datoviz.exe test test_scene_ax To build the Cython package: setup-env.bat cd bindings\\cython pip install -r requirements.txt python setup.py build_ext -i -c mingw32 python setup.py develop Copy build\\libdatoviz.dll to bindings\\cython\\datoviz\\ (there has to be a better way) python bindings\\cython\\examples\\quickstart.py Notes \u00b6 CPU emulation \u00b6 CPU Vulkan emulation is useful on computers with no GPUs or on continuous integration servers, for testing purposes only. It is provided by the Swiftshader library developed by Google. Install https://github.com/google/swiftshader Temporarily override your native Vulkan driver with the SwiftShader one: Linux: export LD_LIBRARY_PATH=/path/to/swiftshader/build/Linux/:$LD_LIBRARY_PATH Run the Datoviz tests as usual ./test.sh","title":"Installation"},{"location":"tutorials/install/#installation","text":"Warning Datoviz is at an early stage of development and doesn't yet provide precompiled packages. Manual compilation is required. Datoviz should work on most systems. It has been developed on Linux (Ubuntu 20.04), tested on macOS (Intel). Windows support is preliminary, help appreciated on this platform. This section describes how to compile Datoviz on different operating systems.","title":"Installation"},{"location":"tutorials/install/#python-bindings","text":"Datoviz comprises two parts: A full featured C library , Light Python bindings written in Cython. The philosophy of Datoviz is to implement all the logic and functionality in C , and provide minimal bindings in high-level languages around this functionality. This design choice ensures that future bindings that might be developed in different languages would all share the same functionality (Julia, R, MATLAB, Rust, C#, and so on). Note Most efforts were so far dedicated to the C library, whereas the Python bindings are at still an early stage of development. Dependencies of the Python bindings are: NumPy IPython Building the Cython bindings manually (which is mandatory at the moment) requires the following additional dependencies: cython pyparsing colorcet imageio","title":"Python bindings"},{"location":"tutorials/install/#dependencies-of-the-c-library","text":"Mandatory dependencies that need to be installed before compiling the C library are: LunarG Vulkan SDK cmake 3.16+ (build) ninja (build) Optional dependencies are: freetype (optional) libpng (optional) ffmpeg (optional) Qt5 (optional, not supported on Ubuntu strictly below 20.04) The other dependencies below are already included in the library, so you don't need to install them manually : glfw3 3.3+ : cross-platform windowing system cglm : basic types and math computations on vectors and matrices stb_image : image file input and output Dear ImGui : rich graphical user interfaces earcut.hpp : triangulation of polygons triangle : triangulation of complex polygons and planar straight-line graphs (PSLG) tiny_obj_loader : loading of .obj mesh files","title":"Dependencies of the C library"},{"location":"tutorials/install/#manual-compilation","text":"","title":"Manual compilation"},{"location":"tutorials/install/#ubuntu-2004","text":"Install the latest graphics drivers. Install the build tools: sudo apt install build-essential cmake ninja-build xcb libx11-xcb-dev libglfw3-dev Install the optional dependencies: sudo apt install libpng-dev libavcodec-dev libavformat-dev libavfilter-dev libavutil-dev libswresample-dev libvncserver-dev xtightvncviewer libqt5opengl5-dev libfreetype6-dev libassimp-dev Install the latest Lunarg Vulkan SDK (tarball SDK), for example in ~/vulkan cd ~/vulkan ./vulkansdk samples (build the Vulkan samples) ./samples/build/Sample-Programs/Hologram/Hologram (test an example) Important : add source ~/vulkan/setup-env.sh to your ~/.bashrc so that the $VULKAN_SDK environment variable and other variables are properly set in your terminal. Clone the Datoviz repository and build the library: git clone --recursive git@github.com:datoviz/datoviz.git cd datoviz ./manage.sh build Check that the compilation worked by running an example: ./manage.sh demo Note: this will only work if Vulkan SDK's setup-env.sh file is source-ed in the terminal. Compile the Cython module: ./manage.sh cython Export the shared library path in your environment: source setup-env.sh Try a Python example: python bindings/cython/examples/quickstart.py","title":"Ubuntu 20.04+"},{"location":"tutorials/install/#macos","text":"","title":"macOS"},{"location":"tutorials/install/#windows-10","text":"Warning Only mingw-w64 is supported at the moment. Microsoft Visual C++ is not yet supported. Help needed to fill in the details below : Install the latest graphics drivers for your system and hardware. Install Winlibs , a Windows port of gcc, using mingw-w64. Make sure the mingw executable is in the PATH. Install CMake for Windows Install the latest Lunarg Vulkan SDK ( .exe executable). Install the Windows Universal C Runtime https://stackoverflow.com/a/52329698 Clone the repository: git clone --recursive git@github.com:datoviz/datoviz.git Enter the following commands within the repository's directory (using the Windows terminal, not in Windows for Linux subsystem): mkdir build cd build cmake .. -G \"MinGW Makefiles\" mingw32-make cd .. To run an example (the batch script will only work in cmd.exe, not Powershell): setup-env.bat set DVZ_INTERACT=1 build\\datoviz.exe test test_scene_ax To build the Cython package: setup-env.bat cd bindings\\cython pip install -r requirements.txt python setup.py build_ext -i -c mingw32 python setup.py develop Copy build\\libdatoviz.dll to bindings\\cython\\datoviz\\ (there has to be a better way) python bindings\\cython\\examples\\quickstart.py","title":"Windows 10"},{"location":"tutorials/install/#notes","text":"","title":"Notes"},{"location":"tutorials/install/#cpu-emulation","text":"CPU Vulkan emulation is useful on computers with no GPUs or on continuous integration servers, for testing purposes only. It is provided by the Swiftshader library developed by Google. Install https://github.com/google/swiftshader Temporarily override your native Vulkan driver with the SwiftShader one: Linux: export LD_LIBRARY_PATH=/path/to/swiftshader/build/Linux/:$LD_LIBRARY_PATH Run the Datoviz tests as usual ./test.sh","title":"CPU emulation"},{"location":"tutorials/quickstart/","text":"Quickstart: using Datoviz in Python \u00b6 Once Datoviz has been properly installed, you can start to use it in a few lines of code! In this tutorial, we'll show how to make a simple 2D plot with Datoviz in Python , and we'll go through the most important features of the library. We'll cover the following steps: how to create an application, how to create a canvas, how to create a panel with an axes controller, how to add a visual, how to use colormaps, how to set visual data, how to run the application, how to create a minimal GUI, how to specify event callbacks, how to use mouse picking, how to make a live screencast video ( requires compilation with ffmpeg ), and more. Creating custom visuals, creating standalone C applications with Datoviz are advanced topics, they are covered in the How to section of the documentation. Note The Python bindings are at an early stage of development. They will be significantly improved in the near future. Importing the library \u00b6 Note For now, Datoviz should be used from a Python script. Integration with IPython and Jupyter is still a work in progress. First, we import NumPy and datoviz: import numpy as np import numpy.random as nr from datoviz import canvas , run , colormap Creating a canvas \u00b6 We create a canvas : c = canvas ( show_fps = False ) We can also specify the initial width and height of the window using keyword arguments to canvas() . Creating a panel \u00b6 Next, we create a panel , which is another word for \"subplot\". By default, there is only one panel spanning the entire canvas, but we can also define multiple panels. We also specify the panel's controller , which defines how we interact with it. The axes controller displays axes and ticks for 2D graphics. panel = c . panel ( controller = 'axes' ) Choosing one of the existing visuals \u00b6 We'll make a simple scatter plot with 2D random points, and different colors and marker sizes. We refer to the list of all included visuals provided by the Datoviz documentation, and we find that the marker visual is what we want for our scatter plot. We look at the visual properties (or props ) for this visual: this is the data we'll need to feed to our visual. But first, we create our visual object by specifying its type: visual = panel . visual ( 'marker' ) Preparing the visual data \u00b6 We'll set: the marker positions : pos prop, the marker colors : color prop, the marker sizes : ms prop. First, we generate the data for these props. Random positions \u00b6 N = 100_000 pos = nr . randn ( N , 3 ) Note that positions always have three dimensions in Datoviz. When using 2D plotting, we set the third component to zero. Datoviz uses the standard OpenGL 3D coordinate system: Datoviz coordinate system Note Note that this is different from the Vulkan coordinate system, where y and z go in the opposite direction. The other difference is that in Datoviz, all axes range in the interval [-1, +1] . In the original Vulkan coordinate system, z goes from 0 to 1 instead. The convention used in Datoviz makes it possible to use existing camera matrix routines implemented in the cglm library. The GPU code of all included shaders include the final OpenGL->Vulkan transformation right before the vertex shader output. Other conventions for x, y, z axes will be supported in the future. Position props are specified in the original data coordinate system corresponding to the scientific data to be visualized. Yet, Datoviz requires vertex positions to be in normalized coordinates (between -1 and 1) when sent to the GPU. Since the GPU only deals with single-precision floating point numbers, doing data normalization on the GPU would result in significant loss of precision and would harm performance. Therefore, Datoviz provides a system to make transformations on the CPU in double precision before uploading the data to the GPU. By default, the data is linearly transformed to fit the [-1, +1] cube. Other types of transformations will soon be implemented (polar coordinates, geographic coordinate systems, and so on). Random marker size \u00b6 We define random marker sizes as an array of floating-point values: ms = nr . uniform ( low = 2 , high = 40 , size = N ) Colormap \u00b6 Let's define the colors. We could use random RGBA values for the colors, but we'll use one of the built-in colormap instead. color = colormap ( nr . rand ( N ), vmin = 0 , vmax = 1 , alpha =. 75 * np . ones ( N ), cmap = 'viridis' ) The variable color is an (N, 4) array of uint8 (byte values between 0 and 255). This line involves the following steps: Choosing a colormap, here viridis (see the colormap reference page with the list of ~150 included colormaps), Defining an array of scalar values to be fed to the colormap (random values between 0 and 1 here), (Optional) Defining the colormap range ( [0, 1] here), (Optional) Setting an alpha transparency channel (0.75 here). Set the visual data \u00b6 Finally, the most important bit is to set the visual prop data with the arrays we just created : visual . data ( 'pos' , pos ) visual . data ( 'color' , color ) visual . data ( 'ms' , ms ) Running the application \u00b6 Finally, we run the application by starting the main event loop: run () Making a screenshot \u00b6 We can easily make a screenshot of the first frame of the canvas. Note Screenshot support will be improved soon. run ( screenshot = \"screenshot.png\" ) Recording a screencast video \u00b6 If the library was compiled with ffmpeg, we can easily make a live mp4 screencast of the canvas. Warning The canvas should NOT be resized when doing a screencast, or the video will be corrupted. run ( video = \"screencast.mp4\" ) This command does not start the video recording, one needs to press the Play button at the bottom right corner. We can pause and resume at any time. When we're done, we press the Stop button and the video will be saved to disk. Event callbacks and mouse picking \u00b6 Important From now on, all the code snippets below needs to be added before calling run() . We'll write a callback function that is called when the user clicks in the canvas, and that prints the coordinates of the clicked point in the original data coordinate system. # We define an event callback to implement mouse picking @c . connect def on_mouse_click ( x , y , button , modifiers = ()): # x, y are in pixel coordinates # First, we find the picked panel p = c . panel_at ( x , y ) if not p : return # Then, we transform into the data coordinate system # Supported coordinate systems: # target_cds='data' / 'scene' / 'vulkan' / 'framebuffer' / 'window' xd , yd = p . pick ( x , y ) print ( f \"Pick at ( { xd : .4f } , { yd : .4f } ), modifiers= { modifiers } \" ) Clicking somewhere shows in the terminal output: Pick at (0.4605, -0.1992), modifiers=() Coordinate systems \u00b6 By default, the panel.pick() function converts coordinates from the window coordinate system (used by the event callbacks) to the data coordinate system. There are other coordinate systems that you can convert to using the target_cds keyword argument to pick() : Name Description data original coordinates of the data scene the coordinates before controller transformation (panzoom etc) in [-1, +1] vulkan the coordinates after controller transformation, in [-1, +1] framebuffer the coordinates in framebuffer pixel coordinates window the coordinates in screen pixel coordinates A few technical notes: The scene coordinate system corresponds to the vertex shader input . The vulkan coordinate system corresponds to the vertex shader output . There's a difference between the framebuffer and window systems with high-DPI monitors. This depends on the OS. For now, DPI support is semi-manual. Datoviz supports a special dpi_scaling variable that rescales the visual elements depending on this value, and that can be adjusted manually (to be documented later). Adding a simple GUI \u00b6 Datoviz integrates the Dear ImGUI library which allows one to create GUIs directly in a Datoviz canvas, without using external dependencies such as Qt. Adding a GUI dialog \u00b6 We create a new GUI dialog. # We create a GUI dialog. gui = c . gui ( \"Test GUI\" ) Adding a control to the GUI \u00b6 We add a slider to change the visual marker size. # We add a control, a slider controlling a float @gui . control ( \"slider_float\" , \"marker size\" , vmin =. 5 , vmax = 2 ) def on_change ( value ): # Every time the slider value changes, we update the visual's marker size visual . data ( 'ms' , ms * value ) # NOTE: an upcoming version will support partial updates We add another slider, using integers this time, to change the colormap. # We add another control, a slider controlling an int between 1 and 4, to change the colormap. # NOTE: an upcoming version will provide a dropdown menu control cmaps = [ 'viridis' , 'cividis' , 'autumn' , 'winter' ] @gui . control ( \"slider_int\" , \"colormap\" , vmin = 0 , vmax = 3 ) def on_change ( value ): # Recompute the colors. color = colormap ( color_values , vmin = 0 , vmax = 1 , alpha =. 75 * np . ones ( N ), cmap = cmaps [ value ]) # Update the color visual visual . data ( 'color' , color ) Finally we add a button to regenerate the marker positions. # We add a button to regenerate the marker positions @gui . control ( \"button\" , \"new positions\" ) def on_change ( value ): pos = nr . randn ( N , 3 ) visual . data ( 'pos' , pos )","title":"Quickstart"},{"location":"tutorials/quickstart/#quickstart-using-datoviz-in-python","text":"Once Datoviz has been properly installed, you can start to use it in a few lines of code! In this tutorial, we'll show how to make a simple 2D plot with Datoviz in Python , and we'll go through the most important features of the library. We'll cover the following steps: how to create an application, how to create a canvas, how to create a panel with an axes controller, how to add a visual, how to use colormaps, how to set visual data, how to run the application, how to create a minimal GUI, how to specify event callbacks, how to use mouse picking, how to make a live screencast video ( requires compilation with ffmpeg ), and more. Creating custom visuals, creating standalone C applications with Datoviz are advanced topics, they are covered in the How to section of the documentation. Note The Python bindings are at an early stage of development. They will be significantly improved in the near future.","title":"Quickstart: using Datoviz in Python"},{"location":"tutorials/quickstart/#importing-the-library","text":"Note For now, Datoviz should be used from a Python script. Integration with IPython and Jupyter is still a work in progress. First, we import NumPy and datoviz: import numpy as np import numpy.random as nr from datoviz import canvas , run , colormap","title":"Importing the library"},{"location":"tutorials/quickstart/#creating-a-canvas","text":"We create a canvas : c = canvas ( show_fps = False ) We can also specify the initial width and height of the window using keyword arguments to canvas() .","title":"Creating a canvas"},{"location":"tutorials/quickstart/#creating-a-panel","text":"Next, we create a panel , which is another word for \"subplot\". By default, there is only one panel spanning the entire canvas, but we can also define multiple panels. We also specify the panel's controller , which defines how we interact with it. The axes controller displays axes and ticks for 2D graphics. panel = c . panel ( controller = 'axes' )","title":"Creating a panel"},{"location":"tutorials/quickstart/#choosing-one-of-the-existing-visuals","text":"We'll make a simple scatter plot with 2D random points, and different colors and marker sizes. We refer to the list of all included visuals provided by the Datoviz documentation, and we find that the marker visual is what we want for our scatter plot. We look at the visual properties (or props ) for this visual: this is the data we'll need to feed to our visual. But first, we create our visual object by specifying its type: visual = panel . visual ( 'marker' )","title":"Choosing one of the existing visuals"},{"location":"tutorials/quickstart/#preparing-the-visual-data","text":"We'll set: the marker positions : pos prop, the marker colors : color prop, the marker sizes : ms prop. First, we generate the data for these props.","title":"Preparing the visual data"},{"location":"tutorials/quickstart/#random-positions","text":"N = 100_000 pos = nr . randn ( N , 3 ) Note that positions always have three dimensions in Datoviz. When using 2D plotting, we set the third component to zero. Datoviz uses the standard OpenGL 3D coordinate system: Datoviz coordinate system Note Note that this is different from the Vulkan coordinate system, where y and z go in the opposite direction. The other difference is that in Datoviz, all axes range in the interval [-1, +1] . In the original Vulkan coordinate system, z goes from 0 to 1 instead. The convention used in Datoviz makes it possible to use existing camera matrix routines implemented in the cglm library. The GPU code of all included shaders include the final OpenGL->Vulkan transformation right before the vertex shader output. Other conventions for x, y, z axes will be supported in the future. Position props are specified in the original data coordinate system corresponding to the scientific data to be visualized. Yet, Datoviz requires vertex positions to be in normalized coordinates (between -1 and 1) when sent to the GPU. Since the GPU only deals with single-precision floating point numbers, doing data normalization on the GPU would result in significant loss of precision and would harm performance. Therefore, Datoviz provides a system to make transformations on the CPU in double precision before uploading the data to the GPU. By default, the data is linearly transformed to fit the [-1, +1] cube. Other types of transformations will soon be implemented (polar coordinates, geographic coordinate systems, and so on).","title":"Random positions"},{"location":"tutorials/quickstart/#random-marker-size","text":"We define random marker sizes as an array of floating-point values: ms = nr . uniform ( low = 2 , high = 40 , size = N )","title":"Random marker size"},{"location":"tutorials/quickstart/#colormap","text":"Let's define the colors. We could use random RGBA values for the colors, but we'll use one of the built-in colormap instead. color = colormap ( nr . rand ( N ), vmin = 0 , vmax = 1 , alpha =. 75 * np . ones ( N ), cmap = 'viridis' ) The variable color is an (N, 4) array of uint8 (byte values between 0 and 255). This line involves the following steps: Choosing a colormap, here viridis (see the colormap reference page with the list of ~150 included colormaps), Defining an array of scalar values to be fed to the colormap (random values between 0 and 1 here), (Optional) Defining the colormap range ( [0, 1] here), (Optional) Setting an alpha transparency channel (0.75 here).","title":"Colormap"},{"location":"tutorials/quickstart/#set-the-visual-data","text":"Finally, the most important bit is to set the visual prop data with the arrays we just created : visual . data ( 'pos' , pos ) visual . data ( 'color' , color ) visual . data ( 'ms' , ms )","title":"Set the visual data"},{"location":"tutorials/quickstart/#running-the-application","text":"Finally, we run the application by starting the main event loop: run ()","title":"Running the application"},{"location":"tutorials/quickstart/#making-a-screenshot","text":"We can easily make a screenshot of the first frame of the canvas. Note Screenshot support will be improved soon. run ( screenshot = \"screenshot.png\" )","title":"Making a screenshot"},{"location":"tutorials/quickstart/#recording-a-screencast-video","text":"If the library was compiled with ffmpeg, we can easily make a live mp4 screencast of the canvas. Warning The canvas should NOT be resized when doing a screencast, or the video will be corrupted. run ( video = \"screencast.mp4\" ) This command does not start the video recording, one needs to press the Play button at the bottom right corner. We can pause and resume at any time. When we're done, we press the Stop button and the video will be saved to disk.","title":"Recording a screencast video"},{"location":"tutorials/quickstart/#event-callbacks-and-mouse-picking","text":"Important From now on, all the code snippets below needs to be added before calling run() . We'll write a callback function that is called when the user clicks in the canvas, and that prints the coordinates of the clicked point in the original data coordinate system. # We define an event callback to implement mouse picking @c . connect def on_mouse_click ( x , y , button , modifiers = ()): # x, y are in pixel coordinates # First, we find the picked panel p = c . panel_at ( x , y ) if not p : return # Then, we transform into the data coordinate system # Supported coordinate systems: # target_cds='data' / 'scene' / 'vulkan' / 'framebuffer' / 'window' xd , yd = p . pick ( x , y ) print ( f \"Pick at ( { xd : .4f } , { yd : .4f } ), modifiers= { modifiers } \" ) Clicking somewhere shows in the terminal output: Pick at (0.4605, -0.1992), modifiers=()","title":"Event callbacks and mouse picking"},{"location":"tutorials/quickstart/#coordinate-systems","text":"By default, the panel.pick() function converts coordinates from the window coordinate system (used by the event callbacks) to the data coordinate system. There are other coordinate systems that you can convert to using the target_cds keyword argument to pick() : Name Description data original coordinates of the data scene the coordinates before controller transformation (panzoom etc) in [-1, +1] vulkan the coordinates after controller transformation, in [-1, +1] framebuffer the coordinates in framebuffer pixel coordinates window the coordinates in screen pixel coordinates A few technical notes: The scene coordinate system corresponds to the vertex shader input . The vulkan coordinate system corresponds to the vertex shader output . There's a difference between the framebuffer and window systems with high-DPI monitors. This depends on the OS. For now, DPI support is semi-manual. Datoviz supports a special dpi_scaling variable that rescales the visual elements depending on this value, and that can be adjusted manually (to be documented later).","title":"Coordinate systems"},{"location":"tutorials/quickstart/#adding-a-simple-gui","text":"Datoviz integrates the Dear ImGUI library which allows one to create GUIs directly in a Datoviz canvas, without using external dependencies such as Qt.","title":"Adding a simple GUI"},{"location":"tutorials/quickstart/#adding-a-gui-dialog","text":"We create a new GUI dialog. # We create a GUI dialog. gui = c . gui ( \"Test GUI\" )","title":"Adding a GUI dialog"},{"location":"tutorials/quickstart/#adding-a-control-to-the-gui","text":"We add a slider to change the visual marker size. # We add a control, a slider controlling a float @gui . control ( \"slider_float\" , \"marker size\" , vmin =. 5 , vmax = 2 ) def on_change ( value ): # Every time the slider value changes, we update the visual's marker size visual . data ( 'ms' , ms * value ) # NOTE: an upcoming version will support partial updates We add another slider, using integers this time, to change the colormap. # We add another control, a slider controlling an int between 1 and 4, to change the colormap. # NOTE: an upcoming version will provide a dropdown menu control cmaps = [ 'viridis' , 'cividis' , 'autumn' , 'winter' ] @gui . control ( \"slider_int\" , \"colormap\" , vmin = 0 , vmax = 3 ) def on_change ( value ): # Recompute the colors. color = colormap ( color_values , vmin = 0 , vmax = 1 , alpha =. 75 * np . ones ( N ), cmap = cmaps [ value ]) # Update the color visual visual . data ( 'color' , color ) Finally we add a button to regenerate the marker positions. # We add a button to regenerate the marker positions @gui . control ( \"button\" , \"new positions\" ) def on_change ( value ): pos = nr . randn ( N , 3 ) visual . data ( 'pos' , pos )","title":"Adding a control to the GUI"}]}